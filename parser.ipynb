{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "2.7.17-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python271764bitfc26ea4e491d46f7ba4683f54ef403ba",
   "display_name": "Python 2.7.17 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Parse les donn√©es d'apprentissage et de test"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "from xml.dom.minidom import parseString\n",
    "from xml.dom.minidom import parse\n",
    "import xml.etree.ElementTree as et\n",
    "\n",
    "sens_path = \"trial/data/multilingual-all-words.fr.senses\"\n",
    "gold_truth_path = \"trial/keys/keys-bn.fr\"\n",
    "corpus_path = \"trial/data/multilingual-all-words.fr.xml\"\n",
    "\n",
    "output_corpus_path = \"trial_corpus.xml\""
   ]
  },
  {
   "source": [
    "## Parser sur le fichier de sens"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "5 first entries in the BabelNet senses dictionnary\n(' -', 'mardi', '(6)', ['bn:01718496n', 'bn:01502893n', 'bn:01433525n', 'bn:02861587n', 'bn:00078546n', 'bn:00807619n'])\n(' -', 'dollar', '(10)', ['bn:00028114n', 'bn:02156595n', 'bn:01827683n', 'bn:00008375n', 'bn:00015129n', 'bn:02122502n', 'bn:00028118n', 'bn:00028116n', 'bn:02897671n', 'bn:01524928n'])\n(' -', 'rang', '(10)', ['bn:00054281n', 'bn:01743576n', 'bn:02256034n', 'bn:00073933n', 'bn:02155551n', 'bn:00066152n', 'bn:00066151n', 'bn:01646319n', 'bn:00827444n', 'bn:02587718n'])\n(' -', 'conseiller', '(6)', ['bn:00023123n', 'bn:00023132n', 'bn:00391567n', 'bn:00001598n', 'bn:02181897n', 'bn:00001604n'])\n(' -', 'radio', '(9)', ['bn:00065899n', 'bn:00065901n', 'bn:02558854n', 'bn:01441457n', 'bn:00065900n', 'bn:01152589n', 'bn:02599193n', 'bn:01936146n', 'bn:02231112n'])\n"
     ]
    }
   ],
   "source": [
    "def parse_senses_file(file):\n",
    "    \"\"\"Parse the BabelNet senses contained in .senses file given by SemEval\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    file: file object\n",
    "        An open .senses file\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        A dictionnary mapping a lemma with it's BabelNet senses\n",
    "    \"\"\"\n",
    "    # BabelNet Sense Dictionnary\n",
    "    # bn_sens_dict[<lemma>] = [<senses>]\n",
    "    bn_sens_dict = {}\n",
    "\n",
    "    # Capture only BabelNet senses in the given senses file\n",
    "    for s in file.readlines():\n",
    "        splitted_line = re.split(\"\\s\", s)\n",
    "        \n",
    "        # Parse and get the lemma\n",
    "        lemma = splitted_line[0].split(\"#\")[0]\n",
    "        \n",
    "        # Parse BabelNet data\n",
    "        bn_num = int(splitted_line[2])\n",
    "        bn_senses = []\n",
    "        if bn_num > 0:\n",
    "            for i in range(bn_num):\n",
    "                bn_senses.append(splitted_line[3+i])\n",
    "        \n",
    "        # Parse WordNet data\n",
    "        #wn_num = int(splitted_line[3+bn_num])\n",
    "        #wn_senses = []\n",
    "        #if wn_num > 0:\n",
    "        #    for i in range(wn_num):\n",
    "        #        wn_senses.append(splitted_line[4+bn_num+i])\n",
    "        \n",
    "        # Parse Wikipedia data \n",
    "        #wiki_num = int(splitted_line[4+bn_num+wn_num])\n",
    "        #wiki_senses = []\n",
    "        #if wiki_num > 0:\n",
    "        #    for i in range(wiki_num):\n",
    "        #        wiki_senses.append(splitted_line[5+bn_num+wn_num+i])\n",
    "\n",
    "        bn_sens_dict[lemma] = bn_senses\n",
    "    \n",
    "    return bn_sens_dict\n",
    "\n",
    "senses_dict = parse_senses_file(open(sens_path))\n",
    "keys = list(senses_dict.keys())\n",
    "print(\"5 first entries in the BabelNet senses dictionnary\")\n",
    "for i in range(5):\n",
    "    print(\" -\", keys[i], \"(%d)\"%(len(senses_dict[keys[i]])), senses_dict[keys[i]])"
   ]
  },
  {
   "source": [
    "## Parser sur le fichier \"gold truth\""
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "5 first entries in the BabelNet gold truth dictionnary\n(' -', 'd001.s004.t001', '(1)', ['bn:00006997n'])\n(' -', 'd001.s004.t002', '(1)', ['bn:00007299n'])\n(' -', 'd001.s004.t003', '(1)', ['bn:00023471n'])\n(' -', 'd001.s004.t004', '(1)', ['bn:00053479n'])\n(' -', 'd001.s004.t005', '(1)', ['bn:00007140n'])\n"
     ]
    }
   ],
   "source": [
    "def parse_gold_truth(file):\n",
    "    \"\"\"Parse a SemEval gold truth (keys) file\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    file: file object\n",
    "        An open file\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        A dictionnary mapping a SemEval ids with their senses (according to the gold truth, an id can be link to multiple senses)\n",
    "    \"\"\"\n",
    "    # BabelNet gold truth of the corpus\n",
    "    bn_gt = {}\n",
    "\n",
    "    for line in file.readlines():\n",
    "        line = line.split()\n",
    "\n",
    "        id = line[1]\n",
    "        senses = []\n",
    "        i = 2\n",
    "        while i < len(line) and line[i] != \"!!\":\n",
    "            senses.append(line[i])\n",
    "            i+=1\n",
    "        bn_gt[id] = senses\n",
    "    return bn_gt\n",
    "\n",
    "gt_dict = parse_gold_truth(open(gold_truth_path))\n",
    "keys = list(gt_dict.keys())\n",
    "print(\"5 first entries in the BabelNet gold truth dictionnary\")\n",
    "for i in range(5):\n",
    "    print(\" -\", keys[i], \"(%d)\"%(len(gt_dict[keys[i]])), gt_dict[keys[i]])"
   ]
  },
  {
   "source": [
    "## Parser sur le corpus"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "def parse_corpus_file(file):\n",
    "    \"\"\"Parse a SemEval corpus\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    file\n",
    "        Either an open .xml SemEval corpus file or it's path\n",
    "    text_id: int\n",
    "        The index of the text to parse in the SemEval corpus starting from 0\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        list of tuple, each one contains a str (the sentence where elements are separated by a space) and a list of tuples mapping the SemEval id of the word (dxxx.sxxx.txxx), the index of the lemma in the sentence (index start from 0) and the lemma itself\n",
    "    \"\"\"\n",
    "\n",
    "    DOMTree = parse(file)\n",
    "    corpus = DOMTree.documentElement\n",
    "    \n",
    "    documents = []\n",
    "\n",
    "    # Iterate through the different documents (text markers in the SemEval corpora)    \n",
    "    for t in corpus.getElementsByTagName(\"text\"):\n",
    "        sentences = []\n",
    "        polysems = []\n",
    "        # Iterate through the sentences in each documents\n",
    "        for s in t.getElementsByTagName(\"sentence\"):\n",
    "            idx = 0\n",
    "            sentence = \"\"\n",
    "            polysem = []\n",
    "            for n in s.childNodes:\n",
    "                if n.nodeName == \"wf\":\n",
    "                    sentence += n.childNodes[0].data + \" \"\n",
    "                    idx += 1\n",
    "                if n.nodeName == \"instance\":\n",
    "                    lemma = n.getAttribute(\"lemma\")\n",
    "                    id = n.getAttribute(\"id\")\n",
    "                    sentence += n.childNodes[0].data + \" \"\n",
    "                    polysem.append((id, idx, lemma))\n",
    "                    idx += 1\n",
    "            sentences.append(sentence)\n",
    "            polysems.append(polysem)\n",
    "    \n",
    "        documents.append((sentences, polysems))\n",
    "\n",
    "    return documents\n",
    "    \n",
    "\n",
    "d = parse_corpus_file(corpus_path)\n",
    "print(\"Documents in the corpus:\", len(d))\n",
    "print()\n",
    "for i, (s,_) in enumerate(d):\n",
    "    print(\"Document %d: %d sentences\" % (i, len(s)))"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 11,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "('Documents in the corpus:', 1)\n()\nDocument 0: 36 sentences\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "unknow = []\n",
    "\n",
    "def parse_data(semeval_corpus_path, semeval_gt_path, output_path):\n",
    "    \"\"\"Parse the given SemEval data to a new xml.\n",
    "    All given path has to exist\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    semeval_corpus_path: str\n",
    "        Path to a SemEval .xml corpus file\n",
    "    semeval_gt_path: str\n",
    "        Path to a SemEval keys fils (related to the previous given one and preferably a BabelNet file : keys-bn)\n",
    "    output_path: str\n",
    "        Path to the output xml file, will contain the newly formated SemEval corpus\n",
    "    \"\"\"\n",
    "\n",
    "    documents = parse_corpus_file(semeval_corpus_path)\n",
    "    gt_dict = parse_gold_truth(open(semeval_gt_path))\n",
    "\n",
    "    root = et.Element(\"corpus\")\n",
    "\n",
    "    for d, (sentences, polysems) in enumerate(documents):\n",
    "        document = et.SubElement(root, \"document\", {\"id\":str(d)})\n",
    "        for i in range(len(sentences)):\n",
    "            sentence = et.SubElement(document, \"sentence\", {\n",
    "                \"id\":str(i), \"s\": sentences[i]\n",
    "            })\n",
    "            for p, (id, idx, lemma) in enumerate(polysems[i]):\n",
    "                if id in gt_dict:\n",
    "                    lemma = et.SubElement(sentence, \"lemma\", {\n",
    "                        \"id\" : str(p),\n",
    "                        \"idx\" : str(idx), \n",
    "                        \"lemma\" : lemma,\n",
    "                        \"senses\" : \" \".join(gt_dict[id])\n",
    "                    })\n",
    "                else:\n",
    "                    unknow.append(id)\n",
    "\n",
    "    with open(output_path, \"wb\") as f:\n",
    "        f.write(parseString(et.tostring(root)).toprettyxml(encoding=\"UTF-8\"))"
   ]
  },
  {
   "source": [
    "## Parsing des donn√©es de dev"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "('unknow lemma:', 0)\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n\n<corpus>\n\n\t<document id=\"0\">\n\n\t\t<sentence id=\"0\" s=\"Nous ne savons pas qui gagnera la guerre_contre_la_drogue en Am√©rique_Latine , mais nous savons qui est en train de la perdre - la presse . \">\n\n\t\t\t<lemma id=\"0\" idx=\"7\" lemma=\"guerre_contre_la_drogue\" senses=\"bn:00028885n\"/>\n\n\t\t\t<lemma id=\"1\" idx=\"9\" lemma=\"Am√©rique_Latine\" senses=\"bn:00050165n\"/>\n\n\t\t\t<lemma id=\"2\" idx=\"23\" lemma=\"presse\" senses=\"bn:00064245n\"/>\n\n\t\t</sentence>\n\n\t\t<sentence id=\"1\" s=\"Au cours des six derniers mois , six journalistes ont √©t√© tu√©s et 10 ont √©t√© enlev√©s par des trafiquants_de_drogue ou des gu√©rilleros de gauche - souvent il s ‚Äô agit des m√™mes personnes - en Colombie . \">\n\n\t\t\t<lemma id=\"0\" idx=\"5\" lemma=\"mois\" senses=\"bn:00014710n\"/>\n\n(...)\n"
     ]
    }
   ],
   "source": [
    "parse_data(corpus_path, gold_truth_path, output_corpus_path)\n",
    "print(\"unknow lemma:\", len(unknow))\n",
    "\n",
    "f = open(output_corpus_path)\n",
    "for i in range(10):\n",
    "    print(f.readline())\n",
    "print(\"(...)\")"
   ]
  },
  {
   "source": [
    "## Parsing des donn√©es de test"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "('unknow lemma:', 226)\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n\n<corpus>\n\n\t<document id=\"0\">\n\n\t\t<sentence id=\"0\" s=\"Le groupe des Nations_Unies a des projets de plans pour la r√©duction des √©missions \">\n\n\t\t\t<lemma id=\"0\" idx=\"1\" lemma=\"groupe\" senses=\"bn:00041942n\"/>\n\n\t\t\t<lemma id=\"1\" idx=\"3\" lemma=\"nations_unies\" senses=\"bn:00078931n\"/>\n\n\t\t\t<lemma id=\"3\" idx=\"8\" lemma=\"plan\" senses=\"bn:00062759n\"/>\n\n\t\t\t<lemma id=\"4\" idx=\"11\" lemma=\"r√©duction\" senses=\"bn:00025780n\"/>\n\n\t\t\t<lemma id=\"5\" idx=\"13\" lemma=\"√©mission\" senses=\"bn:00030455n\"/>\n\n\t\t</sentence>\n\n(...)\n"
     ]
    }
   ],
   "source": [
    "test_corpus_path = \"test/data/multilingual-all-words.fr.xml\"\n",
    "test_gt_path = \"test/keys/gold/babelnet/babelnet.fr.key\"\n",
    "output_path = \"test_corpus.xml\"\n",
    "\n",
    "parse_data(test_corpus_path, test_gt_path, output_path)\n",
    "print(\"unknow lemma:\", len(unknow))\n",
    "\n",
    "f = open(output_path)\n",
    "for i in range(10):\n",
    "    print(f.readline())\n",
    "print(\"(...)\")"
   ]
  },
  {
   "source": [
    "## Analyse donn√©es BabelNet"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib2\n",
    "import urllib\n",
    "import json\n",
    "import gzip\n",
    "\n",
    "from StringIO import StringIO\n",
    "\n",
    "key1 = \"\"\n",
    "key2 = \"\"\n",
    "key=key1"
   ]
  },
  {
   "source": [
    "Interoge l'API BabelNet pour r√©cup√©rer les ids associ√©s √† un lemme donn√©."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemma2ids(lemma):\n",
    "    ids = []\n",
    "    \n",
    "    service_url = 'https://babelnet.io/v5/getSynsetIds'\n",
    "\n",
    "    params = {\n",
    "            'lemma' : lemma,\n",
    "            'searchLang' : \"FR\",\n",
    "            'key'  : key\n",
    "    }\n",
    "\n",
    "    url = service_url + '?' + urllib.urlencode(params)\n",
    "    request = urllib2.Request(url)\n",
    "    request.add_header('Accept-encoding', 'gzip')\n",
    "    response = urllib2.urlopen(request)\n",
    "\n",
    "    if response.info().get('Content-Encoding') == 'gzip':\n",
    "            buf = StringIO( response.read())\n",
    "            f = gzip.GzipFile(fileobj=buf)\n",
    "            data = json.loads(f.read())\n",
    "            ids = [str(res[\"id\"]) for res in data]\n",
    "    \n",
    "    return ids\n",
    "\n",
    "#lemma2ids(\"apple\")\n",
    "#['bn:00289737n', 'bn:03739345n', 'bn:00955003n', 'bn:00512973n']"
   ]
  },
  {
   "source": [
    "Interoge l'API BabelNet pour r√©cuperer la d√©finition associ√©e √† un id."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def id2glosses(id):\n",
    "    res = []\n",
    "\n",
    "    service_url = 'https://babelnet.io/v5/getSynset'\n",
    "\n",
    "    params = {\n",
    "        'id' : id,\n",
    "        'targetLang' : 'FR',\n",
    "        'key'  : key\n",
    "    }\n",
    "\n",
    "    url = service_url + '?' + urllib.urlencode(params)\n",
    "    request = urllib2.Request(url)\n",
    "    request.add_header('Accept-encoding', 'gzip')\n",
    "    response = urllib2.urlopen(request)\n",
    "\n",
    "    if response.info().get('Content-Encoding') == 'gzip':\n",
    "        buf = StringIO( response.read())\n",
    "        f = gzip.GzipFile(fileobj=buf)\n",
    "        data = json.loads(f.read())\n",
    "\n",
    "        # retrieving BabelSense data\n",
    "        #senses = data['senses']\n",
    "        #for result in senses:\n",
    "        #    lemma = result[\"properties\"].get('fullLemma')\n",
    "        #    language = result[\"properties\"].get('language')\n",
    "        #    print language.encode('utf-8') + \"\\t\" + str(lemma.encode('utf-8'))\n",
    "\n",
    "        # retrieving BabelGloss data\n",
    "        glosses = data['glosses']\n",
    "        for result in glosses:\n",
    "            gloss = result.get('gloss')\n",
    "            res.append(gloss)\n",
    "    \n",
    "    return res\n",
    "\n",
    "#for g in id2glosses(\"bn:00015540n\"):\n",
    "#    print(g)\n",
    "#Paris est la capitale de la France.\n",
    "#La Rue de l'Abb√©-de-l'√âp√©e jouxte l'Institut des Jeunes sourds.\n",
    "#Capitale de la France\n",
    "#Capitale et plus grosse ville de France.\n",
    "#Paris est une ville fran√ßaise, capitale de la France et le chef-lieu de la r√©gion d'√éle-de-France."
   ]
  },
  {
   "source": [
    "Produit un dictionnaire des d√©finitions des mots polysemiques du corpus de test"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "50 lemmas in the dictionnary\n",
      "mardi\n",
      "dollar\n",
      "rang\n",
      "conseiller\n",
      "radio\n",
      "gu√©rilla\n",
      "kilo\n",
      "d√©claration\n",
      "famille\n",
      "semaine\n",
      "el_spectador\n",
      "√©tat\n",
      "h√©misph√®re\n",
      "domicile\n",
      "manuel_noriega\n",
      "presse\n",
      "coup_d'√©tat\n",
      "indignation\n",
      "extradition\n",
      "cuba\n",
      "massacre\n",
      "personne\n",
      "jaime_paz_zamora\n",
      "courage\n",
      "probl√®me\n",
      "libert√©\n",
      "d√©mocratie\n",
      "trafiquant_de_drogue\n",
      "medellin\n",
      "guerre_contre_la_drogue\n",
      "relation\n",
      "david_asman\n",
      "el\n",
      "autorit√©\n",
      "t√©l√©vision\n",
      "bogota\n",
      "aide\n",
      "fr√®re\n",
      "trait√©\n",
      "nicaragua\n",
      "homme\n",
      "canada\n",
      "panama\n",
      "exil\n",
      "ordinateur\n",
      "√©ditorial\n",
      "bureau\n",
      "gouvernement\n",
      "cible\n",
      "r√©dacteur\n",
      "meurtre\n",
      "\tadding it\n",
      "journal\n",
      "\tadding it\n",
      "million\n",
      "\tadding it\n",
      "narcotrafic\n",
      "\tadding it\n",
      "directeur\n",
      "\tadding it\n",
      "jose_abello_silva\n",
      "\tadding it\n",
      "jour\n",
      "\tadding it\n",
      "m√®re\n",
      "\tadding it\n",
      "voiture\n",
      "\tadding it\n",
      "acte_d'accusation\n",
      "\tadding it\n",
      "bataillon\n",
      "\tadding it\n",
      "instant\n",
      "\tadding it\n",
      "forme\n",
      "\tadding it\n",
      "floride\n",
      "\tadding it\n",
      "pouvoir\n",
      "\tadding it\n",
      "hier\n",
      "\tadding it\n",
      "ann√©e\n",
      "\tadding it\n",
      "employ√©\n",
      "\tadding it\n",
      "pays\n",
      "\tadding it\n",
      "colombie\n",
      "\tadding it\n",
      "besoin\n",
      "\tadding it\n",
      "trafiquant\n",
      "\tadding it\n",
      "ao√ªt\n",
      "\tadding it\n",
      "monterrey\n",
      "\tadding it\n",
      "attaque\n",
      "\tadding it\n",
      "heure\n",
      "\tadding it\n",
      "mort\n",
      "\tadding it\n",
      "station\n",
      "\tadding it\n",
      "leonidas_vargas\n",
      "\tadding it\n",
      "dynamite\n",
      "\tadding it\n",
      "r√©sultat\n",
      "\tadding it\n",
      "cartel_de_la_drogue\n",
      "\tadding it\n",
      "bolivie\n",
      "\tadding it\n",
      "r√©pression\n",
      "\tadding it\n",
      "mois\n",
      "\tadding it\n",
      "mexique\n",
      "\tadding it\n",
      "fonctionnaire\n",
      "\tadding it\n",
      "gu√©rillero\n",
      "\tadding it\n",
      "journaliste\n",
      "\tadding it\n",
      "am√©rique_latine\n",
      "\tadding it\n",
      "liste\n",
      "\tadding it\n",
      "conf√©rence\n",
      "\tadding it\n",
      "gauche\n",
      "\tadding it\n",
      "castro\n",
      "\tadding it\n",
      "pr√©sident\n",
      "\tadding it\n",
      "sandinistes\n",
      "\tadding it\n",
      "am√©riques\n",
      "\tadding it\n",
      "etats-unis\n",
      "\tadding it\n",
      "p√©rou\n",
      "\tadding it\n",
      "politique\n",
      "\tadding it\n",
      "cara√Øbes\n",
      "\tadding it\n",
      "el_espectador\n",
      "\tadding it\n",
      "assassinat\n",
      "\tadding it\n",
      "alan_garcia\n",
      "\tadding it\n",
      "seigneur_de_la_drogue\n",
      "\tadding it\n",
      "cartel\n",
      "\tadding it\n",
      "participant\n",
      "\tadding it\n",
      "personnel\n",
      "\tadding it\n",
      "progr√®s\n",
      "\tadding it\n",
      "rapport\n",
      "\tadding it\n",
      "n√©gociation\n",
      "\tadding it\n",
      "drogue\n",
      "\tadding it\n",
      "procureur\n",
      "\tadding it\n",
      "√©diteur\n",
      "\tadding it\n",
      "feu_rouge\n",
      "\tadding it\n",
      "contr√¥le\n",
      "\tadding it\n",
      "censure\n",
      "\tadding it\n",
      "proc√®s\n",
      "\tadding it\n",
      "tomas_borge\n",
      "\tadding it\n",
      "troupe\n",
      "\tadding it\n",
      "trafic_de_drogue\n",
      "\tadding it\n",
      "lutte\n",
      "\tadding it\n",
      "ortega\n",
      "\tadding it\n",
      "fiasco\n",
      "\tadding it\n"
     ]
    }
   ],
   "source": [
    "with open(\"dict.dictionary\", \"r+\") as file:\n",
    "    # look for the lemmas allready in the dictionary file\n",
    "    # no need to consume babelnet coin for a lemma we know\n",
    "    reader = csv.reader(file, delimiter=\";\")\n",
    "    alreadyin = [row[0] for row in reader]\n",
    "    print len(alreadyin)-1, \"lemmas in the dictionnary\"\n",
    "    \n",
    "    \n",
    "    writer = csv.writer(file, delimiter=\";\")\n",
    "    \n",
    "    if len(alreadyin) <= 1:\n",
    "        writer.writerow([\"lemma\", \"nb_senses\", \"BN_senses\", \"definitions\"])\n",
    "    \n",
    "    for lemma,senses in senses_dict.items():\n",
    "        print lemma\n",
    "        if not lemma in alreadyin:\n",
    "            print \"\\tadding it\"\n",
    "            ids_lemma = lemma2ids(lemma)\n",
    "            \n",
    "            defs_lemma = []\n",
    "            for id in ids_lemma:\n",
    "                try:\n",
    "                    defs_lemma.append(id2glosses(id))\n",
    "                except urllib2.HTTPError:\n",
    "                    defs_lemma.append([])\n",
    "            \n",
    "            ids = []\n",
    "            defs = []\n",
    "            for i in range(len(defs_lemma)):\n",
    "                # Iterate through the definitions\n",
    "                # if the definition is empty : pass\n",
    "                # else append the id and definition\n",
    "                if defs_lemma[i]:\n",
    "                    ids.append(ids_lemma[i])\n",
    "                    defs.append(defs_lemma[i])\n",
    "            # append the new lemma to the file\n",
    "            file.write(lemma + \";\" +\n",
    "                str(len(ids)) + \";\" +\n",
    "                (\",\".join(ids)) + \";\" +\n",
    "                (\",\".join([\"\\\"\" + (\" \".join(d)).encode(\"utf-8\") + \"\\\"\" for d in defs])) + \"\\n\")\n",
    "            alreadyin.append(lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}