{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "2.7.17-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python271764bitfc26ea4e491d46f7ba4683f54ef403ba",
   "display_name": "Python 2.7.17 64-bit",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Parse les données d'apprentissage et de test de SemEval"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "from xml.dom.minidom import parseString\n",
    "from xml.dom.minidom import parse\n",
    "import xml.etree.ElementTree as et\n",
    "\n",
    "sens_path = \"trial/data/multilingual-all-words.fr.senses\"\n",
    "gold_truth_path = \"trial/keys/keys-bn.fr\"\n",
    "corpus_path = \"trial/data/multilingual-all-words.fr.xml\"\n",
    "\n",
    "output_corpus_path = \"trial_corpus.xml\""
   ]
  },
  {
   "source": [
    "## Fournis un parser sur le fichier de sens SemEval (fichiers au format *.senses)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "5 first entries in the BabelNet senses dictionnary\n(' -', 'mardi', '(6)', ['bn:01718496n', 'bn:01502893n', 'bn:01433525n', 'bn:02861587n', 'bn:00078546n', 'bn:00807619n'])\n(' -', 'dollar', '(10)', ['bn:00028114n', 'bn:02156595n', 'bn:01827683n', 'bn:00008375n', 'bn:00015129n', 'bn:02122502n', 'bn:00028118n', 'bn:00028116n', 'bn:02897671n', 'bn:01524928n'])\n(' -', 'rang', '(10)', ['bn:00054281n', 'bn:01743576n', 'bn:02256034n', 'bn:00073933n', 'bn:02155551n', 'bn:00066152n', 'bn:00066151n', 'bn:01646319n', 'bn:00827444n', 'bn:02587718n'])\n(' -', 'conseiller', '(6)', ['bn:00023123n', 'bn:00023132n', 'bn:00391567n', 'bn:00001598n', 'bn:02181897n', 'bn:00001604n'])\n(' -', 'radio', '(9)', ['bn:00065899n', 'bn:00065901n', 'bn:02558854n', 'bn:01441457n', 'bn:00065900n', 'bn:01152589n', 'bn:02599193n', 'bn:01936146n', 'bn:02231112n'])\n"
     ]
    }
   ],
   "source": [
    "def parse_senses_file(file):\n",
    "    \"\"\"Parse the BabelNet senses contained in .senses file given by SemEval\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    file: file object\n",
    "        An open .senses file\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        A dictionnary mapping a lemma with it's BabelNet senses\n",
    "    \"\"\"\n",
    "    # BabelNet Sense Dictionnary\n",
    "    # bn_sens_dict[<lemma>] = [<senses>]\n",
    "    bn_sens_dict = {}\n",
    "\n",
    "    # Capture only BabelNet senses in the given senses file\n",
    "    for s in file.readlines():\n",
    "        splitted_line = re.split(\"\\s\", s)\n",
    "        \n",
    "        # Parse and get the lemma\n",
    "        lemma = splitted_line[0].split(\"#\")[0]\n",
    "        \n",
    "        # Parse BabelNet data\n",
    "        bn_num = int(splitted_line[2])\n",
    "        bn_senses = []\n",
    "        if bn_num > 0:\n",
    "            for i in range(bn_num):\n",
    "                bn_senses.append(splitted_line[3+i])\n",
    "        \n",
    "        # Parse WordNet data\n",
    "        #wn_num = int(splitted_line[3+bn_num])\n",
    "        #wn_senses = []\n",
    "        #if wn_num > 0:\n",
    "        #    for i in range(wn_num):\n",
    "        #        wn_senses.append(splitted_line[4+bn_num+i])\n",
    "        \n",
    "        # Parse Wikipedia data \n",
    "        #wiki_num = int(splitted_line[4+bn_num+wn_num])\n",
    "        #wiki_senses = []\n",
    "        #if wiki_num > 0:\n",
    "        #    for i in range(wiki_num):\n",
    "        #        wiki_senses.append(splitted_line[5+bn_num+wn_num+i])\n",
    "\n",
    "        bn_sens_dict[lemma] = bn_senses\n",
    "    \n",
    "    return bn_sens_dict\n",
    "\n",
    "senses_dict = parse_senses_file(open(sens_path))\n",
    "keys = list(senses_dict.keys())\n",
    "print(\"5 first entries in the BabelNet senses dictionnary\")\n",
    "for i in range(5):\n",
    "    print(\" -\", keys[i], \"(%d)\"%(len(senses_dict[keys[i]])), senses_dict[keys[i]])"
   ]
  },
  {
   "source": [
    "## Fournis un parser sur le fichier \"gold truth\" de SemEval (fichiers \"key\")"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "5 first entries in the BabelNet gold truth dictionnary\n(' -', 'd001.s004.t001', '(1)', ['bn:00006997n'])\n(' -', 'd001.s004.t002', '(1)', ['bn:00007299n'])\n(' -', 'd001.s004.t003', '(1)', ['bn:00023471n'])\n(' -', 'd001.s004.t004', '(1)', ['bn:00053479n'])\n(' -', 'd001.s004.t005', '(1)', ['bn:00007140n'])\n"
     ]
    }
   ],
   "source": [
    "def parse_gold_truth(file):\n",
    "    \"\"\"Parse a SemEval gold truth (keys) file\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    file: file object\n",
    "        An open file\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        A dictionnary mapping a SemEval ids with their senses (according to the gold truth, an id can be link to multiple senses)\n",
    "    \"\"\"\n",
    "    # BabelNet gold truth of the corpus\n",
    "    bn_gt = {}\n",
    "\n",
    "    for line in file.readlines():\n",
    "        line = line.split()\n",
    "\n",
    "        id = line[1]\n",
    "        senses = []\n",
    "        i = 2\n",
    "        while i < len(line) and line[i] != \"!!\":\n",
    "            senses.append(line[i])\n",
    "            i+=1\n",
    "        bn_gt[id] = senses\n",
    "    return bn_gt\n",
    "\n",
    "gt_dict = parse_gold_truth(open(gold_truth_path))\n",
    "keys = list(gt_dict.keys())\n",
    "print(\"5 first entries in the BabelNet gold truth dictionnary\")\n",
    "for i in range(5):\n",
    "    print(\" -\", keys[i], \"(%d)\"%(len(gt_dict[keys[i]])), gt_dict[keys[i]])"
   ]
  },
  {
   "source": [
    "## Fournis une méthode pour parser un corpus SemEval au format .xml"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "def parse_corpus_file(file):\n",
    "    \"\"\"Parse a SemEval corpus\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    file\n",
    "        Either an open .xml SemEval corpus file or it's path\n",
    "    text_id: int\n",
    "        The index of the text to parse in the SemEval corpus starting from 0\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        list of tuple, each one contains a str (the sentence where elements are separated by a space) and a list of tuples mapping the SemEval id of the word (dxxx.sxxx.txxx), the index of the lemma in the sentence (index start from 0) and the lemma itself\n",
    "    \"\"\"\n",
    "\n",
    "    DOMTree = parse(file)\n",
    "    corpus = DOMTree.documentElement\n",
    "    \n",
    "    documents = []\n",
    "\n",
    "    # Iterate through the different documents (text markers in the SemEval corpora)    \n",
    "    for t in corpus.getElementsByTagName(\"text\"):\n",
    "        sentences = []\n",
    "        polysems = []\n",
    "        # Iterate through the sentences in each documents\n",
    "        for s in t.getElementsByTagName(\"sentence\"):\n",
    "            idx = 0\n",
    "            sentence = \"\"\n",
    "            polysem = []\n",
    "            for n in s.childNodes:\n",
    "                if n.nodeName == \"wf\":\n",
    "                    sentence += n.childNodes[0].data + \" \"\n",
    "                    idx += 1\n",
    "                if n.nodeName == \"instance\":\n",
    "                    lemma = n.getAttribute(\"lemma\")\n",
    "                    id = n.getAttribute(\"id\")\n",
    "                    sentence += n.childNodes[0].data + \" \"\n",
    "                    polysem.append((id, idx, lemma))\n",
    "                    idx += 1\n",
    "            sentences.append(sentence)\n",
    "            polysems.append(polysem)\n",
    "    \n",
    "        documents.append((sentences, polysems))\n",
    "\n",
    "    return documents\n",
    "    \n",
    "\n",
    "d = parse_corpus_file(corpus_path)\n",
    "print(\"Documents in the corpus:\", len(d))\n",
    "print()\n",
    "for i, (s,_) in enumerate(d):\n",
    "    print(\"Document %d: %d sentences\" % (i, len(s)))"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "('Documents in the corpus:', 1)\n()\nDocument 0: 36 sentences\n"
     ]
    }
   ]
  },
  {
   "source": [
    "## Fournis une méthode pour convertir un corpus SemEval vers un format plus pratique"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "unknow = []\n",
    "\n",
    "def parse_data(semeval_corpus_path, semeval_gt_path, output_path):\n",
    "    \"\"\"Parse the given SemEval data to a new xml.\n",
    "    All given path has to exist\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    semeval_corpus_path: str\n",
    "        Path to a SemEval .xml corpus file\n",
    "    semeval_gt_path: str\n",
    "        Path to a SemEval keys fils (related to the previous given one and preferably a BabelNet file : keys-bn)\n",
    "    output_path: str\n",
    "        Path to the output xml file, will contain the newly formated SemEval corpus\n",
    "    \"\"\"\n",
    "\n",
    "    documents = parse_corpus_file(semeval_corpus_path)\n",
    "    gt_dict = parse_gold_truth(open(semeval_gt_path))\n",
    "\n",
    "    root = et.Element(\"corpus\")\n",
    "\n",
    "    for d, (sentences, polysems) in enumerate(documents):\n",
    "        document = et.SubElement(root, \"document\", {\"id\":str(d)})\n",
    "        for i in range(len(sentences)):\n",
    "            sentence = et.SubElement(document, \"sentence\", {\n",
    "                \"id\":str(i), \"s\": sentences[i]\n",
    "            })\n",
    "            for p, (id, idx, lemma) in enumerate(polysems[i]):\n",
    "                if id in gt_dict:\n",
    "                    lemma = et.SubElement(sentence, \"lemma\", {\n",
    "                        \"id\" : str(p),\n",
    "                        \"idx\" : str(idx), \n",
    "                        \"lemma\" : lemma,\n",
    "                        \"senses\" : \" \".join(gt_dict[id])\n",
    "                    })\n",
    "                else:\n",
    "                    unknow.append(id)\n",
    "\n",
    "    with open(output_path, \"wb\") as f:\n",
    "        f.write(parseString(et.tostring(root)).toprettyxml(encoding=\"UTF-8\"))"
   ]
  },
  {
   "source": [
    "## Parse les données de Dev de SemEval vers un nouveau fichier : trial_corpus.xml"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "('unknow lemma:', 0)\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n\n<corpus>\n\n\t<document id=\"0\">\n\n\t\t<sentence id=\"0\" s=\"Nous ne savons pas qui gagnera la guerre_contre_la_drogue en Amérique_Latine , mais nous savons qui est en train de la perdre - la presse . \">\n\n\t\t\t<lemma id=\"0\" idx=\"7\" lemma=\"guerre_contre_la_drogue\" senses=\"bn:00028885n\"/>\n\n\t\t\t<lemma id=\"1\" idx=\"9\" lemma=\"Amérique_Latine\" senses=\"bn:00050165n\"/>\n\n\t\t\t<lemma id=\"2\" idx=\"23\" lemma=\"presse\" senses=\"bn:00064245n\"/>\n\n\t\t</sentence>\n\n\t\t<sentence id=\"1\" s=\"Au cours des six derniers mois , six journalistes ont été tués et 10 ont été enlevés par des trafiquants_de_drogue ou des guérilleros de gauche - souvent il s ’ agit des mêmes personnes - en Colombie . \">\n\n\t\t\t<lemma id=\"0\" idx=\"5\" lemma=\"mois\" senses=\"bn:00014710n\"/>\n\n(...)\n"
     ]
    }
   ],
   "source": [
    "parse_data(corpus_path, gold_truth_path, output_corpus_path)\n",
    "print(\"unknow lemma:\", len(unknow))\n",
    "\n",
    "f = open(output_corpus_path)\n",
    "for i in range(10):\n",
    "    print(f.readline())\n",
    "print(\"(...)\")"
   ]
  },
  {
   "source": [
    "## Parse les données de Test de SemEval vers un nouveau fichier : test_corpus.xml"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "('unknow lemma:', 226)\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n\n<corpus>\n\n\t<document id=\"0\">\n\n\t\t<sentence id=\"0\" s=\"Le groupe des Nations_Unies a des projets de plans pour la réduction des émissions \">\n\n\t\t\t<lemma id=\"0\" idx=\"1\" lemma=\"groupe\" senses=\"bn:00041942n\"/>\n\n\t\t\t<lemma id=\"1\" idx=\"3\" lemma=\"nations_unies\" senses=\"bn:00078931n\"/>\n\n\t\t\t<lemma id=\"3\" idx=\"8\" lemma=\"plan\" senses=\"bn:00062759n\"/>\n\n\t\t\t<lemma id=\"4\" idx=\"11\" lemma=\"réduction\" senses=\"bn:00025780n\"/>\n\n\t\t\t<lemma id=\"5\" idx=\"13\" lemma=\"émission\" senses=\"bn:00030455n\"/>\n\n\t\t</sentence>\n\n(...)\n"
     ]
    }
   ],
   "source": [
    "test_corpus_path = \"test/data/multilingual-all-words.fr.xml\"\n",
    "test_gt_path = \"test/keys/gold/babelnet/babelnet.fr.key\"\n",
    "output_path = \"test_corpus.xml\"\n",
    "\n",
    "parse_data(test_corpus_path, test_gt_path, output_path)\n",
    "print(\"unknow lemma:\", len(unknow))\n",
    "\n",
    "f = open(output_path)\n",
    "for i in range(10):\n",
    "    print(f.readline())\n",
    "print(\"(...)\")"
   ]
  },
  {
   "source": [
    "# Analyse les données BabelNet\n",
    "ATTENTION : Les Librairies BabelNet necessitent l'utilisation de python 2"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "import urllib2\n",
    "import urllib\n",
    "import json\n",
    "import gzip\n",
    "\n",
    "from StringIO import StringIO\n",
    "\n",
    "key=\"\""
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 8,
   "outputs": []
  },
  {
   "source": [
    "## Fournis une méthode pour convertir un lemme en un ensemble d'id BabelNet"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemma2ids(lemma):\n",
    "    ids = []\n",
    "    \n",
    "    service_url = 'https://babelnet.io/v5/getSynsetIds'\n",
    "\n",
    "    params = {\n",
    "            'lemma' : lemma,\n",
    "            'searchLang' : \"FR\",\n",
    "            'key'  : key\n",
    "    }\n",
    "\n",
    "    url = service_url + '?' + urllib.urlencode(params)\n",
    "    request = urllib2.Request(url)\n",
    "    request.add_header('Accept-encoding', 'gzip')\n",
    "    response = urllib2.urlopen(request)\n",
    "\n",
    "    if response.info().get('Content-Encoding') == 'gzip':\n",
    "            buf = StringIO( response.read())\n",
    "            f = gzip.GzipFile(fileobj=buf)\n",
    "            data = json.loads(f.read())\n",
    "            ids = [str(res[\"id\"]) for res in data]\n",
    "    \n",
    "    return ids\n",
    "\n",
    "#lemma2ids(\"apple\")\n",
    "\n",
    "# OUT :\n",
    "#['bn:00289737n', 'bn:03739345n', 'bn:00955003n', 'bn:00512973n']"
   ]
  },
  {
   "source": [
    "## Fournis une méthode pour récuperer la définition associée à une id BabelNet"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def id2glosses(id):\n",
    "    res = []\n",
    "\n",
    "    service_url = 'https://babelnet.io/v5/getSynset'\n",
    "\n",
    "    params = {\n",
    "        'id' : id,\n",
    "        'targetLang' : 'FR',\n",
    "        'key'  : key\n",
    "    }\n",
    "\n",
    "    url = service_url + '?' + urllib.urlencode(params)\n",
    "    request = urllib2.Request(url)\n",
    "    request.add_header('Accept-encoding', 'gzip')\n",
    "    response = urllib2.urlopen(request)\n",
    "\n",
    "    if response.info().get('Content-Encoding') == 'gzip':\n",
    "        buf = StringIO( response.read())\n",
    "        f = gzip.GzipFile(fileobj=buf)\n",
    "        data = json.loads(f.read())\n",
    "\n",
    "        # retrieving BabelSense data\n",
    "        #senses = data['senses']\n",
    "        #for result in senses:\n",
    "        #    lemma = result[\"properties\"].get('fullLemma')\n",
    "        #    language = result[\"properties\"].get('language')\n",
    "        #    print language.encode('utf-8') + \"\\t\" + str(lemma.encode('utf-8'))\n",
    "\n",
    "        # retrieving BabelGloss data\n",
    "        glosses = data['glosses']\n",
    "        for result in glosses:\n",
    "            gloss = result.get('gloss')\n",
    "            res.append(gloss)\n",
    "    \n",
    "    return res\n",
    "\n",
    "#for g in id2glosses(\"bn:00015540n\"):\n",
    "#    print(g)\n",
    "\n",
    "# OUT :\n",
    "#Paris est la capitale de la France.\n",
    "#La Rue de l'Abbé-de-l'Épée jouxte l'Institut des Jeunes sourds.\n",
    "#Capitale de la France\n",
    "#Capitale et plus grosse ville de France.\n",
    "#Paris est une ville française, capitale de la France et le chef-lieu de la région d'Île-de-France."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capture all the polysemous word to desambiguate\n",
    "# in the test and trial corpora\n",
    "words = []\n",
    "\n",
    "for _,d in parse_corpus_file(test_corpus_path):\n",
    "    for s in d:\n",
    "        for _,_,w in s:\n",
    "            words.append(w.encode(\"utf-8\"))\n",
    "for _,d in parse_corpus_file(corpus_path):\n",
    "    for s in d:\n",
    "        for _,_,w in s:\n",
    "            words.append(w.encode(\"utf-8\"))\n",
    "\n",
    "words = set(words)"
   ]
  },
  {
   "source": [
    "## Produit un dictionnaire des définitions des mots polysemiques des corpus de dev et test de SemEval.\n",
    "Crée un dictionnaire qui est le fichier : dict.dictionary"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "976/959 lemmas in the dictionary\n",
      "977/959\n"
     ]
    }
   ],
   "source": [
    "# BabelNet API key\n",
    "key = \"eda2e938-8518-40c9-b46f-4cdb0005e6cd\"\n",
    "\n",
    "# Update the dictionary file\n",
    "with open(\"dict.dictionary\", \"a+\") as file:\n",
    "    # look for the lemmas allready in the dictionary file\n",
    "    # no need to consume babelnet coin for a lemma we know\n",
    "    reader = csv.reader(file, delimiter=\";\")\n",
    "    alreadyin = [row[0] for row in reader]\n",
    "    print \"%d/%d lemmas in the dictionary\"%(len(alreadyin)-1, len(words))\n",
    "    \n",
    "    writer = csv.writer(file, delimiter=\";\")\n",
    "    \n",
    "    if len(alreadyin) <= 1:\n",
    "        writer.writerow([\"lemma\", \"nb_senses\", \"BN_senses\", \"definitions\"])\n",
    "    \n",
    "    for lemma in words:\n",
    "        print \"\\r%3d/%d\"%(len(alreadyin)-1, len(words)),\n",
    "\n",
    "        if not lemma in alreadyin:\n",
    "\n",
    "            ids_lemma = lemma2ids(lemma)\n",
    "            \n",
    "            defs_lemma = []\n",
    "            for id in ids_lemma:\n",
    "                try:\n",
    "                    defs_lemma.append(id2glosses(id))\n",
    "                except urllib2.HTTPError:\n",
    "                    defs_lemma.append([])\n",
    "            \n",
    "            ids = []\n",
    "            defs = []\n",
    "            for i in range(len(defs_lemma)):\n",
    "                # Iterate through the definitions\n",
    "                # if the definition is empty : pass\n",
    "                # else append the id and definition\n",
    "                if defs_lemma[i]:\n",
    "                    ids.append(ids_lemma[i])\n",
    "                    defs.append(defs_lemma[i])\n",
    "            # append the new lemma to the file\n",
    "            file.write(lemma + \";\" +\n",
    "                str(len(ids)) + \";\" +\n",
    "                (\",\".join(ids)) + \";\" +\n",
    "                (\",\".join([\"\\\"\" + \" \".join(d).encode(\"utf-8\") + \"\\\"\" for d in defs])) + \"\\n\")\n",
    "            alreadyin.append(lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}