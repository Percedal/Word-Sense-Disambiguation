{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "2.7.17-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python271764bitfc26ea4e491d46f7ba4683f54ef403ba",
   "display_name": "Python 2.7.17 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Parse les données d'apprentissage et de test"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "from xml.dom.minidom import parseString\n",
    "from xml.dom.minidom import parse\n",
    "import xml.etree.ElementTree as et\n",
    "\n",
    "sens_path = \"trial/data/multilingual-all-words.fr.senses\"\n",
    "gold_truth_path = \"trial/keys/keys-bn.fr\"\n",
    "corpus_path = \"trial/data/multilingual-all-words.fr.xml\"\n",
    "\n",
    "output_corpus_path = \"trial_corpus.xml\""
   ]
  },
  {
   "source": [
    "## Parser sur le fichier de sens"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "5 first entries in the BabelNet senses dictionnary\n(' -', 'mardi', '(6)', ['bn:01718496n', 'bn:01502893n', 'bn:01433525n', 'bn:02861587n', 'bn:00078546n', 'bn:00807619n'])\n(' -', 'dollar', '(10)', ['bn:00028114n', 'bn:02156595n', 'bn:01827683n', 'bn:00008375n', 'bn:00015129n', 'bn:02122502n', 'bn:00028118n', 'bn:00028116n', 'bn:02897671n', 'bn:01524928n'])\n(' -', 'rang', '(10)', ['bn:00054281n', 'bn:01743576n', 'bn:02256034n', 'bn:00073933n', 'bn:02155551n', 'bn:00066152n', 'bn:00066151n', 'bn:01646319n', 'bn:00827444n', 'bn:02587718n'])\n(' -', 'conseiller', '(6)', ['bn:00023123n', 'bn:00023132n', 'bn:00391567n', 'bn:00001598n', 'bn:02181897n', 'bn:00001604n'])\n(' -', 'radio', '(9)', ['bn:00065899n', 'bn:00065901n', 'bn:02558854n', 'bn:01441457n', 'bn:00065900n', 'bn:01152589n', 'bn:02599193n', 'bn:01936146n', 'bn:02231112n'])\n"
     ]
    }
   ],
   "source": [
    "def parse_senses_file(file):\n",
    "    \"\"\"Parse the BabelNet senses contained in .senses file given by SemEval\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    file: file object\n",
    "        An open .senses file\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        A dictionnary mapping a lemma with it's BabelNet senses\n",
    "    \"\"\"\n",
    "    # BabelNet Sense Dictionnary\n",
    "    # bn_sens_dict[<lemma>] = [<senses>]\n",
    "    bn_sens_dict = {}\n",
    "\n",
    "    # Capture only BabelNet senses in the given senses file\n",
    "    for s in file.readlines():\n",
    "        splitted_line = re.split(\"\\s\", s)\n",
    "        \n",
    "        # Parse and get the lemma\n",
    "        lemma = splitted_line[0].split(\"#\")[0]\n",
    "        \n",
    "        # Parse BabelNet data\n",
    "        bn_num = int(splitted_line[2])\n",
    "        bn_senses = []\n",
    "        if bn_num > 0:\n",
    "            for i in range(bn_num):\n",
    "                bn_senses.append(splitted_line[3+i])\n",
    "        \n",
    "        # Parse WordNet data\n",
    "        #wn_num = int(splitted_line[3+bn_num])\n",
    "        #wn_senses = []\n",
    "        #if wn_num > 0:\n",
    "        #    for i in range(wn_num):\n",
    "        #        wn_senses.append(splitted_line[4+bn_num+i])\n",
    "        \n",
    "        # Parse Wikipedia data \n",
    "        #wiki_num = int(splitted_line[4+bn_num+wn_num])\n",
    "        #wiki_senses = []\n",
    "        #if wiki_num > 0:\n",
    "        #    for i in range(wiki_num):\n",
    "        #        wiki_senses.append(splitted_line[5+bn_num+wn_num+i])\n",
    "\n",
    "        bn_sens_dict[lemma] = bn_senses\n",
    "    \n",
    "    return bn_sens_dict\n",
    "\n",
    "senses_dict = parse_senses_file(open(sens_path))\n",
    "keys = list(senses_dict.keys())\n",
    "print(\"5 first entries in the BabelNet senses dictionnary\")\n",
    "for i in range(5):\n",
    "    print(\" -\", keys[i], \"(%d)\"%(len(senses_dict[keys[i]])), senses_dict[keys[i]])"
   ]
  },
  {
   "source": [
    "## Parser sur le fichier \"gold truth\""
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "5 first entries in the BabelNet gold truth dictionnary\n(' -', 'd001.s004.t001', '(1)', ['bn:00006997n'])\n(' -', 'd001.s004.t002', '(1)', ['bn:00007299n'])\n(' -', 'd001.s004.t003', '(1)', ['bn:00023471n'])\n(' -', 'd001.s004.t004', '(1)', ['bn:00053479n'])\n(' -', 'd001.s004.t005', '(1)', ['bn:00007140n'])\n"
     ]
    }
   ],
   "source": [
    "def parse_gold_truth(file):\n",
    "    \"\"\"Parse a SemEval gold truth (keys) file\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    file: file object\n",
    "        An open file\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        A dictionnary mapping a SemEval ids with their senses (according to the gold truth, an id can be link to multiple senses)\n",
    "    \"\"\"\n",
    "    # BabelNet gold truth of the corpus\n",
    "    bn_gt = {}\n",
    "\n",
    "    for line in file.readlines():\n",
    "        line = line.split()\n",
    "\n",
    "        id = line[1]\n",
    "        senses = []\n",
    "        i = 2\n",
    "        while i < len(line) and line[i] != \"!!\":\n",
    "            senses.append(line[i])\n",
    "            i+=1\n",
    "        bn_gt[id] = senses\n",
    "    return bn_gt\n",
    "\n",
    "gt_dict = parse_gold_truth(open(gold_truth_path))\n",
    "keys = list(gt_dict.keys())\n",
    "print(\"5 first entries in the BabelNet gold truth dictionnary\")\n",
    "for i in range(5):\n",
    "    print(\" -\", keys[i], \"(%d)\"%(len(gt_dict[keys[i]])), gt_dict[keys[i]])"
   ]
  },
  {
   "source": [
    "## Parser sur le corpus"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "def parse_corpus_file(file):\n",
    "    \"\"\"Parse a SemEval corpus\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    file\n",
    "        Either an open .xml SemEval corpus file or it's path\n",
    "    text_id: int\n",
    "        The index of the text to parse in the SemEval corpus starting from 0\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        list of tuple, each one contains a str (the sentence where elements are separated by a space) and a list of tuples mapping the SemEval id of the word (dxxx.sxxx.txxx), the index of the lemma in the sentence (index start from 0) and the lemma itself\n",
    "    \"\"\"\n",
    "\n",
    "    DOMTree = parse(file)\n",
    "    corpus = DOMTree.documentElement\n",
    "    \n",
    "    documents = []\n",
    "\n",
    "    # Iterate through the different documents (text markers in the SemEval corpora)    \n",
    "    for t in corpus.getElementsByTagName(\"text\"):\n",
    "        sentences = []\n",
    "        polysems = []\n",
    "        # Iterate through the sentences in each documents\n",
    "        for s in t.getElementsByTagName(\"sentence\"):\n",
    "            idx = 0\n",
    "            sentence = \"\"\n",
    "            polysem = []\n",
    "            for n in s.childNodes:\n",
    "                if n.nodeName == \"wf\":\n",
    "                    sentence += n.childNodes[0].data + \" \"\n",
    "                    idx += 1\n",
    "                if n.nodeName == \"instance\":\n",
    "                    lemma = n.getAttribute(\"lemma\")\n",
    "                    id = n.getAttribute(\"id\")\n",
    "                    sentence += n.childNodes[0].data + \" \"\n",
    "                    polysem.append((id, idx, lemma))\n",
    "                    idx += 1\n",
    "            sentences.append(sentence)\n",
    "            polysems.append(polysem)\n",
    "    \n",
    "        documents.append((sentences, polysems))\n",
    "\n",
    "    return documents\n",
    "    \n",
    "\n",
    "d = parse_corpus_file(corpus_path)\n",
    "print(\"Documents in the corpus:\", len(d))\n",
    "print()\n",
    "for i, (s,_) in enumerate(d):\n",
    "    print(\"Document %d: %d sentences\" % (i, len(s)))"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 11,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "('Documents in the corpus:', 1)\n()\nDocument 0: 36 sentences\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "unknow = []\n",
    "\n",
    "def parse_data(semeval_corpus_path, semeval_gt_path, output_path):\n",
    "    \"\"\"Parse the given SemEval data to a new xml.\n",
    "    All given path has to exist\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    semeval_corpus_path: str\n",
    "        Path to a SemEval .xml corpus file\n",
    "    semeval_gt_path: str\n",
    "        Path to a SemEval keys fils (related to the previous given one and preferably a BabelNet file : keys-bn)\n",
    "    output_path: str\n",
    "        Path to the output xml file, will contain the newly formated SemEval corpus\n",
    "    \"\"\"\n",
    "\n",
    "    documents = parse_corpus_file(semeval_corpus_path)\n",
    "    gt_dict = parse_gold_truth(open(semeval_gt_path))\n",
    "\n",
    "    root = et.Element(\"corpus\")\n",
    "\n",
    "    for d, (sentences, polysems) in enumerate(documents):\n",
    "        document = et.SubElement(root, \"document\", {\"id\":str(d)})\n",
    "        for i in range(len(sentences)):\n",
    "            sentence = et.SubElement(document, \"sentence\", {\n",
    "                \"id\":str(i), \"s\": sentences[i]\n",
    "            })\n",
    "            for p, (id, idx, lemma) in enumerate(polysems[i]):\n",
    "                if id in gt_dict:\n",
    "                    lemma = et.SubElement(sentence, \"lemma\", {\n",
    "                        \"id\" : str(p),\n",
    "                        \"idx\" : str(idx), \n",
    "                        \"lemma\" : lemma,\n",
    "                        \"senses\" : \" \".join(gt_dict[id])\n",
    "                    })\n",
    "                else:\n",
    "                    unknow.append(id)\n",
    "\n",
    "    with open(output_path, \"wb\") as f:\n",
    "        f.write(parseString(et.tostring(root)).toprettyxml(encoding=\"UTF-8\"))"
   ]
  },
  {
   "source": [
    "## Parsing des données de dev"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "('unknow lemma:', 0)\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n\n<corpus>\n\n\t<document id=\"0\">\n\n\t\t<sentence id=\"0\" s=\"Nous ne savons pas qui gagnera la guerre_contre_la_drogue en Amérique_Latine , mais nous savons qui est en train de la perdre - la presse . \">\n\n\t\t\t<lemma id=\"0\" idx=\"7\" lemma=\"guerre_contre_la_drogue\" senses=\"bn:00028885n\"/>\n\n\t\t\t<lemma id=\"1\" idx=\"9\" lemma=\"Amérique_Latine\" senses=\"bn:00050165n\"/>\n\n\t\t\t<lemma id=\"2\" idx=\"23\" lemma=\"presse\" senses=\"bn:00064245n\"/>\n\n\t\t</sentence>\n\n\t\t<sentence id=\"1\" s=\"Au cours des six derniers mois , six journalistes ont été tués et 10 ont été enlevés par des trafiquants_de_drogue ou des guérilleros de gauche - souvent il s ’ agit des mêmes personnes - en Colombie . \">\n\n\t\t\t<lemma id=\"0\" idx=\"5\" lemma=\"mois\" senses=\"bn:00014710n\"/>\n\n(...)\n"
     ]
    }
   ],
   "source": [
    "parse_data(corpus_path, gold_truth_path, output_corpus_path)\n",
    "print(\"unknow lemma:\", len(unknow))\n",
    "\n",
    "f = open(output_corpus_path)\n",
    "for i in range(10):\n",
    "    print(f.readline())\n",
    "print(\"(...)\")"
   ]
  },
  {
   "source": [
    "## Parsing des données de test"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "('unknow lemma:', 226)\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n\n<corpus>\n\n\t<document id=\"0\">\n\n\t\t<sentence id=\"0\" s=\"Le groupe des Nations_Unies a des projets de plans pour la réduction des émissions \">\n\n\t\t\t<lemma id=\"0\" idx=\"1\" lemma=\"groupe\" senses=\"bn:00041942n\"/>\n\n\t\t\t<lemma id=\"1\" idx=\"3\" lemma=\"nations_unies\" senses=\"bn:00078931n\"/>\n\n\t\t\t<lemma id=\"3\" idx=\"8\" lemma=\"plan\" senses=\"bn:00062759n\"/>\n\n\t\t\t<lemma id=\"4\" idx=\"11\" lemma=\"réduction\" senses=\"bn:00025780n\"/>\n\n\t\t\t<lemma id=\"5\" idx=\"13\" lemma=\"émission\" senses=\"bn:00030455n\"/>\n\n\t\t</sentence>\n\n(...)\n"
     ]
    }
   ],
   "source": [
    "test_corpus_path = \"test/data/multilingual-all-words.fr.xml\"\n",
    "test_gt_path = \"test/keys/gold/babelnet/babelnet.fr.key\"\n",
    "output_path = \"test_corpus.xml\"\n",
    "\n",
    "parse_data(test_corpus_path, test_gt_path, output_path)\n",
    "print(\"unknow lemma:\", len(unknow))\n",
    "\n",
    "f = open(output_path)\n",
    "for i in range(10):\n",
    "    print(f.readline())\n",
    "print(\"(...)\")"
   ]
  },
  {
   "source": [
    "## Analyse données BabelNet"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib2\n",
    "import urllib\n",
    "import json\n",
    "import gzip\n",
    "\n",
    "from StringIO import StringIO\n",
    "\n",
    "key1 = \"\"\n",
    "key2 = \"\"\n",
    "key=key1"
   ]
  },
  {
   "source": [
    "Interoge l'API BabelNet pour récupérer les ids associés à un lemme donné."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemma2ids(lemma):\n",
    "    ids = []\n",
    "    \n",
    "    service_url = 'https://babelnet.io/v5/getSynsetIds'\n",
    "\n",
    "    params = {\n",
    "            'lemma' : lemma,\n",
    "            'searchLang' : \"FR\",\n",
    "            'key'  : key\n",
    "    }\n",
    "\n",
    "    url = service_url + '?' + urllib.urlencode(params)\n",
    "    request = urllib2.Request(url)\n",
    "    request.add_header('Accept-encoding', 'gzip')\n",
    "    response = urllib2.urlopen(request)\n",
    "\n",
    "    if response.info().get('Content-Encoding') == 'gzip':\n",
    "            buf = StringIO( response.read())\n",
    "            f = gzip.GzipFile(fileobj=buf)\n",
    "            data = json.loads(f.read())\n",
    "            ids = [str(res[\"id\"]) for res in data]\n",
    "    \n",
    "    return ids\n",
    "\n",
    "#lemma2ids(\"apple\")\n",
    "#['bn:00289737n', 'bn:03739345n', 'bn:00955003n', 'bn:00512973n']"
   ]
  },
  {
   "source": [
    "Interoge l'API BabelNet pour récuperer la définition associée à un id."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def id2glosses(id):\n",
    "    res = []\n",
    "\n",
    "    service_url = 'https://babelnet.io/v5/getSynset'\n",
    "\n",
    "    params = {\n",
    "        'id' : id,\n",
    "        'targetLang' : 'FR',\n",
    "        'key'  : key\n",
    "    }\n",
    "\n",
    "    url = service_url + '?' + urllib.urlencode(params)\n",
    "    request = urllib2.Request(url)\n",
    "    request.add_header('Accept-encoding', 'gzip')\n",
    "    response = urllib2.urlopen(request)\n",
    "\n",
    "    if response.info().get('Content-Encoding') == 'gzip':\n",
    "        buf = StringIO( response.read())\n",
    "        f = gzip.GzipFile(fileobj=buf)\n",
    "        data = json.loads(f.read())\n",
    "\n",
    "        # retrieving BabelSense data\n",
    "        #senses = data['senses']\n",
    "        #for result in senses:\n",
    "        #    lemma = result[\"properties\"].get('fullLemma')\n",
    "        #    language = result[\"properties\"].get('language')\n",
    "        #    print language.encode('utf-8') + \"\\t\" + str(lemma.encode('utf-8'))\n",
    "\n",
    "        # retrieving BabelGloss data\n",
    "        glosses = data['glosses']\n",
    "        for result in glosses:\n",
    "            gloss = result.get('gloss')\n",
    "            res.append(gloss)\n",
    "    \n",
    "    return res\n",
    "\n",
    "#for g in id2glosses(\"bn:00015540n\"):\n",
    "#    print(g)\n",
    "#Paris est la capitale de la France.\n",
    "#La Rue de l'Abbé-de-l'Épée jouxte l'Institut des Jeunes sourds.\n",
    "#Capitale de la France\n",
    "#Capitale et plus grosse ville de France.\n",
    "#Paris est une ville française, capitale de la France et le chef-lieu de la région d'Île-de-France."
   ]
  },
  {
   "source": [
    "Produit un dictionnaire des définitions des mots polysemiques du corpus de test"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "50 lemmas in the dictionnary\n",
      "mardi\n",
      "dollar\n",
      "rang\n",
      "conseiller\n",
      "radio\n",
      "guérilla\n",
      "kilo\n",
      "déclaration\n",
      "famille\n",
      "semaine\n",
      "el_spectador\n",
      "état\n",
      "hémisphère\n",
      "domicile\n",
      "manuel_noriega\n",
      "presse\n",
      "coup_d'état\n",
      "indignation\n",
      "extradition\n",
      "cuba\n",
      "massacre\n",
      "personne\n",
      "jaime_paz_zamora\n",
      "courage\n",
      "problème\n",
      "liberté\n",
      "démocratie\n",
      "trafiquant_de_drogue\n",
      "medellin\n",
      "guerre_contre_la_drogue\n",
      "relation\n",
      "david_asman\n",
      "el\n",
      "autorité\n",
      "télévision\n",
      "bogota\n",
      "aide\n",
      "frère\n",
      "traité\n",
      "nicaragua\n",
      "homme\n",
      "canada\n",
      "panama\n",
      "exil\n",
      "ordinateur\n",
      "éditorial\n",
      "bureau\n",
      "gouvernement\n",
      "cible\n",
      "rédacteur\n",
      "meurtre\n",
      "\tadding it\n",
      "journal\n",
      "\tadding it\n",
      "million\n",
      "\tadding it\n",
      "narcotrafic\n",
      "\tadding it\n",
      "directeur\n",
      "\tadding it\n",
      "jose_abello_silva\n",
      "\tadding it\n",
      "jour\n",
      "\tadding it\n",
      "mère\n",
      "\tadding it\n",
      "voiture\n",
      "\tadding it\n",
      "acte_d'accusation\n",
      "\tadding it\n",
      "bataillon\n",
      "\tadding it\n",
      "instant\n",
      "\tadding it\n",
      "forme\n",
      "\tadding it\n",
      "floride\n",
      "\tadding it\n",
      "pouvoir\n",
      "\tadding it\n",
      "hier\n",
      "\tadding it\n",
      "année\n",
      "\tadding it\n",
      "employé\n",
      "\tadding it\n",
      "pays\n",
      "\tadding it\n",
      "colombie\n",
      "\tadding it\n",
      "besoin\n",
      "\tadding it\n",
      "trafiquant\n",
      "\tadding it\n",
      "août\n",
      "\tadding it\n",
      "monterrey\n",
      "\tadding it\n",
      "attaque\n",
      "\tadding it\n",
      "heure\n",
      "\tadding it\n",
      "mort\n",
      "\tadding it\n",
      "station\n",
      "\tadding it\n",
      "leonidas_vargas\n",
      "\tadding it\n",
      "dynamite\n",
      "\tadding it\n",
      "résultat\n",
      "\tadding it\n",
      "cartel_de_la_drogue\n",
      "\tadding it\n",
      "bolivie\n",
      "\tadding it\n",
      "répression\n",
      "\tadding it\n",
      "mois\n",
      "\tadding it\n",
      "mexique\n",
      "\tadding it\n",
      "fonctionnaire\n",
      "\tadding it\n",
      "guérillero\n",
      "\tadding it\n",
      "journaliste\n",
      "\tadding it\n",
      "amérique_latine\n",
      "\tadding it\n",
      "liste\n",
      "\tadding it\n",
      "conférence\n",
      "\tadding it\n",
      "gauche\n",
      "\tadding it\n",
      "castro\n",
      "\tadding it\n",
      "président\n",
      "\tadding it\n",
      "sandinistes\n",
      "\tadding it\n",
      "amériques\n",
      "\tadding it\n",
      "etats-unis\n",
      "\tadding it\n",
      "pérou\n",
      "\tadding it\n",
      "politique\n",
      "\tadding it\n",
      "caraïbes\n",
      "\tadding it\n",
      "el_espectador\n",
      "\tadding it\n",
      "assassinat\n",
      "\tadding it\n",
      "alan_garcia\n",
      "\tadding it\n",
      "seigneur_de_la_drogue\n",
      "\tadding it\n",
      "cartel\n",
      "\tadding it\n",
      "participant\n",
      "\tadding it\n",
      "personnel\n",
      "\tadding it\n",
      "progrès\n",
      "\tadding it\n",
      "rapport\n",
      "\tadding it\n",
      "négociation\n",
      "\tadding it\n",
      "drogue\n",
      "\tadding it\n",
      "procureur\n",
      "\tadding it\n",
      "éditeur\n",
      "\tadding it\n",
      "feu_rouge\n",
      "\tadding it\n",
      "contrôle\n",
      "\tadding it\n",
      "censure\n",
      "\tadding it\n",
      "procès\n",
      "\tadding it\n",
      "tomas_borge\n",
      "\tadding it\n",
      "troupe\n",
      "\tadding it\n",
      "trafic_de_drogue\n",
      "\tadding it\n",
      "lutte\n",
      "\tadding it\n",
      "ortega\n",
      "\tadding it\n",
      "fiasco\n",
      "\tadding it\n"
     ]
    }
   ],
   "source": [
    "with open(\"dict.dictionary\", \"r+\") as file:\n",
    "    # look for the lemmas allready in the dictionary file\n",
    "    # no need to consume babelnet coin for a lemma we know\n",
    "    reader = csv.reader(file, delimiter=\";\")\n",
    "    alreadyin = [row[0] for row in reader]\n",
    "    print len(alreadyin)-1, \"lemmas in the dictionnary\"\n",
    "    \n",
    "    \n",
    "    writer = csv.writer(file, delimiter=\";\")\n",
    "    \n",
    "    if len(alreadyin) <= 1:\n",
    "        writer.writerow([\"lemma\", \"nb_senses\", \"BN_senses\", \"definitions\"])\n",
    "    \n",
    "    for lemma,senses in senses_dict.items():\n",
    "        print lemma\n",
    "        if not lemma in alreadyin:\n",
    "            print \"\\tadding it\"\n",
    "            ids_lemma = lemma2ids(lemma)\n",
    "            \n",
    "            defs_lemma = []\n",
    "            for id in ids_lemma:\n",
    "                try:\n",
    "                    defs_lemma.append(id2glosses(id))\n",
    "                except urllib2.HTTPError:\n",
    "                    defs_lemma.append([])\n",
    "            \n",
    "            ids = []\n",
    "            defs = []\n",
    "            for i in range(len(defs_lemma)):\n",
    "                # Iterate through the definitions\n",
    "                # if the definition is empty : pass\n",
    "                # else append the id and definition\n",
    "                if defs_lemma[i]:\n",
    "                    ids.append(ids_lemma[i])\n",
    "                    defs.append(defs_lemma[i])\n",
    "            # append the new lemma to the file\n",
    "            file.write(lemma + \";\" +\n",
    "                str(len(ids)) + \";\" +\n",
    "                (\",\".join(ids)) + \";\" +\n",
    "                (\",\".join([\"\\\"\" + (\" \".join(d)).encode(\"utf-8\") + \"\\\"\" for d in defs])) + \"\\n\")\n",
    "            alreadyin.append(lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}