{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Parse les données d'apprentissage et de test"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Parse le corpus"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from xml.dom.minidom import parse\n",
    "import xml.dom.minidom\n",
    "\n",
    "sens_path = \"trial/data/multilingual-all-words.fr.senses\"\n",
    "\n",
    "corpus_path = \"trial/data/multilingual-all-words.fr.xml\"\n",
    "DOMTree = parse(corpus_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Nous\nne\nsavons\npas\nqui\ngagnera\nla\nguerre_contre_la_drogue >instance<\nen\nAmérique_Latine >instance<\n,\nmais\nnous\nsavons\nqui\nest\nen\ntrain\nde\nla\nperdre\n-\nla\npresse >instance<\n.\n"
     ]
    }
   ],
   "source": [
    "corpus = DOMTree.documentElement\n",
    "sentences = corpus.getElementsByTagName(\"sentence\")\n",
    "\n",
    "s = sentences[0]\n",
    "#tokens = s.getElementsByTagName(\"wf\")\n",
    "for n in s.childNodes:\n",
    "    if n.nodeName == \"wf\":\n",
    "        print(n.childNodes[0].data)\n",
    "    if n.nodeName == \"instance\":\n",
    "        print(n.childNodes[0].data, \">instance<\")\n",
    "        \n",
    "#for t in tokens:\n",
    "#    print(t.childNodes[0].data)"
   ]
  },
  {
   "source": [
    "Perse le dictionnaire de sens"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "senses_file = open(sens_path)\n",
    "\n",
    "# BabelNet Sense Dictionnary\n",
    "# bn_sens_dict[<lemma>] = [<senses>]\n",
    "bn_sens_dict = {}\n",
    "\n",
    "# Capture only BabelNet senses in the given senses file\n",
    "for s in senses_file.readlines():\n",
    "    splitted_line = re.split(\"\\s\", s)\n",
    "    \n",
    "    # The 2nd split remove the class of the lemma (noun, verb or adjective)\n",
    "    lemma = splitted_line[0].split(\"#\")[0]\n",
    "    bn_num = int(splitted_line[2])\n",
    "    bn_senses = []\n",
    "    if bn_num > 0:\n",
    "        for i in range(bn_num):\n",
    "            bn_senses.append(splitted_line[3+i])\n",
    "    \n",
    "    #wn_num = int(splitted_line[3+bn_num])\n",
    "    #wn_senses = []\n",
    "    #if wn_num > 0:\n",
    "    #    for i in range(wn_num):\n",
    "    #        wn_senses.append(splitted_line[4+bn_num+i])\n",
    "    #wiki_num = int(splitted_line[4+bn_num+wn_num])\n",
    "    #wiki_senses = []\n",
    "    #if wiki_num > 0:\n",
    "    #    for i in range(wiki_num):\n",
    "    #        wiki_senses.append(splitted_line[5+bn_num+wn_num+i])\n",
    "\n",
    "    bn_sens_dict[lemma] = bn_senses\n",
    "\n",
    "#bn_sens_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}