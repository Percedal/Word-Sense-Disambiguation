{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Parse les données d'apprentissage et de test"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from xml.dom.minidom import parseString\n",
    "from xml.dom.minidom import parse\n",
    "import xml.etree.ElementTree as et\n",
    "\n",
    "sens_path = \"trial/data/multilingual-all-words.fr.senses\"\n",
    "gold_truth_path = \"trial/keys/keys-bn.fr\"\n",
    "corpus_path = \"trial/data/multilingual-all-words.fr.xml\"\n",
    "\n",
    "output_corpus_path = \"trial_corpus.xml\""
   ]
  },
  {
   "source": [
    "## Parser sur le fichier de sens"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "5 first entries in the BabelNet senses dictionnary\n - trafiquant_de_drogue (2) ['bn:01761518n', 'bn:00028881n']\n - drogue (2) ['bn:00026546n', 'bn:00028872n']\n - semaine (4) ['bn:00080815n', 'bn:00043484n', 'bn:00080821n', 'bn:00080813n']\n - indignation (6) ['bn:00046491n', 'bn:00004087n', 'bn:00004086n', 'bn:01960121n', 'bn:00004085n', 'bn:01328234n']\n - employé (1) ['bn:00030618n']\n"
     ]
    }
   ],
   "source": [
    "def parse_senses_file(file):\n",
    "    \"\"\"Parse the BabelNet senses contained in .senses file given by SemEval\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    file: file object\n",
    "        An open .senses file\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        A dictionnary mapping a lemma with it's BabelNet senses\n",
    "    \"\"\"\n",
    "    # BabelNet Sense Dictionnary\n",
    "    # bn_sens_dict[<lemma>] = [<senses>]\n",
    "    bn_sens_dict = {}\n",
    "\n",
    "    # Capture only BabelNet senses in the given senses file\n",
    "    for s in file.readlines():\n",
    "        splitted_line = re.split(\"\\s\", s)\n",
    "        \n",
    "        # Parse and get the lemma\n",
    "        lemma = splitted_line[0].split(\"#\")[0]\n",
    "        \n",
    "        # Parse BabelNet data\n",
    "        bn_num = int(splitted_line[2])\n",
    "        bn_senses = []\n",
    "        if bn_num > 0:\n",
    "            for i in range(bn_num):\n",
    "                bn_senses.append(splitted_line[3+i])\n",
    "        \n",
    "        # Parse WordNet data\n",
    "        #wn_num = int(splitted_line[3+bn_num])\n",
    "        #wn_senses = []\n",
    "        #if wn_num > 0:\n",
    "        #    for i in range(wn_num):\n",
    "        #        wn_senses.append(splitted_line[4+bn_num+i])\n",
    "        \n",
    "        # Parse Wikipedia data \n",
    "        #wiki_num = int(splitted_line[4+bn_num+wn_num])\n",
    "        #wiki_senses = []\n",
    "        #if wiki_num > 0:\n",
    "        #    for i in range(wiki_num):\n",
    "        #        wiki_senses.append(splitted_line[5+bn_num+wn_num+i])\n",
    "\n",
    "        bn_sens_dict[lemma] = bn_senses\n",
    "    \n",
    "    return bn_sens_dict\n",
    "\n",
    "senses_dict = parse_senses_file(open(sens_path))\n",
    "keys = list(senses_dict.keys())\n",
    "print(\"5 first entries in the BabelNet senses dictionnary\")\n",
    "for i in range(5):\n",
    "    print(\" -\", keys[i], \"(%d)\"%(len(senses_dict[keys[i]])), senses_dict[keys[i]])"
   ]
  },
  {
   "source": [
    "## Parser sur le fichier \"gold truth\""
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "5 first entries in the BabelNet gold truth dictionnary\n - d001.s001.t001 (1) ['bn:00028885n']\n - d001.s001.t002 (1) ['bn:00050165n']\n - d001.s001.t003 (1) ['bn:00064245n']\n - d001.s002.t001 (1) ['bn:00014710n']\n - d001.s002.t002 (1) ['bn:00048461n']\n"
     ]
    }
   ],
   "source": [
    "def parse_gold_truth(file):\n",
    "    \"\"\"Parse a SemEval gold truth (keys) file\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    file: file object\n",
    "        An open file\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        A dictionnary mapping a SemEval ids with their senses (according to the gold truth, an id can be link to multiple senses)\n",
    "    \"\"\"\n",
    "    # BabelNet gold truth of the corpus\n",
    "    bn_gt = {}\n",
    "\n",
    "    for line in file.readlines():\n",
    "        line = line.split()\n",
    "\n",
    "        id = line[1]\n",
    "        senses = []\n",
    "        i = 2\n",
    "        while i < len(line) and line[i] != \"!!\":\n",
    "            senses.append(line[i])\n",
    "            i+=1\n",
    "        bn_gt[id] = senses\n",
    "    return bn_gt\n",
    "\n",
    "gt_dict = parse_gold_truth(open(gold_truth_path))\n",
    "keys = list(gt_dict.keys())\n",
    "print(\"5 first entries in the BabelNet gold truth dictionnary\")\n",
    "for i in range(5):\n",
    "    print(\" -\", keys[i], \"(%d)\"%(len(gt_dict[keys[i]])), gt_dict[keys[i]])"
   ]
  },
  {
   "source": [
    "## Parser sur le corpus"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "def parse_corpus_file(file):\n",
    "    \"\"\"Parse a SemEval corpus\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    file\n",
    "        Either an open .xml SemEval corpus file or it's path\n",
    "    text_id: int\n",
    "        The index of the text to parse in the SemEval corpus starting from 0\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        list of tuple, each one contains a str (the sentence where elements are separated by a space) and a list of tuples mapping the SemEval id of the word (dxxx.sxxx.txxx), the index of the lemma in the sentence (index start from 0) and the lemma itself\n",
    "    \"\"\"\n",
    "\n",
    "    DOMTree = parse(file)\n",
    "    corpus = DOMTree.documentElement\n",
    "    \n",
    "    documents = []\n",
    "\n",
    "    # Iterate through the different documents (text markers in the SemEval corpora)    \n",
    "    for t in corpus.getElementsByTagName(\"text\"):\n",
    "        sentences = []\n",
    "        polysems = []\n",
    "        # Iterate through the sentences in each documents\n",
    "        for s in t.getElementsByTagName(\"sentence\"):\n",
    "            idx = 0\n",
    "            sentence = \"\"\n",
    "            polysem = []\n",
    "            for n in s.childNodes:\n",
    "                if n.nodeName == \"wf\":\n",
    "                    sentence += n.childNodes[0].data + \" \"\n",
    "                    idx += 1\n",
    "                if n.nodeName == \"instance\":\n",
    "                    lemma = n.getAttribute(\"lemma\")\n",
    "                    id = n.getAttribute(\"id\")\n",
    "                    sentence += n.childNodes[0].data + \" \"\n",
    "                    polysem.append((id, idx, lemma))\n",
    "                    idx += 1\n",
    "            sentences.append(sentence)\n",
    "            polysems.append(polysem)\n",
    "    \n",
    "        documents.append((sentences, polysems))\n",
    "\n",
    "    return documents\n",
    "    \n",
    "\n",
    "d = parse_corpus_file(corpus_path)\n",
    "print(\"Documents in the corpus:\", len(d))\n",
    "print()\n",
    "for i, (s,_) in enumerate(d):\n",
    "    print(\"Document %d: %d sentences\" % (i, len(s)))"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 9,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Documents in the corpus: 1\n\nDocument 0: 36 sentences\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "unknow = []\n",
    "\n",
    "def parse_data(semeval_corpus_path, semeval_gt_path, output_path):\n",
    "    \"\"\"Parse the given SemEval data to a new xml.\n",
    "    All given path has to exist\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    semeval_corpus_path: str\n",
    "        Path to a SemEval .xml corpus file\n",
    "    semeval_gt_path: str\n",
    "        Path to a SemEval keys fils (related to the previous given one and preferably a BabelNet file : keys-bn)\n",
    "    output_path: str\n",
    "        Path to the output xml file, will contain the newly formated SemEval corpus\n",
    "    \"\"\"\n",
    "\n",
    "    documents = parse_corpus_file(semeval_corpus_path)\n",
    "    gt_dict = parse_gold_truth(open(semeval_gt_path))\n",
    "\n",
    "    root = et.Element(\"corpus\")\n",
    "\n",
    "    for d, (sentences, polysems) in enumerate(documents):\n",
    "        document = et.SubElement(root, \"document\", {\"id\":str(d)})\n",
    "        for i in range(len(sentences)):\n",
    "            sentence = et.SubElement(document, \"sentence\", {\n",
    "                \"id\":str(i), \"s\": sentences[i]\n",
    "            })\n",
    "            for p, (id, idx, lemma) in enumerate(polysems[i]):\n",
    "                if id in gt_dict:\n",
    "                    lemma = et.SubElement(sentence, \"lemma\", {\n",
    "                        \"id\" : str(p),\n",
    "                        \"idx\" : str(idx), \n",
    "                        \"lemma\" : lemma,\n",
    "                        \"senses\" : \" \".join(gt_dict[id])\n",
    "                    })\n",
    "                else:\n",
    "                    unknow.append(id)\n",
    "\n",
    "    with open(output_path, \"wb\") as f:\n",
    "        f.write(parseString(et.tostring(root)).toprettyxml(encoding=\"UTF-8\"))"
   ]
  },
  {
   "source": [
    "## Parsing des données de dev"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "unknow lemma: 0\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n\n<corpus>\n\n\t<document id=\"0\">\n\n\t\t<sentence id=\"0\" s=\"Nous ne savons pas qui gagnera la guerre_contre_la_drogue en Amérique_Latine , mais nous savons qui est en train de la perdre - la presse . \">\n\n\t\t\t<lemma id=\"0\" idx=\"7\" lemma=\"guerre_contre_la_drogue\" senses=\"bn:00028885n\"/>\n\n\t\t\t<lemma id=\"1\" idx=\"9\" lemma=\"Amérique_Latine\" senses=\"bn:00050165n\"/>\n\n\t\t\t<lemma id=\"2\" idx=\"23\" lemma=\"presse\" senses=\"bn:00064245n\"/>\n\n\t\t</sentence>\n\n\t\t<sentence id=\"1\" s=\"Au cours des six derniers mois , six journalistes ont été tués et 10 ont été enlevés par des trafiquants_de_drogue ou des guérilleros de gauche - souvent il s ’ agit des mêmes personnes - en Colombie . \">\n\n\t\t\t<lemma id=\"0\" idx=\"5\" lemma=\"mois\" senses=\"bn:00014710n\"/>\n\n(...)\n"
     ]
    }
   ],
   "source": [
    "parse_data(corpus_path, gold_truth_path, output_corpus_path)\n",
    "print(\"unknow lemma:\", len(unknow))\n",
    "\n",
    "f = open(output_corpus_path)\n",
    "for i in range(10):\n",
    "    print(f.readline())\n",
    "print(\"(...)\")"
   ]
  },
  {
   "source": [
    "## Parsing des données de test"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "unknow lemma: 226\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n\n<corpus>\n\n\t<document id=\"0\">\n\n\t\t<sentence id=\"0\" s=\"Le groupe des Nations_Unies a des projets de plans pour la réduction des émissions \">\n\n\t\t\t<lemma id=\"0\" idx=\"1\" lemma=\"groupe\" senses=\"bn:00041942n\"/>\n\n\t\t\t<lemma id=\"1\" idx=\"3\" lemma=\"nations_unies\" senses=\"bn:00078931n\"/>\n\n\t\t\t<lemma id=\"3\" idx=\"8\" lemma=\"plan\" senses=\"bn:00062759n\"/>\n\n\t\t\t<lemma id=\"4\" idx=\"11\" lemma=\"réduction\" senses=\"bn:00025780n\"/>\n\n\t\t\t<lemma id=\"5\" idx=\"13\" lemma=\"émission\" senses=\"bn:00030455n\"/>\n\n\t\t</sentence>\n\n(...)\n"
     ]
    }
   ],
   "source": [
    "test_corpus_path = \"test/data/multilingual-all-words.fr.xml\"\n",
    "test_gt_path = \"test/keys/gold/babelnet/babelnet.fr.key\"\n",
    "output_path = \"test_corpus.xml\"\n",
    "\n",
    "parse_data(test_corpus_path, test_gt_path, output_path)\n",
    "print(\"unknow lemma:\", len(unknow))\n",
    "\n",
    "f = open(output_path)\n",
    "for i in range(10):\n",
    "    print(f.readline())\n",
    "print(\"(...)\")"
   ]
  }
 ]
}