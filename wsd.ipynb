{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Word Sens Disambiguation\n",
    "Ce projet étudiant se réalise dans le cadre du projet PolysEmY,  qui a pour but de désambiguïser un ensemble connu d'acronymes dont les différents sens sont également connus.<br/>\n",
    "En essayant d'améliorer l'apprentissage des embeddings de mots, ce travail s'inspire en partie des travaux de <b>Huang et al. <i>Improving Word Representations via Global Context and Multiple Word Prototypes</i>, 2012</b>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "import nltk\n",
    "import random\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import decomposition\n",
    "from xml.dom.minidom import parse\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.cluster.kmeans import KMeansClusterer\n",
    "\n",
    "trial_corpus_path = \"trial_corpus.xml\"\n",
    "test_corpus_path = \"test_corpus.xml\""
   ]
  },
  {
   "source": [
    "Fournis une méthode pour charger un corpus au format xml"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Documents (1):\n\tDoc  0: 36 sentences\n\nLemmas (124):\n\tLemma 0: guerre_contre_la_drogue -> 1 values\n\tLemma 1: Amérique_Latine -> 2 values\n\tLemma 2: presse -> 9 values\n\tLemma 3: mois -> 2 values\n\tLemma 4: journaliste -> 5 values\n\tLemma 5: trafiquant_de_drogue -> 2 values\n\tLemma 6: guérillero -> 1 values\n\tLemma 7: gauche -> 2 values\n\tLemma 8: personne -> 1 values\n\tLemma 9: Colombie -> 4 values\n\t...\n\n\nEx. lemma 0: \n\t guerre_contre_la_drogue -> [(0, 0, 7, 'bn:00028885n')]\n"
     ]
    }
   ],
   "source": [
    "def loadCorpus(path):\n",
    "    \"\"\"Load a formatted corpus data file\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path: str\n",
    "        Path to the corpus xml file to load\n",
    "\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        A 3 dimensional list containing for each document, the sentences that it is composed of.\n",
    "        Where each sentence is a list of single tokens.\n",
    "    dict\n",
    "        A dictionnary mapping a lemma with it's document/sentence/index position and the BabelNet Sense attributed\n",
    "    \"\"\"\n",
    "\n",
    "    DOMTree = parse(path)\n",
    "\n",
    "    documents = []\n",
    "    sens_dict = {}\n",
    "    for doc in DOMTree.getElementsByTagName(\"document\"):\n",
    "        # For each document\n",
    "        sentences = []\n",
    "        for sent in doc.getElementsByTagName(\"sentence\"):\n",
    "            # And for each sentence\n",
    "            # Append the new sentence\n",
    "            s = sent.getAttribute(\"s\")\n",
    "            sentences.append(s.split())\n",
    "\n",
    "            # Map the lemmas in the sentence with it's doc/sentence/index position and BabelNet sense\n",
    "            for lem in sent.getElementsByTagName(\"lemma\"):\n",
    "                idx = lem.getAttribute(\"idx\")\n",
    "                lemma = lem.getAttribute(\"lemma\")\n",
    "                # Few lemma may have more than 1 BabelNet sense (due to redundancy in BN)\n",
    "                # Only keep the 1st one\n",
    "                sense = lem.getAttribute(\"senses\").split()[0] \n",
    "                \n",
    "                ctx = (int(doc.getAttribute(\"id\")),\n",
    "                        int(sent.getAttribute(\"id\")),\n",
    "                        int(idx),\n",
    "                        sense)\n",
    "                if not lemma in sens_dict:\n",
    "                    sens_dict[lemma] = [ctx]\n",
    "                else:\n",
    "                    sens_dict[lemma].append(ctx)\n",
    "\n",
    "        documents.append(sentences)\n",
    "\n",
    "    return (documents, sens_dict)\n",
    "\n",
    "\n",
    "documents, sens_dict = loadCorpus(trial_corpus_path)\n",
    "\n",
    "\n",
    "print(\"Documents (%d):\"%(len(documents)))\n",
    "for i, d in enumerate(documents):\n",
    "    print(\"\\tDoc %2d: %02d sentences\"%(i, len(d)))\n",
    "\n",
    "print(\"\\nLemmas (%d):\"%(len(sens_dict)))\n",
    "for i, (k, v) in zip(range(10), sens_dict.items()):\n",
    "    print(\"\\tLemma %d: %s -> %d values\"%(i, k, len(v)))\n",
    "print(\"\\t...\\n\")\n",
    "print(\"\\nEx. lemma 0: \")\n",
    "print(\"\\t\", list(sens_dict.keys())[0], \"->\", list(sens_dict.values())[0])"
   ]
  },
  {
   "source": [
    "# Méthode 1 : méthode «Huang»"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Implémentation de Word2vec pour l'apprentissage «naïf» d'embedding"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "w2v Vocab size: 429\n"
     ]
    }
   ],
   "source": [
    "def documentsSentences(doc):\n",
    "    docSentences = []\n",
    "    for d in doc:\n",
    "        for s in d:\n",
    "            docSentences.append(s)\n",
    "    return docSentences\n",
    "\n",
    "def createW2vModel():\n",
    "    return Word2Vec(documentsSentences(documents), min_count=1)\n",
    "\n",
    "w2v = createW2vModel()\n",
    "w2v.save(\"w2v.model\")\n",
    "\n",
    "print(\"w2v Vocab size:\", len(w2v.wv.vocab))"
   ]
  },
  {
   "source": [
    "## Apprentissage d'embeddings par la méthode de Huang et al."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nEx. guerre_contre_la_drogue\n\t-> [(0, 0, 7, 'bn:00028885n')]\n\t-> ['bn:00028885n']\n"
     ]
    }
   ],
   "source": [
    "def lemma2Senses(lemma):\n",
    "    senses = [bn for _,_,_,bn in sens_dict[lemma]]\n",
    "    return list(set(senses))\n",
    "\n",
    "print(\"\\nEx.\", list(sens_dict.keys())[0])\n",
    "print(\"\\t->\", list(sens_dict.values())[0])\n",
    "print(\"\\t->\", lemma2Senses(list(sens_dict.keys())[0]))"
   ]
  },
  {
   "source": [
    "### Visualisation rapide des lemmes à désambiguïser\n",
    "\n",
    "On remarque que peu de lemmes sont associés à plusieurs sens. Certains apparaissent plusieurs fois avec toujours le meme sens. Pire ! D'autres n'apparaissent qu'une seule fois.<br/>\n",
    "\n",
    "Il est aussi intéressant de remarquer que certains lemmes sont associés 9 fois avec le sens_1 et 1 fois avec le sens_2. Ceci peut trouver son origine dans les annotations via BabelNet qui propose différents sens redondants d'un mot.<br/>\n",
    "Pour exemple, le lemme <i>journaliste</i> est associé aux sens BabelNet suivants :\n",
    "<ol>\n",
    "    <li>bn:00048461n : celui qui recueille, écrit ou distribue des informations</li>\n",
    "    <li>bn:00057562n : celui qui enquête, rapporte ou rédige les actualités</li>\n",
    "</ol>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Nombre de lemmes à désambiguïser : 124\n\nLemmes ayant plusieurs sens : 3\n\t journaliste  ->  [(0, 1, 8, 'bn:00048461n'), (0, 2, 10, 'bn:00048461n'), (0, 16, 43, 'bn:00048461n'), (0, 18, 38, 'bn:00057562n'), (0, 19, 27, 'bn:00048461n')]\n\t contrôle  ->  [(0, 3, 29, 'bn:00022287n'), (0, 26, 19, 'bn:00022283n')]\n\t journal  ->  [(0, 5, 30, 'bn:00057563n'), (0, 6, 5, 'bn:00057563n'), (0, 7, 11, 'bn:00057563n'), (0, 8, 19, 'bn:00057564n'), (0, 18, 5, 'bn:00057563n')]\n\nLemmes présents une seule fois : 86\n\t guerre_contre_la_drogue  ->  [(0, 0, 7, 'bn:00028885n')]\n\t guérillero  ->  [(0, 1, 22, 'bn:02557244n')]\n\t personne  ->  [(0, 1, 33, 'bn:00046516n')]\n\t année  ->  [(0, 2, 5, 'bn:00078738n')]\n\t août  ->  [(0, 3, 18, 'bn:00007140n')]\n\nLemmes n'ayant qu'un seul sens : 35\n\t Amérique_Latine  ->  [(0, 0, 9, 'bn:00050165n'), (0, 4, 14, 'bn:00050165n')]\n\t presse  ->  [(0, 0, 23, 'bn:00064245n'), (0, 4, 38, 'bn:00064245n'), (0, 12, 4, 'bn:00064245n'), (0, 16, 29, 'bn:00064245n'), (0, 17, 19, 'bn:00064245n'), (0, 19, 19, 'bn:00064245n'), (0, 20, 9, 'bn:00064245n'), (0, 21, 28, 'bn:00064245n'), (0, 35, 1, 'bn:00064245n')]\n\t mois  ->  [(0, 1, 5, 'bn:00014710n'), (0, 9, 8, 'bn:00014710n')]\n\t trafiquant_de_drogue  ->  [(0, 1, 19, 'bn:00028881n'), (0, 17, 11, 'bn:00028881n')]\n\t gauche  ->  [(0, 1, 24, 'bn:00149192n'), (0, 21, 49, 'bn:00149192n')]\n"
     ]
    }
   ],
   "source": [
    "polysem = {}\n",
    "solo = {}\n",
    "npolysem = {}\n",
    "\n",
    "for k,v in sens_dict.items():\n",
    "    if len(v) == 1:\n",
    "        solo[k] = v\n",
    "        continue\n",
    "    \n",
    "    _,_,_,sense_bn = v[0]\n",
    "    poly = False\n",
    "    for _,_,_,bn in v:\n",
    "        if bn != sense_bn:\n",
    "            polysem[k] = v\n",
    "            poly = True\n",
    "            break\n",
    "    if not poly:\n",
    "        npolysem[k] = v\n",
    "\n",
    "print(\"Nombre de lemmes à désambiguïser :\", len(sens_dict))\n",
    "print()\n",
    "\n",
    "print(\"Lemmes ayant plusieurs sens :\", len(polysem))\n",
    "for _, (k, v) in zip(range(5), polysem.items()):\n",
    "    print(\"\\t\", k, \" -> \", v)\n",
    "\n",
    "print(\"\\nLemmes présents une seule fois :\", len(solo))\n",
    "for _, (k, v) in zip(range(5), solo.items()):\n",
    "    print(\"\\t\", k, \" -> \", v)\n",
    "\n",
    "print(\"\\nLemmes n'ayant qu'un seul sens :\", len(npolysem))\n",
    "for _, (k, v) in zip(range(5), npolysem.items()):\n",
    "    print(\"\\t\", k, \" -> \", v)"
   ]
  },
  {
   "source": [
    "On remarque qu'au final peu de lemmes dans les corpus de dev et test possèdent plusieurs sens dans les corpus. Certains ne sont présents qu'une fois pendant que d'autre n'apparaisse tout le temps qu'avec le meme sens.<br/>\n",
    "- Il est a noter que ce n'est pas parce qu'un mot apparait toujours avec le meme sens dans le corpus qu'il n'est polysémique."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Implémentation de la méthode proposée par Huang\n",
    "<ol>\n",
    "    <li>Collecter les contextes d'occurrence d'un mot a désambiguïser</li>\n",
    "    <li>Calculer un vecteur de contexte, moyenne des embedding de chaque mots dans le contexte</li>\n",
    "    <li>Clusterer les vecteurs de contextes, spherical K-means</li>\n",
    "    <li>Associe à chaque cluster un sens, le centroïde du cluster est l'embedding du sens du mot</li>\n",
    "</ol>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meanSentenceVector(w2vModel, sentence):\n",
    "    return np.array([w2vModel.wv[word] for word in sentence]).mean(axis=0)\n",
    "\n",
    "def sumSentenceVector(w2vModel, sentence):\n",
    "    return np.array([w2vModel.wv[word] for word in sentence]).sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['suis', 'très', 'content', 'de', 'manger']\n['Je', 'suis', 'très', 'content', 'de', 'manger']\n['suis', 'très', 'content', 'de', 'manger', 'une', 'pomme', '!']\n"
     ]
    }
   ],
   "source": [
    "def extractWindow(sentence, start, end):\n",
    "    return sentence[max(0, start) : min(len(sentence), end)+1]\n",
    "\n",
    "print(extractWindow(\"Je suis très content de manger une pomme !\".split(), 1, 5))\n",
    "print(extractWindow(\"Je suis très content de manger une pomme !\".split(), -1, 5))\n",
    "print(extractWindow(\"Je suis très content de manger une pomme !\".split(), 1, 15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ctx_w = 11 # Contexte window size\n",
    "\n",
    "global_truth = []\n",
    "global_classif = []\n",
    "\n",
    "for lemma, senses in polysem.items():\n",
    "    labels = lemma2Senses(lemma)\n",
    "    num_senses = len(labels)\n",
    "\n",
    "    # Map clusters with a value from, 0 to num_senses-1\n",
    "    truth = [labels.index(bn) for _,_,_,bn in senses]\n",
    "    global_truth.append(truth)\n",
    "\n",
    "    mean_vectors = []\n",
    "    for d,s,i,_ in senses:\n",
    "        l = len(documents[d][s])\n",
    "        # Extract the words in the contexte window\n",
    "        window = extractWindow(documents[d][s], i-math.floor((ctx_w-1)/2), i+math.ceil((ctx_w-1)/2))\n",
    "\n",
    "        # Compute the context vector (mean of the words vectors in the window)\n",
    "        mean_vectors.append(meanSentenceVector(w2v, window))\n",
    "\n",
    "    # Spherical K-means clustering\n",
    "    skm = KMeansClusterer(num_senses, nltk.cluster.util.cosine_distance, rng=random.Random(0), repeats=10)\n",
    "    assigned_clusters = skm.cluster(mean_vectors, assign_clusters=True)\n",
    "    global_classif.append(assigned_clusters)\n",
    "\n",
    "    #print(truth)\n",
    "    #print(assigned_clusters)\n",
    "    #print()"
   ]
  },
  {
   "source": [
    "### Associe à chaque cluster un sens"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{0: 0, 1: 1}"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "def argsmax(lst):\n",
    "    max = lst[0]\n",
    "    argsmax = []\n",
    "    for i in range(len(lst)):\n",
    "        if lst[i] == max:\n",
    "            argsmax.append(i)\n",
    "        elif lst[i] > max:\n",
    "            max = lst[i]\n",
    "            argsmax = [i]\n",
    "    return argsmax\n",
    "\n",
    "def couple(arr1, arr2):\n",
    "    # truth, classif\n",
    "    l = len(arr1)\n",
    "    nb_c = len(set(arr1))\n",
    "    arr = np.zeros((nb_c,nb_c), dtype=int)\n",
    "\n",
    "    for i in range(l):\n",
    "        arr[arr2[i],arr1[i]] += 1\n",
    "\n",
    "    cs = [i for i in range(len(arr))]\n",
    "    ts = [i for i in range(len(arr))]\n",
    "    map = {}\n",
    "\n",
    "    for iter in range(len(arr)):\n",
    "        temp = len(arr)\n",
    "        for i in range(len(arr)):\n",
    "            c = arr[i]\n",
    "            ams = argsmax(c)\n",
    "            if len(ams) == 1:\n",
    "                ams = ams[0]\n",
    "                map[cs[i]] = ts[ams]\n",
    "                arr = np.delete(np.delete(arr, ams, 1), i, 0)\n",
    "                cs = np.delete(cs, i)\n",
    "                ts = np.delete(ts, ams)\n",
    "                break\n",
    "        \n",
    "        if len(arr) == temp:\n",
    "            j = np.argmax(arr[0])\n",
    "            map[cs[0]] = ts[j]\n",
    "            arr = np.delete(np.delete(arr, j, 1), 0, 0)\n",
    "            cs = np.delete(cs, 0)\n",
    "            ts = np.delete(ts, j)\n",
    "\n",
    "    return map\n",
    "\n",
    "couple(global_truth[0], global_classif[0])"
   ]
  },
  {
   "source": [
    "## Premiers Résultats (dev)\n",
    "Pour chaque lemme affiche :\n",
    "- La position d'apparission dans le corpus de chaque occurence ainsi que l'id BabelNet de sa définition. Pour une occurence, on a un tuple (n° document, n° phrase, position dans la phrase, id BabelNet).\n",
    "- La prédiction (résulat du clustering). Chaque occurence est associée à un cluster.\n",
    "- La target, ou chaque occurence est associée à son sens réel.\n",
    "\n",
    "À noter que le nombre de sens d'un mot se base qur les données de SemEval, le corpus définit pour chaque mot à désambiguiser entre 1 et 3 sens. Dans ces résultats lorsque l'on voit \"Prédiction : 0, 1\", cela signifit que la premiere occurence est associée au sens 0 du mot, la deuxieme au sens 1 du mot. Les sens 0 ou 1 sont ceux explicité précedemment, définit par le corpus SemEval."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Lemme\t\t journaliste\nPos & id\t [(0, 1, 8, 'bn:00048461n'), (0, 2, 10, 'bn:00048461n'), (0, 16, 43, 'bn:00048461n'), (0, 18, 38, 'bn:00057562n'), (0, 19, 27, 'bn:00048461n')]\nPrédiction\t [0, 1, 0, 1, 0]\nRéalité\t\t [0, 0, 0, 1, 0]\n\nLemme\t\t contrôle\nPos & id\t [(0, 3, 29, 'bn:00022287n'), (0, 26, 19, 'bn:00022283n')]\nPrédiction\t [1, 0]\nRéalité\t\t [1, 0]\n\nLemme\t\t journal\nPos & id\t [(0, 5, 30, 'bn:00057563n'), (0, 6, 5, 'bn:00057563n'), (0, 7, 11, 'bn:00057563n'), (0, 8, 19, 'bn:00057564n'), (0, 18, 5, 'bn:00057563n')]\nPrédiction\t [1, 1, 1, 1, 0]\nRéalité\t\t [0, 0, 0, 1, 0]\n\n"
     ]
    }
   ],
   "source": [
    "final_classif = []\n",
    "for i in range(len(global_classif)):\n",
    "    cluster_tags = couple(global_truth[i], global_classif[i])\n",
    "    final_classif.append([cluster_tags[i] for i in global_classif[i]])\n",
    "\n",
    "    print(\"Lemme\\t\\t\", list(polysem.keys())[i])\n",
    "    print(\"Pos & id\\t\", list(polysem.values())[i])\n",
    "    print(\"Prédiction\\t\", final_classif[i])\n",
    "    print(\"Réalité\\t\\t\", global_truth[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Taux de succès de prédiction : 0.67%\nTaux d'erreurs de prédiction : 0.33%\n"
     ]
    }
   ],
   "source": [
    "def precision(pred, real):\n",
    "    somme = 0\n",
    "    precision = 0\n",
    "    for i in range(len(pred)):\n",
    "        for j in range(len(pred[i])):\n",
    "            somme += 1\n",
    "            if pred[i][j] == real[i][j]:\n",
    "                precision += 1\n",
    "    return precision/somme\n",
    "\n",
    "res = precision(final_classif, global_truth)\n",
    "\n",
    "print(\"Taux de succès de prédiction : {:.2}%\".format(res))\n",
    "print(\"Taux d'erreurs de prédiction : {:.2}%\".format(1-res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(sklearn.metrics.classification_report([y for x in global_truth for y in x], [y for x in final_classif for y in x]))"
   ]
  },
  {
   "source": [
    "## Résultats sur le test\n",
    "Réamplique la procédure, cette fois-ci sur le test."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents, sens_dict = loadCorpus(test_corpus_path)\n",
    "\n",
    "w2v = createW2vModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "global_truth = []\n",
    "global_classif = []\n",
    "\n",
    "for lemma, senses in sens_dict.items():\n",
    "    labels = lemma2Senses(lemma)\n",
    "    num_senses = len(labels)\n",
    "\n",
    "    # Map clusters with a value from, 0 to num_senses-1\n",
    "    truth = [labels.index(bn) for _,_,_,bn in senses]\n",
    "    global_truth.append(truth)\n",
    "\n",
    "    mean_vectors = []\n",
    "    for d,s,i,_ in senses:\n",
    "        l = len(documents[d][s])\n",
    "        # Extract the words in the contexte window\n",
    "        window = extractWindow(documents[d][s], i-math.floor((ctx_w-1)/2), i+math.ceil((ctx_w-1)/2))\n",
    "\n",
    "        # Compute the context vector (mean of the words vectors in the window)\n",
    "        mean_vectors.append(meanSentenceVector(w2v, window))\n",
    "\n",
    "    # Spherical K-means clustering\n",
    "    skm = KMeansClusterer(num_senses, nltk.cluster.util.cosine_distance, rng=random.Random(0), repeats=10)\n",
    "    assigned_clusters = skm.cluster(mean_vectors, assign_clusters=True)\n",
    "    global_classif.append(assigned_clusters)\n",
    "\n",
    "final_classif = []\n",
    "for i in range(len(global_classif)):\n",
    "    cluster_tags = couple(global_truth[i], global_classif[i])\n",
    "    final_classif.append([cluster_tags[i] for i in global_classif[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Taux de succès de prédiction : 0.935%\nTaux d'erreurs de prédiction : 0.0653%\n"
     ]
    }
   ],
   "source": [
    "res = precision(final_classif, global_truth)\n",
    "\n",
    "print(\"Taux de succès de prédiction : {:.3}%\".format(res))\n",
    "print(\"Taux d'erreurs de prédiction : {:.3}%\".format(1-res))"
   ]
  },
  {
   "source": [
    "## Remarque\n",
    "On peut observer un score très honorable de 93% de bonne prédiction. Cependant ce score est biaisé par les nombreux mot n'ayant qu'un seul sens."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Lemmes ayant plusieurs sens : 69 (9.3%)\n\nLemmes présents une seule fois : 432 (58.0%)\n\nLemmes n'ayant qu'un seul sens : 244 (32.8%)\n"
     ]
    }
   ],
   "source": [
    "polysem = {}\n",
    "solo = {}\n",
    "npolysem = {}\n",
    "\n",
    "for k,v in sens_dict.items():\n",
    "    if len(v) == 1:\n",
    "        solo[k] = v\n",
    "        continue\n",
    "    \n",
    "    _,_,_,sense_bn = v[0]\n",
    "    poly = False\n",
    "    for _,_,_,bn in v:\n",
    "        if bn != sense_bn:\n",
    "            polysem[k] = v\n",
    "            poly = True\n",
    "            break\n",
    "    if not poly:\n",
    "        npolysem[k] = v\n",
    "\n",
    "somme = len(polysem) + len(solo) + len(npolysem)\n",
    "\n",
    "print(\"Lemmes ayant plusieurs sens : {} ({:2.2}%)\".format(len(polysem), len(polysem)/somme*100))\n",
    "print(\"\\nLemmes présents une seule fois : {} ({:2.3}%)\".format(len(solo), len(solo)/somme*100))\n",
    "print(\"\\nLemmes n'ayant qu'un seul sens : {} ({:2.3}%)\".format(len(npolysem), len(npolysem)/somme*100))"
   ]
  },
  {
   "source": [
    "## Remarque\n",
    "On voit ici que le problème de desambiguïsation n'a d'interet que sur seulement 9% du corpus.*"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Résultats sur le test bis (sans tenir compte des mots non polysémiques)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Taux de succès de prédiction : 0.68%\nTaux d'erreurs de prédiction : 0.32%\n"
     ]
    }
   ],
   "source": [
    "global_truth = []\n",
    "global_classif = []\n",
    "\n",
    "for lemma, senses in polysem.items():\n",
    "    labels = lemma2Senses(lemma)\n",
    "    num_senses = len(labels)\n",
    "\n",
    "    # Map clusters with a value from, 0 to num_senses-1\n",
    "    truth = [labels.index(bn) for _,_,_,bn in senses]\n",
    "    global_truth.append(truth)\n",
    "\n",
    "    mean_vectors = []\n",
    "    for d,s,i,_ in senses:\n",
    "        l = len(documents[d][s])\n",
    "        # Extract the words in the contexte window\n",
    "        window = extractWindow(documents[d][s], i-math.floor((ctx_w-1)/2), i+math.ceil((ctx_w-1)/2))\n",
    "\n",
    "        # Compute the context vector (mean of the words vectors in the window)\n",
    "        mean_vectors.append(meanSentenceVector(w2v, window))\n",
    "\n",
    "    # Spherical K-means clustering\n",
    "    skm = KMeansClusterer(num_senses, nltk.cluster.util.cosine_distance, rng=random.Random(0), repeats=10)\n",
    "    assigned_clusters = skm.cluster(mean_vectors, assign_clusters=True)\n",
    "    global_classif.append(assigned_clusters)\n",
    "\n",
    "final_classif = []\n",
    "for i in range(len(global_classif)):\n",
    "    cluster_tags = couple(global_truth[i], global_classif[i])\n",
    "    final_classif.append([cluster_tags[i] for i in global_classif[i]])\n",
    "\n",
    "res = precision(final_classif, global_truth)\n",
    "\n",
    "print(\"Taux de succès de prédiction : {:.3}%\".format(res))\n",
    "print(\"Taux d'erreurs de prédiction : {:.3}%\".format(1-res))"
   ]
  },
  {
   "source": [
    "Les résultats sont cette fois-ci plus réalistes avec 68% de bonne prédictions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Méthode 2, méthode à base de définitions\n",
    "calule des similarités entre une occurence d'un mot et ses définitions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import xml.etree.ElementTree as et\n",
    "\n",
    "output_classif_path = \"test_corpus.classif\"\n",
    "\n",
    "documents, sens_dict = loadCorpus(trial_corpus_path)"
   ]
  },
  {
   "source": [
    "Charge le dictionnaire"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Dictionary length: 949\n"
     ]
    }
   ],
   "source": [
    "def load_dictionary(path):\n",
    "    dictionary_dict = {}\n",
    "\n",
    "    with open(path, \"r\") as file:\n",
    "        file.readline()\n",
    "        for row in file:\n",
    "            row = row.split(\";\")\n",
    "            lemma = row.pop(0)\n",
    "            nb = int(row.pop(0))\n",
    "            \n",
    "            # Make sure the definition is not empty...\n",
    "            if nb > 0:\n",
    "                ids = (row.pop(0)).split(\",\")\n",
    "                defs = (\";\".join(row)).split(\"\\\",\\\"\")\n",
    "                temp = defs[0].split(\",\\\"\")\n",
    "                defs = temp + defs[1:]\n",
    "\n",
    "                dictionary_dict[lemma] = (ids, defs)\n",
    "    \n",
    "    return dictionary_dict\n",
    "\n",
    "dictionary = load_dictionary(\"dict.dictionary\")\n",
    "print(\"Dictionary length:\", len(dictionary))"
   ]
  },
  {
   "source": [
    "### Exemple du contenu du dictionnaire pour le mot \"mardi\""
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(['bn:00078546n', 'bn:00118342n'],\n",
       " ['\"Le mardi est le jour de la semaine qui succède au lundi et qui précède le mercredi. Jour de la semaine Le deuxième jour de la semaine en Europe et dans les pays utilisant la norme ISO 8601; le troisième jour de la semaine aux États-Unis d\\'Amérique.',\n",
       "  'Mardi est le troisième livre publié par l\\'écrivain américain Herman Melville.\"\\n'])"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "dictionary[\"mardi\"]"
   ]
  },
  {
   "source": [
    "## Word2Vec\n",
    "Apprend les embeddings sur le corpus et les definitions du dictionnaire"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Extended w2v Vocab size: 25722\n"
     ]
    }
   ],
   "source": [
    "def definitionsSentences():\n",
    "    defSentences = []\n",
    "    for _,defs in dictionary.values():\n",
    "        for s in defs:\n",
    "            defSentences.append(s.split())\n",
    "    return defSentences\n",
    "\n",
    "def createExtendedW2vModel():\n",
    "    return Word2Vec(documentsSentences(documents)+definitionsSentences(), min_count=1)\n",
    "\n",
    "w2v = createExtendedW2vModel()\n",
    "\n",
    "print(\"Extended w2v Vocab size:\", len(w2v.wv.vocab))"
   ]
  },
  {
   "source": [
    "## Calcule des vecteurs de définitions\n",
    "Un vecteur de définition est la moyenne des embeddings des mots d'une définition"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "defsVectors = {} #map id lemma -> vecteur\n",
    "\n",
    "for k,(ids,defs) in dictionary.items():\n",
    "    defsVectors[k] = np.array([meanSentenceVector(w2v,d.split()) for d in defs]).reshape((len(defs), 100))"
   ]
  },
  {
   "source": [
    "## Test association mot->sens\n",
    "Associe un vecteur de contexte avec le vecteur de définition le plus proche.<br/>\n",
    "La notion de \"proche\" se traduit par une similarité cosine.<br/>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Corpus de dev\n",
    "Exemple d'association mot->sens pour les occurences du mots journal sur le dev"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "---- Phrase :\n\t Le mardi , les participants à la conférence ont été informés d' une autre atrocité , l' assassinat à Medellin de deux employés d' El_Espectador , le deuxième plus grand journal de Colombie .\n\tDocument 0, phrase 5 à la position 30 (BabelNet : bn:00057563n)\n---- Sortie :\n\tid BabelNet prédit :  bn:00057563n\n\tDéfinition prédite :  L'expression presse écrite désigne, d'une manière générale, l'ensemble des moyens de diffusion de l’information écrite, ce qui englobe notamment les journaux quotidiens, les publications périodiques et les organismes professionnels liés à la diffusion de l'information. Périodique présentant des nouvelles et autres informations diverses Publication (en général publiée chaque semaine ou chaque mois et imprimé sur du papier de basse qualité) qui contient des actualités et d'autres articles. Journal qui est publié dans la soirée. Publication publiée une ou deux fois par jour (en général imprimé sur un papier de faible qualité) qui contient les actualités et d'autres articles.\nSimilarité cosine :  0.99987304\n\n\n\n---- Phrase :\n\t L ’ administrateur local du journal , Luz Maria Lopez , a été abattue et sa mère blessée , tandis que sa voiture était arrêtée à un feu_rouge .\n\tDocument 0, phrase 6 à la position 5 (BabelNet : bn:00057563n)\n---- Sortie :\n\tid BabelNet prédit :  bn:01658173n\n\tDéfinition prédite :  Un journal est la partie d'un système de fichiers journalisé qui trace les opérations d'écriture tant qu'elles ne sont pas terminées et cela en vue de garantir l'intégrité des données en cas d'arrêt brutal.\nSimilarité cosine :  0.9998664\n\n\n\n---- Phrase :\n\t Une heure plus tard , le directeur de la diffusion du journal , Miguel Soler , a été abattu près de son domicile .\n\tDocument 0, phrase 7 à la position 11 (BabelNet : bn:00057563n)\n---- Sortie :\n\tid BabelNet prédit :  bn:00057563n\n\tDéfinition prédite :  L'expression presse écrite désigne, d'une manière générale, l'ensemble des moyens de diffusion de l’information écrite, ce qui englobe notamment les journaux quotidiens, les publications périodiques et les organismes professionnels liés à la diffusion de l'information. Périodique présentant des nouvelles et autres informations diverses Publication (en général publiée chaque semaine ou chaque mois et imprimé sur du papier de basse qualité) qui contient des actualités et d'autres articles. Journal qui est publié dans la soirée. Publication publiée une ou deux fois par jour (en général imprimé sur un papier de faible qualité) qui contient les actualités et d'autres articles.\nSimilarité cosine :  0.99982953\n\n\n\n---- Phrase :\n\t Les seigneurs_de_la_drogue qui ont revendiqué la responsabilité des assassinats ont affirmé qu' ils allait faire exploser les bureaux du journal de Bogota si on continuerait à le distribuer à Medellin .\n\tDocument 0, phrase 8 à la position 19 (BabelNet : bn:00057564n)\n---- Sortie :\n\tid BabelNet prédit :  bn:00057563n\n\tDéfinition prédite :  L'expression presse écrite désigne, d'une manière générale, l'ensemble des moyens de diffusion de l’information écrite, ce qui englobe notamment les journaux quotidiens, les publications périodiques et les organismes professionnels liés à la diffusion de l'information. Périodique présentant des nouvelles et autres informations diverses Publication (en général publiée chaque semaine ou chaque mois et imprimé sur du papier de basse qualité) qui contient des actualités et d'autres articles. Journal qui est publié dans la soirée. Publication publiée une ou deux fois par jour (en général imprimé sur un papier de faible qualité) qui contient les actualités et d'autres articles.\nSimilarité cosine :  0.99978876\n\n\n\n---- Phrase :\n\t Noriega a fermé tous les journaux et toutes les stations de radio et de télévision indépendantes , et il a fait arrêter ou torturer , ou bien il a contraint à l' exil , une longue liste de journalistes '' affirme la déclaration .\n\tDocument 0, phrase 18 à la position 5 (BabelNet : bn:00057563n)\n---- Sortie :\n\tid BabelNet prédit :  bn:00057563n\n\tDéfinition prédite :  L'expression presse écrite désigne, d'une manière générale, l'ensemble des moyens de diffusion de l’information écrite, ce qui englobe notamment les journaux quotidiens, les publications périodiques et les organismes professionnels liés à la diffusion de l'information. Périodique présentant des nouvelles et autres informations diverses Publication (en général publiée chaque semaine ou chaque mois et imprimé sur du papier de basse qualité) qui contient des actualités et d'autres articles. Journal qui est publié dans la soirée. Publication publiée une ou deux fois par jour (en général imprimé sur un papier de faible qualité) qui contient les actualités et d'autres articles.\nSimilarité cosine :  0.99950945\n\n\n\n"
     ]
    }
   ],
   "source": [
    "mean_vectors = []\n",
    "for d,s,p,bn in sens_dict[\"journal\"]:\n",
    "    window = extractWindow(documents[d][s], p-math.floor((ctx_w-1)/2), p+math.ceil((ctx_w-1)/2))\n",
    "    mean_vectors.append(meanSentenceVector(w2v, window))\n",
    "\n",
    "# Occurences du mot journal dans le corpus de dev\n",
    "journaux = [(0, 5, 30, 'bn:00057563n'), (0, 6, 5, 'bn:00057563n'), (0, 7, 11, 'bn:00057563n'), (0, 8, 19, 'bn:00057564n'), (0, 18, 5, 'bn:00057563n')]\n",
    "\n",
    "for i in range(len(mean_vectors)):\n",
    "    csim = w2v.wv.cosine_similarities(mean_vectors[i], defsVectors[\"journal\"])\n",
    "    amax = np.argmax(csim)\n",
    "    d,s,p,bn = journaux[i]\n",
    "    print(\"---- Phrase :\\n\\t\", \" \".join(documents[d][s]))\n",
    "    print(\"\\tDocument %d, phrase %d à la position %d (BabelNet : %s)\"%(d, s, p, bn))\n",
    "    print(\"---- Sortie :\")\n",
    "    print(\"\\tid BabelNet prédit : \", dictionary[\"journal\"][0][amax])\n",
    "    print(\"\\tDéfinition prédite : \", dictionary[\"journal\"][1][amax])\n",
    "    print(\"Similarité cosine : \", csim[amax])\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "source": [
    "### Corpus de test\n",
    "Le résultat est disponible dans le fichier \"test_corpus.classif\"<br/>\n",
    "Le dictionnaire n'étant pas parfait, la procédure suivante peut échouer à trouver une définition à un mot. Cale est soit dû au fait que le dictionnaire n'associe aucune définition au mot ou bien dû à la façon dont la recherche s'effectue dans le dictionnaire (sensibilité de la casse, format différent, tirets...)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Missing definition for \"récrimination\"\nMissing definition for \"washington\"\nMissing definition for \"yvo_de_boer\"\nMissing definition for \"union_européenne\"\nMissing definition for \"altercation\"\nMissing definition for \"administration_obama\"\nMissing definition for \"délégué\"\nMissing definition for \"délégué\"\nMissing definition for \"palais_des_congrès\"\nMissing definition for \"politique_mondiale\"\nMissing definition for \"caja_laboral\"\nMissing definition for \"nasdaq\"\nMissing definition for \"nasdaq\"\nMissing definition for \"europe\"\nMissing definition for \"europe\"\nMissing definition for \"irak\"\nMissing definition for \"irak\"\nMissing definition for \"irak\"\nMissing definition for \"irak\"\nMissing definition for \"irak\"\nMissing definition for \"irak\"\nMissing definition for \"internet\"\nMissing definition for \"internet\"\nMissing definition for \"instabilité\"\nMissing definition for \"twitter\"\nMissing definition for \"twitter\"\nMissing definition for \"twitter\"\nMissing definition for \"twitter\"\nMissing definition for \"twitter\"\nMissing definition for \"demi-douzaine\"\nMissing definition for \"connecticut\"\nMissing definition for \"connecticut\"\nMissing definition for \"connecticut\"\nMissing definition for \"cours_d'appel\"\nMissing definition for \"cours_d'appel\"\nMissing definition for \"cleveland\"\nMissing definition for \"demi-siècle\"\nMissing definition for \"stanford\"\nMissing definition for \"ingéniosité\"\nMissing definition for \"la_ligue_des_champions\"\nMissing definition for \"la_ligue_des_champions\"\nMissing definition for \"circonstance\"\nMissing definition for \"cachette\"\nMissing definition for \"désavantage\"\nMissing definition for \"indétermination\"\nMissing definition for \"arizona\"\nMissing definition for \"arizona\"\nMissing definition for \"nasa\"\nMissing definition for \"nasa\"\nMissing definition for \"institut_national_de_la_santé\"\nMissing definition for \"soutien\"\nMissing definition for \"soutien\"\nMissing definition for \"soutien\"\nMissing definition for \"soutien\"\nMissing definition for \"soutien\"\nMissing definition for \"the_economist\"\nMissing definition for \"amérique_du_sud\"\nMissing definition for \"venezuela\"\nMissing definition for \"venezuela\"\nMissing definition for \"venezuela\"\nMissing definition for \"équateur\"\nMissing definition for \"république_dominicaine\"\nMissing definition for \"point_de_ralliement\"\nMissing definition for \"collecte_de_fonds\"\nMissing definition for \"collecte_de_fonds\"\nMissing definition for \"ministre_de_l'intérieur\"\nMissing definition for \"google\"\nMissing definition for \"ebay\"\nMissing definition for \"yahoo\"\nMissing definition for \"usa\"\nMissing definition for \"usa\"\nMissing definition for \"usa\"\nMissing definition for \"usa\"\n"
     ]
    }
   ],
   "source": [
    "documents, sens_dict = loadCorpus(test_corpus_path)\n",
    "\n",
    "w2v = createExtendedW2vModel()\n",
    "w2v.save(\"w2v_extended.model\")\n",
    "\n",
    "with open(output_classif_path, \"w\") as file:\n",
    "    file.write(\"lemma;doc_id;sent_id;sent_pos;output_bn_id;output_def\\n\")\n",
    "\n",
    "    for lemma,v in sens_dict.items():\n",
    "        for d,s,p,bn in v:\n",
    "            window = extractWindow(documents[d][s], p-math.floor((ctx_w-1)/2), p+math.ceil((ctx_w-1)/2))\n",
    "            mean_vector = meanSentenceVector(w2v, window)\n",
    "            \n",
    "            lemma = lemma.lower()\n",
    "            if lemma in defsVectors:\n",
    "                csim = w2v.wv.cosine_similarities(mean_vector, defsVectors[lemma])\n",
    "\n",
    "                amax = np.argmax(csim)\n",
    "\n",
    "                bn_id = dictionary[lemma][0][amax]\n",
    "                bn_def = dictionary[lemma][1][amax]\n",
    "\n",
    "                file.write(\"{};{};{};{};{};\\\"{}\\\"\\n\".format(lemma, d, s, p, bn_id, bn_def))\n",
    "            else:\n",
    "                print(\"Missing definition for \\\"%s\\\"\"%(lemma))"
   ]
  },
  {
   "source": [
    "## Discussion\n",
    "Les résultats obtenus sont tres interessant mais extrement dépendant de la qualité des définitions BabelNet ainsi que de leur nombre.<br/>\n",
    "- Il parrait assez evidant que plus un mot est polysemique et plus la méthode a de chance de se tromper de sens (certains mots comme \"état\" et \"liberté\" ont 21 sens BabelNet)\n",
    "- La redondance des définitons BabelNet ne permet pas de simplement évaluer les définitions une à une et en tirer un score de classification. Par exemple, pour le mot \"journaliste\", sur les 6 concepts, 4 définissent le meme concept de \"journaliste de presse\".\n",
    "- La représentation vectorielle d'une définission est dépendante de la qualité des définitions BabelNet qui sont parfois très succinctes voire inexistantes."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}