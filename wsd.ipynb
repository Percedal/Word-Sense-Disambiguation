{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Dev le(s) modèle(s) de désambiguïsation lexicale"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "import nltk\n",
    "import random\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import decomposition\n",
    "from xml.dom.minidom import parse\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.cluster.kmeans import KMeansClusterer\n",
    "\n",
    "trial_corpus_path = \"trial_corpus.xml\"\n",
    "test_corpus_path = \"test_corpus.xml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Documents (1):\n\tDoc  0: 36 sentences\n\nLemmas (124):\n\tLemma 0: guerre_contre_la_drogue -> 1 values\n\tLemma 1: Amérique_Latine -> 2 values\n\tLemma 2: presse -> 9 values\n\tLemma 3: mois -> 2 values\n\tLemma 4: journaliste -> 5 values\n\tLemma 5: trafiquant_de_drogue -> 2 values\n\tLemma 6: guérillero -> 1 values\n\tLemma 7: gauche -> 2 values\n\tLemma 8: personne -> 1 values\n\tLemma 9: Colombie -> 4 values\n\t...\n\n\nEx. lemma 0: \n\t guerre_contre_la_drogue -> [(0, 0, 7, 'bn:00028885n')]\n"
     ]
    }
   ],
   "source": [
    "def loadCorpus(path):\n",
    "    \"\"\"Load a formatted corpus data file\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path: str\n",
    "        Path to the corpus xml file to load\n",
    "\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        A 3 dimensional list containing for each document, the sentences that it is composed of.\n",
    "        Where each sentence is a list of single tokens.\n",
    "    dict\n",
    "        A dictionnary mapping a lemma with it's document/sentence/index position and the BabelNet Sense attributed\n",
    "    \"\"\"\n",
    "\n",
    "    DOMTree = parse(path)\n",
    "\n",
    "    documents = []\n",
    "    sens_dict = {}\n",
    "    for doc in DOMTree.getElementsByTagName(\"document\"):\n",
    "        # For each document\n",
    "        sentences = []\n",
    "        for sent in doc.getElementsByTagName(\"sentence\"):\n",
    "            # And for each sentence\n",
    "            # Append the new sentence\n",
    "            s = sent.getAttribute(\"s\")\n",
    "            sentences.append(s.split())\n",
    "\n",
    "            # Map the lemmas in the sentence with it's doc/sentence/index position and BabelNet sense\n",
    "            for lem in sent.getElementsByTagName(\"lemma\"):\n",
    "                idx = lem.getAttribute(\"idx\")\n",
    "                lemma = lem.getAttribute(\"lemma\")\n",
    "                # Few lemma may have more than 1 BabelNet sense (due to redundancy in BN)\n",
    "                # Only keep the 1st one\n",
    "                sense = lem.getAttribute(\"senses\").split()[0] \n",
    "                \n",
    "                ctx = (int(doc.getAttribute(\"id\")),\n",
    "                        int(sent.getAttribute(\"id\")),\n",
    "                        int(idx),\n",
    "                        sense)\n",
    "                if not lemma in sens_dict:\n",
    "                    sens_dict[lemma] = [ctx]\n",
    "                else:\n",
    "                    sens_dict[lemma].append(ctx)\n",
    "\n",
    "        documents.append(sentences)\n",
    "\n",
    "    return (documents, sens_dict)\n",
    "\n",
    "\n",
    "documents, sens_dict = loadCorpus(trial_corpus_path)\n",
    "\n",
    "\n",
    "print(\"Documents (%d):\"%(len(documents)))\n",
    "for i, d in enumerate(documents):\n",
    "    print(\"\\tDoc %2d: %02d sentences\"%(i, len(d)))\n",
    "\n",
    "print(\"\\nLemmas (%d):\"%(len(sens_dict)))\n",
    "for i, (k, v) in zip(range(10), sens_dict.items()):\n",
    "    print(\"\\tLemma %d: %s -> %d values\"%(i, k, len(v)))\n",
    "print(\"\\t...\\n\")\n",
    "print(\"\\nEx. lemma 0: \")\n",
    "print(\"\\t\", list(sens_dict.keys())[0], \"->\", list(sens_dict.values())[0])"
   ]
  },
  {
   "source": [
    "# Word2vec"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "w2v Vocab size: 429\n"
     ]
    }
   ],
   "source": [
    "def documentsSentences(doc):\n",
    "    docSentences = []\n",
    "    for d in doc:\n",
    "        for s in d:\n",
    "            docSentences.append(s)\n",
    "    return docSentences\n",
    "\n",
    "def createW2vModel():\n",
    "    return Word2Vec(documentsSentences(documents), min_count=1)\n",
    "\n",
    "try:\n",
    "    w2v = Word2Vec.load(\"w2v.model\")\n",
    "except FileNotFoundError:\n",
    "    w2v = createW2vModel()\n",
    "    w2v.save(\"w2v.model\")\n",
    "\n",
    "print(\"w2v Vocab size:\", len(w2v.wv.vocab))"
   ]
  },
  {
   "source": [
    "# Huang"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nEx. guerre_contre_la_drogue\n\t-> [(0, 0, 7, 'bn:00028885n')]\n\t-> ['bn:00028885n']\n"
     ]
    }
   ],
   "source": [
    "def lemma2Senses(lemma):\n",
    "    senses = [bn for _,_,_,bn in sens_dict[lemma]]\n",
    "    return list(set(senses))\n",
    "\n",
    "print(\"\\nEx.\", list(sens_dict.keys())[0])\n",
    "print(\"\\t->\", list(sens_dict.values())[0])\n",
    "print(\"\\t->\", lemma2Senses(list(sens_dict.keys())[0]))"
   ]
  },
  {
   "source": [
    "## Visualisation rapide des lemmes à désambiguïser\n",
    "On remarque que peu de lemmes sont associés à plusieurs sens. Certains apparaissent plusieurs fois avec toujours le meme sens. Pire ! D'autres n'apparaissent qu'une seule fois.<br/>\n",
    "Il est aussi intéressant de remarquer que certains lemmes sont associés 9 fois avec le sens_1 et 1 fois avec le sens_2. Ceci peut trouver son origine dans les annotations via BabelNet qui propose différents sens redondants d'un mot.<br/>\n",
    "Pour exemple, le lemme <i>journaliste</i> est associé aux sens BabelNet suivants :\n",
    "<ol>\n",
    "    <li>bn:00048461n : celui qui recueille, écrit ou distribue des informations</li>\n",
    "    <li>bn:00057562n : celui qui enquête, rapporte ou rédige les actualités</li>\n",
    "</ol>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Nb lemmas: 124\n\npolysems: 3\n\t journaliste  ->  [(0, 1, 8, 'bn:00048461n'), (0, 2, 10, 'bn:00048461n'), (0, 16, 43, 'bn:00048461n'), (0, 18, 38, 'bn:00057562n'), (0, 19, 27, 'bn:00048461n')]\n\t contrôle  ->  [(0, 3, 29, 'bn:00022287n'), (0, 26, 19, 'bn:00022283n')]\n\t journal  ->  [(0, 5, 30, 'bn:00057563n'), (0, 6, 5, 'bn:00057563n'), (0, 7, 11, 'bn:00057563n'), (0, 8, 19, 'bn:00057564n'), (0, 18, 5, 'bn:00057563n')]\n\nsolo: 86\n\t guerre_contre_la_drogue  ->  [(0, 0, 7, 'bn:00028885n')]\n\t guérillero  ->  [(0, 1, 22, 'bn:02557244n')]\n\t personne  ->  [(0, 1, 33, 'bn:00046516n')]\n\t année  ->  [(0, 2, 5, 'bn:00078738n')]\n\t août  ->  [(0, 3, 18, 'bn:00007140n')]\n\nnon polysem: 35\n\t Amérique_Latine  ->  [(0, 0, 9, 'bn:00050165n'), (0, 4, 14, 'bn:00050165n')]\n\t presse  ->  [(0, 0, 23, 'bn:00064245n'), (0, 4, 38, 'bn:00064245n'), (0, 12, 4, 'bn:00064245n'), (0, 16, 29, 'bn:00064245n'), (0, 17, 19, 'bn:00064245n'), (0, 19, 19, 'bn:00064245n'), (0, 20, 9, 'bn:00064245n'), (0, 21, 28, 'bn:00064245n'), (0, 35, 1, 'bn:00064245n')]\n\t mois  ->  [(0, 1, 5, 'bn:00014710n'), (0, 9, 8, 'bn:00014710n')]\n\t trafiquant_de_drogue  ->  [(0, 1, 19, 'bn:00028881n'), (0, 17, 11, 'bn:00028881n')]\n\t gauche  ->  [(0, 1, 24, 'bn:00149192n'), (0, 21, 49, 'bn:00149192n')]\n"
     ]
    }
   ],
   "source": [
    "polysem = {}\n",
    "solo = {}\n",
    "npolysem = {}\n",
    "\n",
    "for k,v in sens_dict.items():\n",
    "    if len(v) == 1:\n",
    "        solo[k] = v\n",
    "        continue\n",
    "    \n",
    "    _,_,_,sense_bn = v[0]\n",
    "    poly = False\n",
    "    for _,_,_,bn in v:\n",
    "        if bn != sense_bn:\n",
    "            polysem[k] = v\n",
    "            poly = True\n",
    "            break\n",
    "    if not poly:\n",
    "        npolysem[k] = v\n",
    "\n",
    "print(\"Nb lemmas:\", len(sens_dict))\n",
    "print()\n",
    "\n",
    "print(\"polysems:\", len(polysem))\n",
    "for _, (k, v) in zip(range(5), polysem.items()):\n",
    "    print(\"\\t\", k, \" -> \", v)\n",
    "\n",
    "print(\"\\nsolo:\", len(solo))\n",
    "for _, (k, v) in zip(range(5), solo.items()):\n",
    "    print(\"\\t\", k, \" -> \", v)\n",
    "\n",
    "print(\"\\nnon polysem:\", len(npolysem))\n",
    "for _, (k, v) in zip(range(5), npolysem.items()):\n",
    "    print(\"\\t\", k, \" -> \", v)"
   ]
  },
  {
   "source": [
    "## Implémentation de la méthode proposée par Huang\n",
    "<ol>\n",
    "    <li>Collecte les fenetres d'occurrence d'un mot </li>\n",
    "    <li>Calcule le vecteur de contexte, moyenne des vecteurs-mots de chaque mots dans un contexte</li>\n",
    "    <li>Cluster les vecteurs de contextes (spherical K-means)</li>\n",
    "    <li>Associe à chaque cluster un sens</li>\n",
    "</ol>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meanSentenceVector(w2vModel, sentence):\n",
    "    return sumSentenceVector(w2vModel, sentence)\n",
    "    #return np.array([w2vModel.wv[word] for word in sentence]).mean(axis=0)\n",
    "\n",
    "def sumSentenceVector(w2vModel, sentence):\n",
    "    return np.array([w2vModel.wv[word] for word in sentence]).sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0, 0, 0, 1, 0]\n",
      "[0, 1, 0, 1, 1]\n",
      "\n",
      "[1, 0]\n",
      "[0, 1]\n",
      "\n",
      "[1, 1, 1, 0, 1]\n",
      "[0, 0, 0, 1, 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ctx_w = 11 # Contexte window size\n",
    "\n",
    "global_truth = []\n",
    "global_classif = []\n",
    "\n",
    "for lemma, senses in polysem.items():\n",
    "    labels = lemma2Senses(lemma)\n",
    "    num_senses = len(labels)\n",
    "\n",
    "    # Map clusters with a value from, 0 to num_senses-1\n",
    "    truth = [labels.index(bn) for _,_,_,bn in senses]\n",
    "    global_truth.append(truth)\n",
    "\n",
    "    mean_vectors = []\n",
    "    for d,s,i,_ in senses:\n",
    "        l = len(documents[d][s])\n",
    "        # Extract the words in the contexte window\n",
    "        window = documents[d][s][max(0, i-math.floor((ctx_w-1)/2)) : min(l, i+math.ceil((ctx_w-1)/2))+1]\n",
    "\n",
    "        # Compute the context vector (mean of the words vectors in the window)\n",
    "        mean_vectors.append(meanSentenceVector(w2v, window))\n",
    "\n",
    "    # Spherical K-means clustering\n",
    "    skm = KMeansClusterer(num_senses, nltk.cluster.util.cosine_distance, rng=random.Random(0), repeats=10)\n",
    "    assigned_clusters = skm.cluster(mean_vectors, assign_clusters=True)\n",
    "    global_classif.append(assigned_clusters)\n",
    "\n",
    "    print(truth)\n",
    "    print(assigned_clusters)\n",
    "    print()\n",
    "\n",
    "#print(sklearn.metrics.classification_report([y for x in global_truth for y in x], [y for x in global_classif for y in x]))\n"
   ]
  },
  {
   "source": [
    "### Associe à chaque cluster un sens"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{0: 0, 1: 1}"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "def argsmax(lst):\n",
    "    max = lst[0]\n",
    "    argsmax = []\n",
    "    for i in range(len(lst)):\n",
    "        if lst[i] == max:\n",
    "            argsmax.append(i)\n",
    "        elif lst[i] > max:\n",
    "            max = lst[i]\n",
    "            argsmax = [i]\n",
    "    return argsmax\n",
    "\n",
    "def couple(arr1, arr2):\n",
    "    # truth, classif\n",
    "    l = len(arr1)\n",
    "    nb_c = len(set(arr1))\n",
    "    arr = np.zeros((nb_c,nb_c), dtype=int)\n",
    "\n",
    "    for i in range(l):\n",
    "        arr[arr2[i],arr1[i]] += 1\n",
    "\n",
    "    cs = [i for i in range(len(arr))]\n",
    "    ts = [i for i in range(len(arr))]\n",
    "    map = {}\n",
    "\n",
    "    for iter in range(len(arr)):\n",
    "        temp = len(arr)\n",
    "        for i in range(len(arr)):\n",
    "            c = arr[i]\n",
    "            ams = argsmax(c)\n",
    "            if len(ams) == 1:\n",
    "                ams = ams[0]\n",
    "                map[cs[i]] = ts[ams]\n",
    "                arr = np.delete(np.delete(arr, ams, 1), i, 0)\n",
    "                cs = np.delete(cs, i)\n",
    "                ts = np.delete(ts, ams)\n",
    "                break\n",
    "        \n",
    "        if len(arr) == temp:\n",
    "            j = np.argmax(arr[0])\n",
    "            map[cs[0]] = ts[j]\n",
    "            arr = np.delete(np.delete(arr, j, 1), 0, 0)\n",
    "            cs = np.delete(cs, 0)\n",
    "            ts = np.delete(ts, j)\n",
    "\n",
    "    return map\n",
    "\n",
    "couple(global_truth[0], global_classif[0])"
   ]
  },
  {
   "source": [
    "## Premiers Résultats"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "journaliste\n[(0, 1, 8, 'bn:00048461n'), (0, 2, 10, 'bn:00048461n'), (0, 16, 43, 'bn:00048461n'), (0, 18, 38, 'bn:00057562n'), (0, 19, 27, 'bn:00048461n')]\n[0, 1, 0, 1, 1]\n[0, 0, 0, 1, 0]\n\ncontrôle\n[(0, 3, 29, 'bn:00022287n'), (0, 26, 19, 'bn:00022283n')]\n[1, 0]\n[1, 0]\n\njournal\n[(0, 5, 30, 'bn:00057563n'), (0, 6, 5, 'bn:00057563n'), (0, 7, 11, 'bn:00057563n'), (0, 8, 19, 'bn:00057564n'), (0, 18, 5, 'bn:00057563n')]\n[1, 1, 1, 0, 0]\n[1, 1, 1, 0, 1]\n\n"
     ]
    }
   ],
   "source": [
    "final_classif = []\n",
    "for i in range(len(global_classif)):\n",
    "    cluster_tags = couple(global_truth[i], global_classif[i])\n",
    "    final_classif.append([cluster_tags[i] for i in global_classif[i]])\n",
    "\n",
    "    print(list(polysem.keys())[i])\n",
    "    print(list(polysem.values())[i])\n",
    "    print(final_classif[i])\n",
    "    print(global_truth[i])\n",
    "    print()"
   ]
  },
  {
   "source": [
    "Score associations clustering/gold truth"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "              precision    recall  f1-score   support\n\n           0       0.80      0.67      0.73         6\n           1       0.71      0.83      0.77         6\n\n    accuracy                           0.75        12\n   macro avg       0.76      0.75      0.75        12\nweighted avg       0.76      0.75      0.75        12\n\n"
     ]
    }
   ],
   "source": [
    "print(sklearn.metrics.classification_report([y for x in global_truth for y in x], [y for x in final_classif for y in x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Dictionary length: 119\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "def load_dictionary(path):\n",
    "    dictionary_dict = {}\n",
    "\n",
    "    with open(path, \"r\") as file:\n",
    "        file.readline()\n",
    "        for row in file:\n",
    "            row = row.split(\";\")\n",
    "            lemma = row.pop(0)\n",
    "            nb = int(row.pop(0))\n",
    "            \n",
    "            # Make sure the definition is not empty...\n",
    "            if nb > 0:\n",
    "                ids = (row.pop(0)).split(\",\")\n",
    "                defs = (\";\".join(row)).split(\"\\\",\\\"\")\n",
    "                temp = defs[0].split(\",\\\"\")\n",
    "                defs = temp + defs[1:]\n",
    "\n",
    "                dictionary_dict[lemma] = (ids, defs)\n",
    "    \n",
    "    return dictionary_dict\n",
    "\n",
    "dictionary = load_dictionary(\"dict.dictionary\")\n",
    "print(\"Dictionary length:\", len(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(['bn:00078546n', 'bn:00118342n'],\n",
       " ['\"Le mardi est le jour de la semaine qui succède au lundi et qui précède le mercredi. Jour de la semaine Le deuxième jour de la semaine en Europe et dans les pays utilisant la norme ISO 8601; le troisième jour de la semaine aux États-Unis d\\'Amérique.',\n",
       "  'Mardi est le troisième livre publié par l\\'écrivain américain Herman Melville.\"\\n'])"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "dictionary[\"mardi\"]"
   ]
  },
  {
   "source": [
    "Apprend les embeddings avec les definitions des mots"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Word2Vec"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Extended w2v Vocab size: 6628\n"
     ]
    }
   ],
   "source": [
    "def definitionsSentences():\n",
    "    defSentences = []\n",
    "    for _,defs in dictionary.values():\n",
    "        for s in defs:\n",
    "            defSentences.append(s.split())\n",
    "    return defSentences\n",
    "\n",
    "def createExtendedW2vModel():\n",
    "    return Word2Vec(documentsSentences(documents)+definitionsSentences(), min_count=1)\n",
    "\n",
    "try:\n",
    "    w2v = Word2Vec.load(\"w2v.model.extended\")\n",
    "except FileNotFoundError:\n",
    "    w2v = createExtendedW2vModel()\n",
    "    w2v.save(\"w2v.model.extended\")\n",
    "\n",
    "print(\"Extended w2v Vocab size:\", len(w2v.wv.vocab))"
   ]
  },
  {
   "source": [
    "Calcule des vecteurs de définitions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "defsVectors = {} #map id BN -> vecteur\n",
    "\n",
    "for k,(ids,defs) in dictionary.items():\n",
    "    defsVectors[k] = np.array([meanSentenceVector(w2v,d.split()) for d in defs]).reshape((len(defs), 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[(0, 16, 23, 'bn:00074009n')]\n(['bn:00031647n', 'bn:00063703n', 'bn:03719156n', 'bn:00023235n', 'bn:00964536n', 'bn:00007255n', 'bn:01606252n', 'bn:03347921n', 'bn:00074011n', 'bn:02220665n', 'bn:00026336n', 'bn:02108701n', 'bn:00889834n', 'bn:00031648n', 'bn:00023236n', 'bn:03157857n', 'bn:04721822n', 'bn:00064914n', 'bn:00029464n', 'bn:13858447n', 'bn:01110993n'], ['\"Le patrimoine ou patrimoine net d\\'un individu, d\\'une famille ou d\\'un organisme est : le patrimoine brut, ensemble des biens qu\\'il possède, évalués au prix du marché ; moins ses dettes et autres engagements financiers. Ensemble des biens d\\'\\'une personne ou entité', \"Le droit personnel, ou droit de créance est le droit d'exiger d'une personne, le débiteur, une obligation quelconque, que détient le créancier.\", \"En gravure, un état est une forme différente d'une estampe, causée par un changement délibéré et permanent effectué sur une matrice telle qu'une plaque de cuivre ou un bloc de bois.\", \"Un Land est un État fédéré d'Allemagne. Un pays est un territoire habité, constituant une entité géographique et humaine. En Allemagne Subdivision politique allemande Région généralement identifiée comme une entité géopolitique distincte Entité politique ayant autorité sur une aire géographique. Pays vient du latin pagus qui désignait une subdivision territoriale et tribale d'étendue restreinte.\", \"Un État fédéré est un État qui fait partie d'un État fédéral. État qui fait partie d''un État fédéral\", 'Le Commonwealth d\\'Australie est une fédération composée de six États et de dix territoires. Division administrative principale de l\\'\\'australie, ou subdivision des États en \"Territoires\"', 'La République fédérative du Brésil comporte 26 États et un district fédéral. Tyoe de division administrative du Brésil', \"En automatique, une représentation d'état permet de modéliser un système dynamique en utilisant des variables d'état. En calcul : ensemble de tous les paramètres se rapportant à un calcul.\", 'En physique, un état de la matière correspond à un certain degré de cohérence de la matière  Forme que peut prendre la matière Une des trois conditions fondamentales de la matière : solide, liquide ou gazeuse.', \"En informatique, le terme désigne deux choses assez différentes : Valeurs immédiates des variables sur lesquelles un programme d''ordinateur a accès En informatique : condition stable d'un processeur pendant un cycle d'horloge.\", \"Le département d'État est, aux États-Unis, le département exécutif fédéral chargé des relations internationales. Réalise les relations diplomatiques des États-Unis d''Amérique avec d''autres pays\", 'En calcul : valeur de tous les paramètres en un moment donné du calcul.', \"La technique du patron de conception, ou encore modèle de conception, comportemental état utilisé en génie logiciel est utilisé entre autres lorsqu'il est souhaité pouvoir changer le comportement d'un objet sans pour autant en changer l'instance.\", \"La société d'ordres est une théorie d'ordonnancement social selon laquelle la distinction sociale repose sur une hiérarchie de dignité et d'honneur.\", \"L’État possède une triple signification : sociologique ; organisationnelle ; juridique. Un État souverain moderne est, selon la Convention de Montevideo, un État qui possède quatre propriétés : une population permanente, un territoire déterminé, un gouvernement qui n'est subordonné à aucun autre, une capacité d'entrer en relations avec les autres États. Le sens moderne de nation est assez proche de celui de peuple, mais ajoute souvent l'idée d'État. Groupe humain uni par des caractéristiques communes ou un sentiment d''appartenance commun Organisation politique et juridique d''un territoire délimité Organisation politique souveraine sur son territoire Entité politique ayant autorité sur une aire géographique. La région géographique sous le contrôle d'un État politique. Peuple occupant de manière permanente un territoire fixe, lié ensemble par une loi commune, des habitudes et des coutumes dans une seule entité polituique, par l'intermédiaire d'un gouvernement organisé, qui exerce une souveraineté et un contrôle indépendants de toutes les personnes et choses dans son territoire, à moins que ou jusqu'à ce que l'autorité soit remplacée par une fédération ou une union d'autres états. Le sens moderne de est assez proche de celui de, mais ajoute souvent l'idée de, d', de gouvernement, de. L’État est une entité gouvernant l’organisation d'un pays, dont la structure est juridique, qui est délimité par des frontières territoriales et constitué d’institutions lui assurant un pouvoir suprême.\", \"Au Canada, la propriété publique est la propriété de l'État ou entité assimilable, par opposition à la propriété privée des particuliers, des communautés de particuliers et des entreprises.\", 'Pour l’article concernant l’État français, voir Régime de Vichy.', \"Une province est une division administrative. Division administrative d'un pays ou d'un État Subdivision administrative d'un pays, dans certains cas relativement autonome et équivalent à un état, dans d'autres cas, plus petit, moins autonome et plus proche d'un comté. Division politique d'une fédération qui garde un certain degré d'autonomie.\", \"L'Est des États-Unis désigne une grande région de l'Est des États-Unis globalement constituée des États situés à l'Est du fleuve Mississippi. Région géographique des États-Unis\", \"Décrit les propriétés de courant d''un système physique\", 'Les États-Unis sont une république fédérale composée de cinquante États fédérés auxquels s\\'ajoutent le district de Columbia — comprenant la capitale Washington — et plusieurs territoires. Entité fédérée des États-Unis\"\\n'])\n"
     ]
    }
   ],
   "source": [
    "print(sens_dict[\"état\"])\n",
    "print(dictionary[\"état\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.99110925 0.9911546  0.9912008  0.9911021  0.9910225  0.99105155\n 0.99114907 0.9910726  0.99116457 0.991084   0.99093914 0.99117416\n 0.991151   0.991062   0.9910879  0.99111307 0.9908306  0.9910145\n 0.991006   0.9911998  0.9910586 ]\n2\nEn gravure, un état est une forme différente d'une estampe, causée par un changement délibéré et permanent effectué sur une matrice telle qu'une plaque de cuivre ou un bloc de bois.\n"
     ]
    }
   ],
   "source": [
    "csim = w2v.wv.cosine_similarities(w2v.wv[\"état\"], defsVectors[\"état\"])\n",
    "print(csim)\n",
    "print(np.argmax(csim))\n",
    "print(dictionary[\"état\"][1][np.argmax(csim)])"
   ]
  },
  {
   "source": [
    "Test association mot->sens avec les vecteurs de definitions et vecteurs de contextes en utilisant la similarité cosine"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sentence:\t Le mardi , les participants à la conférence ont été informés d' une autre atrocité , l' assassinat à Medellin de deux employés d' El_Espectador , le deuxième plus grand journal de Colombie .\n-> In doc 0 s 5 and pos 30 (BabelNet sense: bn:00057563n)\n Output:\n\t bn:17765228n\n\t Un journal est une publication périodique recensant un certain nombre d'événements présentés sous la forme d'articles relatifs à une période donnée, généralement une journée, d'où son nom. Type de journal publié tous les jours, éventuellement six ou cinq fois par semaine\n0.16082405\n\n\nSentence:\t L ’ administrateur local du journal , Luz Maria Lopez , a été abattue et sa mère blessée , tandis que sa voiture était arrêtée à un feu_rouge .\n-> In doc 0 s 6 and pos 5 (BabelNet sense: bn:00057563n)\n Output:\n\t bn:00048455n\n\t Un palier lisse assure le guidage en rotation par glissement. Type de roulement\n0.12836336\n\n\nSentence:\t Une heure plus tard , le directeur de la diffusion du journal , Miguel Soler , a été abattu près de son domicile .\n-> In doc 0 s 7 and pos 11 (BabelNet sense: bn:00057563n)\n Output:\n\t bn:00057564n\n\t Un magnat des médias est une personne qui a un grand contrôle sur une ou plusieurs entreprises importantes du secteur des médias.\n0.06327159\n\n\nSentence:\t Les seigneurs_de_la_drogue qui ont revendiqué la responsabilité des assassinats ont affirmé qu' ils allait faire exploser les bureaux du journal de Bogota si on continuerait à le distribuer à Medellin .\n-> In doc 0 s 8 and pos 19 (BabelNet sense: bn:00057564n)\n Output:\n\t bn:01658173n\n\t Un journal est la partie d'un système de fichiers journalisé qui trace les opérations d'écriture tant qu'elles ne sont pas terminées et cela en vue de garantir l'intégrité des données en cas d'arrêt brutal.\n0.011886556\n\n\nSentence:\t Noriega a fermé tous les journaux et toutes les stations de radio et de télévision indépendantes , et il a fait arrêter ou torturer , ou bien il a contraint à l' exil , une longue liste de journalistes '' affirme la déclaration .\n-> In doc 0 s 18 and pos 5 (BabelNet sense: bn:00057563n)\n Output:\n\t bn:00037558n\n\t Périodique imprimé consacré aux faits de société et aux actualités\n-0.025397256\n\n\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(mean_vectors)):\n",
    "    csim = w2v.wv.cosine_similarities(mean_vectors[i], defsVectors[\"journal\"])\n",
    "    amax = np.argmax(csim)\n",
    "    d,s,p,bn = polysem[\"journal\"][i]\n",
    "    print(\"Sentence:\\t\", \" \".join(documents[d][s]))\n",
    "    print(\"-> In doc %d s %d and pos %d (BabelNet sense: %s)\"%(d, s, p, bn))\n",
    "    print(\" Output:\")\n",
    "    print(\"\\t\", dictionary[\"journal\"][0][amax])\n",
    "    print(\"\\t\", dictionary[\"journal\"][1][amax])\n",
    "    print(csim[amax])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('lorsque', 0.9098055958747864),\n",
       " ('nombreuses', 0.9096593856811523),\n",
       " ('Caraïbes', 0.9093672633171082),\n",
       " ('Nom', 0.9091154336929321),\n",
       " ('naturel', 0.9080072641372681),\n",
       " ('britannique', 0.9071807861328125),\n",
       " ('liberté', 0.9070574045181274),\n",
       " ('petit', 0.9070220589637756),\n",
       " ('se', 0.9063730835914612),\n",
       " ('mot', 0.9063611030578613)]"
      ]
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "source": [
    "w2v.wv.most_similar(\"journaux\")"
   ]
  }
 ]
}