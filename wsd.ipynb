{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Dev le(s) modèle(s) de désambiguïsation lexicale"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "import nltk\n",
    "import random\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import decomposition\n",
    "from xml.dom.minidom import parse\n",
    "from gensim.models import Word2Vec\n",
    "#from sklearn.cluster import KMeans\n",
    "from nltk.cluster.kmeans import KMeansClusterer\n",
    "\n",
    "trial_corpus_path = \"trial_corpus.xml\"\n",
    "test_corpus_path = \"test_corpus.xml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Documents (13):\n\tDoc  0: 29 sentences\n\tDoc  1: 16 sentences\n\tDoc  2: 13 sentences\n\tDoc  3: 19 sentences\n\tDoc  4: 13 sentences\n\tDoc  5: 32 sentences\n\tDoc  6: 27 sentences\n\tDoc  7: 28 sentences\n\tDoc  8: 29 sentences\n\tDoc  9: 26 sentences\n\tDoc 10: 24 sentences\n\tDoc 11: 25 sentences\n\tDoc 12: 25 sentences\n\nLemmas (745):\n\tLemma 0: groupe -> 5 values\n\tLemma 1: nations_unies -> 2 values\n\tLemma 2: plan -> 2 values\n\tLemma 3: réduction -> 4 values\n\tLemma 4: émission -> 3 values\n\tLemma 5: conférence -> 4 values\n\tLemma 6: climat -> 3 values\n\tLemma 7: récrimination -> 1 values\n\tLemma 8: vendredi -> 7 values\n\tLemma 9: document -> 5 values\n\t...\n\n\nEx. lemma 0: \n\t groupe -> [(0, 0, 1, 'bn:00041942n'), (1, 1, 41, 'bn:00041942n'), (6, 4, 12, 'bn:00041942n'), (9, 6, 35, 'bn:00072536n'), (9, 20, 26, 'bn:00041942n')]\n"
     ]
    }
   ],
   "source": [
    "def load_corpus(path):\n",
    "    \"\"\"Load a formatted corpus data file\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path: str\n",
    "        Path to the corpus xml file to load\n",
    "\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        A 3 dimensional list containing for each document, the sentences that it is composed of.\n",
    "        Where each sentence is a list of single tokens.\n",
    "    dict\n",
    "        A dictionnary mapping a lemma with it's document/sentence/index position and the BabelNet Sense attributed\n",
    "    \"\"\"\n",
    "\n",
    "    DOMTree = parse(path)\n",
    "\n",
    "    documents = []\n",
    "    sens_dict = {}\n",
    "    for doc in DOMTree.getElementsByTagName(\"document\"):\n",
    "        # For each document\n",
    "        sentences = []\n",
    "        for sent in doc.getElementsByTagName(\"sentence\"):\n",
    "            # And for each sentence\n",
    "            # Append the new sentence\n",
    "            s = sent.getAttribute(\"s\")\n",
    "            sentences.append(s.split())\n",
    "\n",
    "            # Map the lemmas in the sentence with it's doc/sentence/index position and BabelNet sense\n",
    "            for lem in sent.getElementsByTagName(\"lemma\"):\n",
    "                idx = lem.getAttribute(\"idx\")\n",
    "                lemma = lem.getAttribute(\"lemma\")\n",
    "                # Few lemma may have more than 1 BabelNet sense (due to redundancy in BN)\n",
    "                # Only keep the 1st one\n",
    "                sense = lem.getAttribute(\"senses\").split()[0] \n",
    "                \n",
    "                ctx = (int(doc.getAttribute(\"id\")),\n",
    "                        int(sent.getAttribute(\"id\")),\n",
    "                        int(idx),\n",
    "                        sense)\n",
    "                if not lemma in sens_dict:\n",
    "                    sens_dict[lemma] = [ctx]\n",
    "                else:\n",
    "                    sens_dict[lemma].append(ctx)\n",
    "\n",
    "        documents.append(sentences)\n",
    "\n",
    "    return (documents, sens_dict)\n",
    "\n",
    "\n",
    "documents, sens_dict = load_corpus(test_corpus_path)\n",
    "\n",
    "\n",
    "print(\"Documents (%d):\"%(len(documents)))\n",
    "for i, d in enumerate(documents):\n",
    "    print(\"\\tDoc %2d: %02d sentences\"%(i, len(d)))\n",
    "\n",
    "print(\"\\nLemmas (%d):\"%(len(sens_dict)))\n",
    "for i, (k, v) in zip(range(10), sens_dict.items()):\n",
    "    print(\"\\tLemma %d: %s -> %d values\"%(i, k, len(v)))\n",
    "print(\"\\t...\\n\")\n",
    "print(\"\\nEx. lemma 0: \")\n",
    "print(\"\\t\", list(sens_dict.keys())[0], \"->\", list(sens_dict.values())[0])"
   ]
  },
  {
   "source": [
    "# Word2vec"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Vocab size: 2694\n"
     ]
    }
   ],
   "source": [
    "vocab = []\n",
    "for d in documents:\n",
    "    for s in d:\n",
    "        vocab.append(s)\n",
    "w2v = Word2Vec(vocab, min_count=1)\n",
    "\n",
    "del(vocab)\n",
    "\n",
    "print(\"Vocab size:\", len(w2v.wv.vocab))"
   ]
  },
  {
   "source": [
    "# Huang"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nEx. groupe\n\t-> [(0, 0, 1, 'bn:00041942n'), (1, 1, 41, 'bn:00041942n'), (6, 4, 12, 'bn:00041942n'), (9, 6, 35, 'bn:00072536n'), (9, 20, 26, 'bn:00041942n')]\n\t-> ['bn:00072536n', 'bn:00041942n']\n"
     ]
    }
   ],
   "source": [
    "def Lemma2Senses(lemma):\n",
    "    senses = [bn for _,_,_,bn in sens_dict[lemma]]\n",
    "    return list(set(senses))\n",
    "\n",
    "print(\"\\nEx.\", list(sens_dict.keys())[0])\n",
    "print(\"\\t->\", list(sens_dict.values())[0])\n",
    "print(\"\\t->\", Lemma2Senses(list(sens_dict.keys())[0]))"
   ]
  },
  {
   "source": [
    "## Visualisation rapide des lemmes à désambiguïser\n",
    "On remarque que peu de lemmes sont associés à plusieurs sens. Certains apparaissent plusieurs fois avec toujours le meme sens. Pire ! D'autres n'apparaissent qu'une seule fois.<br/>\n",
    "Il est aussi intéressant de remarquer que certains lemmes sont associés 9 fois avec le sens_1 et 1 fois avec le sens_2. Ceci peut trouver son origine dans les annotations via BabelNet qui propose différents sens redondants d'un mot.<br/>\n",
    "Pour exemple, le lemme <i>journaliste</i> est associé aux sens BabelNet suivants :\n",
    "<ol>\n",
    "    <li>bn:00048461n : celui qui recueille, écrit ou distribue des informations</li>\n",
    "    <li>bn:00057562n : celui qui enquête, rapporte ou rédige les actualités</li>\n",
    "</ol>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Nb lemmas: 745\n\npolysems: 69\n\t groupe  ->  [(0, 0, 1, 'bn:00041942n'), (1, 1, 41, 'bn:00041942n'), (6, 4, 12, 'bn:00041942n'), (9, 6, 35, 'bn:00072536n'), (9, 20, 26, 'bn:00041942n')]\n\t plan  ->  [(0, 0, 8, 'bn:00062759n'), (4, 2, 34, 'bn:00062795n')]\n\t document  ->  [(0, 1, 38, 'bn:00028015n'), (0, 7, 10, 'bn:00028015n'), (12, 3, 7, 'bn:00028017n'), (12, 5, 1, 'bn:00028017n'), (12, 14, 7, 'bn:00028017n')]\n\t temps  ->  [(0, 1, 65, 'bn:00077270n'), (0, 3, 16, 'bn:00077267n'), (3, 9, 4, 'bn:00077267n'), (6, 3, 28, 'bn:00077267n')]\n\t accord  ->  [(0, 2, 9, 'bn:00000728n'), (0, 8, 7, 'bn:00000728n'), (0, 14, 7, 'bn:00000728n'), (0, 20, 47, 'bn:00000728n'), (0, 23, 34, 'bn:00000726n'), (3, 0, 7, 'bn:00002086n'), (3, 2, 2, 'bn:00002086n'), (3, 18, 25, 'bn:00000726n')]\n\nsolo: 432\n\t récrimination  ->  [(0, 1, 21, 'bn:00066603n')]\n\t obstacle  ->  [(0, 2, 20, 'bn:00058511n')]\n\t chemin  ->  [(0, 2, 29, 'bn:00061005n')]\n\t groupe_de_travail  ->  [(0, 4, 3, 'bn:00081604n')]\n\t terme  ->  [(0, 4, 32, 'bn:00076588n')]\n\nnon polysem: 244\n\t nations_unies  ->  [(0, 0, 3, 'bn:00078931n'), (8, 18, 5, 'bn:00078931n')]\n\t réduction  ->  [(0, 0, 11, 'bn:00025780n'), (0, 1, 41, 'bn:00025780n'), (0, 12, 15, 'bn:00025780n'), (0, 14, 37, 'bn:00025780n')]\n\t émission  ->  [(0, 0, 13, 'bn:00030455n'), (0, 8, 16, 'bn:00030455n'), (0, 9, 6, 'bn:00030455n')]\n\t conférence  ->  [(0, 1, 1, 'bn:00021721n'), (0, 24, 44, 'bn:00021721n'), (9, 2, 16, 'bn:00021721n'), (9, 9, 13, 'bn:00021721n')]\n\t climat  ->  [(0, 1, 4, 'bn:00019780n'), (0, 19, 24, 'bn:00019780n'), (0, 22, 7, 'bn:00019780n')]\n"
     ]
    }
   ],
   "source": [
    "polysem = {}\n",
    "solo = {}\n",
    "npolysem = {}\n",
    "\n",
    "for k,v in sens_dict.items():\n",
    "    if len(v) == 1:\n",
    "        solo[k] = v\n",
    "        continue\n",
    "    \n",
    "    _,_,_,sense_bn = v[0]\n",
    "    poly = False\n",
    "    for _,_,_,bn in v:\n",
    "        if bn != sense_bn:\n",
    "            polysem[k] = v\n",
    "            poly = True\n",
    "            break\n",
    "    if not poly:\n",
    "        npolysem[k] = v\n",
    "\n",
    "print(\"Nb lemmas:\", len(sens_dict))\n",
    "print()\n",
    "\n",
    "print(\"polysems:\", len(polysem))\n",
    "for _, (k, v) in zip(range(5), polysem.items()):\n",
    "    print(\"\\t\", k, \" -> \", v)\n",
    "\n",
    "print(\"\\nsolo:\", len(solo))\n",
    "for _, (k, v) in zip(range(5), solo.items()):\n",
    "    print(\"\\t\", k, \" -> \", v)\n",
    "\n",
    "print(\"\\nnon polysem:\", len(npolysem))\n",
    "for _, (k, v) in zip(range(5), npolysem.items()):\n",
    "    print(\"\\t\", k, \" -> \", v)"
   ]
  },
  {
   "source": [
    "## Implémentation de la méthode proposée par Huang\n",
    "<ol>\n",
    "    <li>Collecte les fenetres d'occurrence d'un mot </li>\n",
    "    <li>Calcule le vecteur de contexte, moyenne des vecteurs-mots de chaque mots dans un contexte</li>\n",
    "    <li>Cluster les vecteurs de contextes (spherical K-means)</li>\n",
    "    <li>Associe à chaque cluster un sens</li>\n",
    "</ol>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[1, 1, 1, 0, 1]\n",
      "[1, 0, 0, 1, 1]\n",
      "\n",
      "[1, 0]\n",
      "[0, 1]\n",
      "\n",
      "[1, 1, 0, 0, 0]\n",
      "[0, 0, 1, 1, 1]\n",
      "\n",
      "[0, 1, 1, 1]\n",
      "[0, 1, 1, 1]\n",
      "\n",
      "[1, 1, 1, 1, 2, 0, 0, 2]\n",
      "[1, 1, 2, 0, 1, 0, 0, 0]\n",
      "\n",
      "[1, 1, 1, 1, 1, 0]\n",
      "[0, 0, 0, 1, 1, 1]\n",
      "\n",
      "[0, 0, 0, 1, 0]\n",
      "[1, 0, 1, 0, 1]\n",
      "\n",
      "[1, 2, 2, 1, 0, 1, 2, 0, 1, 1]\n",
      "[0, 0, 0, 1, 2, 2, 0, 1, 0, 0]\n",
      "\n",
      "[0, 2, 1]\n",
      "[2, 1, 0]\n",
      "\n",
      "[1, 0, 2, 2, 2]\n",
      "[1, 1, 1, 0, 2]\n",
      "\n",
      "[0, 0, 1, 1, 1, 0]\n",
      "[0, 0, 1, 0, 0, 1]\n",
      "\n",
      "[0, 0, 1]\n",
      "[0, 0, 1]\n",
      "\n",
      "[0, 1, 0]\n",
      "[0, 0, 1]\n",
      "\n",
      "[1, 0, 1]\n",
      "[0, 1, 1]\n",
      "\n",
      "[1, 2, 0, 0]\n",
      "[2, 1, 0, 0]\n",
      "\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "\n",
      "[2, 2, 2, 2, 2, 0, 0, 2, 1, 1, 2]\n",
      "[0, 1, 2, 0, 2, 0, 0, 0, 1, 0, 0]\n",
      "\n",
      "[0, 1]\n",
      "[1, 0]\n",
      "\n",
      "[0, 1, 1]\n",
      "[1, 0, 1]\n",
      "\n",
      "[1, 0, 0, 1, 0, 0, 0, 0]\n",
      "[1, 1, 0, 1, 0, 1, 0, 1]\n",
      "\n",
      "[1, 0]\n",
      "[0, 1]\n",
      "\n",
      "[1, 0]\n",
      "[0, 1]\n",
      "\n",
      "[1, 0, 0]\n",
      "[0, 0, 1]\n",
      "\n",
      "[1, 0]\n",
      "[1, 0]\n",
      "\n",
      "[0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 1]\n",
      "[0, 2, 0, 2, 1, 0, 0, 0, 2, 1, 1, 2]\n",
      "\n",
      "[1, 1, 2, 0]\n",
      "[0, 1, 2, 1]\n",
      "\n",
      "[2, 2, 2, 0, 1]\n",
      "[1, 1, 1, 0, 2]\n",
      "\n",
      "[0, 0, 0, 1, 1]\n",
      "[0, 0, 1, 0, 1]\n",
      "\n",
      "[0, 1, 1, 1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 0, 0, 0, 0, 1]\n",
      "\n",
      "[1, 0]\n",
      "[1, 0]\n",
      "\n",
      "[1, 0, 0, 0]\n",
      "[0, 1, 0, 0]\n",
      "\n",
      "[1, 0]\n",
      "[1, 0]\n",
      "\n",
      "[1, 0, 0, 0]\n",
      "[0, 1, 0, 0]\n",
      "\n",
      "[1, 0]\n",
      "[0, 1]\n",
      "\n",
      "[0, 0, 0, 1]\n",
      "[0, 0, 0, 1]\n",
      "\n",
      "[0, 2, 2, 2, 2, 2, 1]\n",
      "[0, 1, 2, 1, 1, 1, 1]\n",
      "\n",
      "[1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0]\n",
      "[1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1]\n",
      "\n",
      "[1, 1, 0, 0]\n",
      "[0, 1, 0, 0]\n",
      "\n",
      "[0, 1, 1]\n",
      "[0, 1, 0]\n",
      "\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "[0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0]\n",
      "\n",
      "[1, 0]\n",
      "[1, 0]\n",
      "\n",
      "[1, 0, 0, 0]\n",
      "[1, 1, 0, 0]\n",
      "\n",
      "[0, 1]\n",
      "[1, 0]\n",
      "\n",
      "[1, 0, 0]\n",
      "[0, 0, 1]\n",
      "\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "\n",
      "[1, 1, 1, 0, 1]\n",
      "[1, 0, 1, 0, 1]\n",
      "\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "\n",
      "[2, 0, 0, 1, 1, 1, 1, 1, 2]\n",
      "[0, 0, 2, 2, 0, 2, 0, 1, 2]\n",
      "\n",
      "[0, 1, 1, 1, 0, 0, 1, 0]\n",
      "[1, 1, 0, 0, 0, 0, 0, 1]\n",
      "\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "\n",
      "[1, 0, 0, 0, 0]\n",
      "[0, 0, 0, 1, 0]\n",
      "\n",
      "[1, 0]\n",
      "[0, 1]\n",
      "\n",
      "[2, 0, 1]\n",
      "[1, 0, 2]\n",
      "\n",
      "[1, 0, 1, 1]\n",
      "[0, 1, 0, 0]\n",
      "\n",
      "[0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]\n",
      "[0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "\n",
      "[0, 1]\n",
      "[1, 0]\n",
      "\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "[0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0]\n",
      "\n",
      "[1, 0, 0, 0]\n",
      "[0, 0, 0, 1]\n",
      "\n",
      "[1, 1, 0, 1, 0, 0]\n",
      "[0, 1, 1, 0, 0, 1]\n",
      "\n",
      "[1, 0, 0]\n",
      "[1, 0, 0]\n",
      "\n",
      "[0, 0, 0, 1]\n",
      "[0, 0, 0, 1]\n",
      "\n",
      "[1, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 1]\n",
      "\n",
      "[1, 0]\n",
      "[0, 1]\n",
      "\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "\n",
      "[1, 0]\n",
      "[1, 0]\n",
      "\n",
      "[0, 1, 0, 0]\n",
      "[0, 1, 0, 0]\n",
      "\n",
      "[1, 1, 0]\n",
      "[0, 1, 0]\n",
      "\n",
      "[0, 0, 1, 0, 0]\n",
      "[1, 0, 1, 0, 1]\n",
      "\n",
      "[1, 1, 0]\n",
      "[0, 0, 1]\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.53      0.51       162\n",
      "           1       0.40      0.40      0.40       141\n",
      "           2       0.35      0.20      0.25        35\n",
      "\n",
      "    accuracy                           0.44       338\n",
      "   macro avg       0.41      0.38      0.39       338\n",
      "weighted avg       0.44      0.44      0.44       338\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ctx_w = 11 # Contexte window size\n",
    "\n",
    "global_truth = []\n",
    "global_classif = []\n",
    "\n",
    "for lemma, senses in polysem.items():\n",
    "    labels = Lemma2Senses(lemma)\n",
    "    num_senses = len(labels)\n",
    "\n",
    "    # Map clusters with a value from, 0 to num_senses-1\n",
    "    truth = [labels.index(bn) for _,_,_,bn in senses]\n",
    "    global_truth.append(truth)\n",
    "\n",
    "    mean_vectors = []\n",
    "    for d,s,i,_ in senses:\n",
    "        l = len(documents[d][s])\n",
    "        # Extract the words in the contexte window\n",
    "        window = documents[d][s][max(0, i-math.floor((ctx_w-1)/2)) : min(l, i+math.ceil((ctx_w-1)/2))+1]\n",
    "\n",
    "        # Compute the context vector (mean of the words vectors in the window)\n",
    "        mean_vectors.append(np.array([w2v.wv[word] for word in window]).mean(axis=0))\n",
    "\n",
    "    # Spherical K-means clustering\n",
    "    skm = KMeansClusterer(num_senses, nltk.cluster.util.cosine_distance, rng=random.Random(0), repeats=10)\n",
    "    assigned_clusters = skm.cluster(mean_vectors, assign_clusters=True)\n",
    "    global_classif.append(assigned_clusters)\n",
    "\n",
    "    print(truth)\n",
    "    print(assigned_clusters)\n",
    "    print()\n",
    "\n",
    "print(sklearn.metrics.classification_report([y for x in global_truth for y in x], [y for x in global_classif for y in x]))\n"
   ]
  },
  {
   "source": [
    "### Associe à chaque cluster un sens"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{0: 1, 1: 0}"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "def argsmax(lst):\n",
    "    max = lst[0]\n",
    "    argsmax = []\n",
    "    for i in range(len(lst)):\n",
    "        if lst[i] == max:\n",
    "            argsmax.append(i)\n",
    "        elif lst[i] > max:\n",
    "            max = lst[i]\n",
    "            argsmax = [i]\n",
    "    return argsmax\n",
    "\n",
    "def couple(arr1, arr2):\n",
    "    # truth, classif\n",
    "    l = len(arr1)\n",
    "    nb_c = len(set(arr1))\n",
    "    arr = np.zeros((nb_c,nb_c), dtype=int)\n",
    "\n",
    "    for i in range(l):\n",
    "        arr[arr2[i],arr1[i]] += 1\n",
    "\n",
    "    cs = [i for i in range(len(arr))]\n",
    "    ts = [i for i in range(len(arr))]\n",
    "    map = {}\n",
    "\n",
    "    for iter in range(len(arr)):\n",
    "        temp = len(arr)\n",
    "        for i in range(len(arr)):\n",
    "            c = arr[i]\n",
    "            ams = argsmax(c)\n",
    "            if len(ams) == 1:\n",
    "                ams = ams[0]\n",
    "                map[cs[i]] = ts[ams]\n",
    "                arr = np.delete(np.delete(arr, ams, 1), i, 0)\n",
    "                cs = np.delete(cs, i)\n",
    "                ts = np.delete(ts, ams)\n",
    "                break\n",
    "        \n",
    "        if len(arr) == temp:\n",
    "            j = np.argmax(arr[0])\n",
    "            map[cs[0]] = ts[j]\n",
    "            arr = np.delete(np.delete(arr, j, 1), 0, 0)\n",
    "            cs = np.delete(cs, 0)\n",
    "            ts = np.delete(ts, j)\n",
    "\n",
    "    return map\n",
    "\n",
    "couple(global_truth[0], global_classif[0])"
   ]
  },
  {
   "source": [
    "## Premiers Résultats"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0, 1, 1, 0, 0]\n",
      "[1, 1, 1, 0, 1]\n",
      "\n",
      "[1, 0]\n",
      "[1, 0]\n",
      "\n",
      "[1, 1, 0, 0, 0]\n",
      "[1, 1, 0, 0, 0]\n",
      "\n",
      "[0, 1, 1, 1]\n",
      "[0, 1, 1, 1]\n",
      "\n",
      "[1, 1, 2, 0, 1, 0, 0, 0]\n",
      "[1, 1, 1, 1, 2, 0, 0, 2]\n",
      "\n",
      "[1, 1, 1, 0, 0, 0]\n",
      "[1, 1, 1, 1, 1, 0]\n",
      "\n",
      "[0, 1, 0, 1, 0]\n",
      "[0, 0, 0, 1, 0]\n",
      "\n",
      "[1, 1, 1, 0, 2, 2, 1, 0, 1, 1]\n",
      "[1, 2, 2, 1, 0, 1, 2, 0, 1, 1]\n",
      "\n",
      "[0, 2, 1]\n",
      "[0, 2, 1]\n",
      "\n",
      "[0, 0, 0, 2, 1]\n",
      "[1, 0, 2, 2, 2]\n",
      "\n",
      "[0, 0, 1, 0, 0, 1]\n",
      "[0, 0, 1, 1, 1, 0]\n",
      "\n",
      "[0, 0, 1]\n",
      "[0, 0, 1]\n",
      "\n",
      "[1, 1, 0]\n",
      "[0, 1, 0]\n",
      "\n",
      "[1, 0, 0]\n",
      "[1, 0, 1]\n",
      "\n",
      "[1, 2, 0, 0]\n",
      "[1, 2, 0, 0]\n",
      "\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "\n",
      "[2, 1, 0, 2, 0, 2, 2, 2, 1, 2, 2]\n",
      "[2, 2, 2, 2, 2, 0, 0, 2, 1, 1, 2]\n",
      "\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "\n",
      "[0, 1, 0]\n",
      "[0, 1, 1]\n",
      "\n",
      "[1, 1, 0, 1, 0, 1, 0, 1]\n",
      "[1, 0, 0, 1, 0, 0, 0, 0]\n",
      "\n",
      "[1, 0]\n",
      "[1, 0]\n",
      "\n",
      "[1, 0]\n",
      "[1, 0]\n",
      "\n",
      "[1, 1, 0]\n",
      "[1, 0, 0]\n",
      "\n",
      "[1, 0]\n",
      "[1, 0]\n",
      "\n",
      "[2, 1, 2, 1, 0, 2, 2, 2, 1, 0, 0, 1]\n",
      "[0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 1]\n",
      "\n",
      "[1, 0, 2, 0]\n",
      "[1, 1, 2, 0]\n",
      "\n",
      "[2, 2, 2, 0, 1]\n",
      "[2, 2, 2, 0, 1]\n",
      "\n",
      "[0, 0, 1, 0, 1]\n",
      "[0, 0, 0, 1, 1]\n",
      "\n",
      "[0, 0, 0, 1, 1, 1, 1, 0]\n",
      "[0, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "[1, 0]\n",
      "[1, 0]\n",
      "\n",
      "[0, 1, 0, 0]\n",
      "[1, 0, 0, 0]\n",
      "\n",
      "[1, 0]\n",
      "[1, 0]\n",
      "\n",
      "[0, 1, 0, 0]\n",
      "[1, 0, 0, 0]\n",
      "\n",
      "[1, 0]\n",
      "[1, 0]\n",
      "\n",
      "[0, 0, 0, 1]\n",
      "[0, 0, 0, 1]\n",
      "\n",
      "[0, 2, 1, 2, 2, 2, 2]\n",
      "[0, 2, 2, 2, 2, 2, 1]\n",
      "\n",
      "[1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1]\n",
      "[1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0]\n",
      "\n",
      "[0, 1, 0, 0]\n",
      "[1, 1, 0, 0]\n",
      "\n",
      "[0, 1, 0]\n",
      "[0, 1, 1]\n",
      "\n",
      "[1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]\n",
      "\n",
      "[1, 0]\n",
      "[1, 0]\n",
      "\n",
      "[1, 1, 0, 0]\n",
      "[1, 0, 0, 0]\n",
      "\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "\n",
      "[1, 1, 0]\n",
      "[1, 0, 0]\n",
      "\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "\n",
      "[1, 0, 1, 0, 1]\n",
      "[1, 1, 1, 0, 1]\n",
      "\n",
      "[1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "\n",
      "[1, 1, 2, 2, 1, 2, 1, 0, 2]\n",
      "[2, 0, 0, 1, 1, 1, 1, 1, 2]\n",
      "\n",
      "[0, 0, 1, 1, 1, 1, 1, 0]\n",
      "[0, 1, 1, 1, 0, 0, 1, 0]\n",
      "\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "\n",
      "[0, 0, 0, 1, 0]\n",
      "[1, 0, 0, 0, 0]\n",
      "\n",
      "[1, 0]\n",
      "[1, 0]\n",
      "\n",
      "[2, 0, 1]\n",
      "[2, 0, 1]\n",
      "\n",
      "[1, 0, 1, 1]\n",
      "[1, 0, 1, 1]\n",
      "\n",
      "[1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1]\n",
      "[0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "\n",
      "[0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "\n",
      "[0, 0, 0, 1]\n",
      "[1, 0, 0, 0]\n",
      "\n",
      "[1, 0, 0, 1, 1, 0]\n",
      "[1, 1, 0, 1, 0, 0]\n",
      "\n",
      "[1, 0, 0]\n",
      "[1, 0, 0]\n",
      "\n",
      "[0, 0, 0, 1]\n",
      "[0, 0, 0, 1]\n",
      "\n",
      "[0, 0, 0, 0, 1]\n",
      "[1, 0, 0, 0, 0]\n",
      "\n",
      "[1, 0]\n",
      "[1, 0]\n",
      "\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "\n",
      "[1, 0]\n",
      "[1, 0]\n",
      "\n",
      "[0, 1, 0, 0]\n",
      "[0, 1, 0, 0]\n",
      "\n",
      "[0, 1, 0]\n",
      "[1, 1, 0]\n",
      "\n",
      "[1, 0, 1, 0, 1]\n",
      "[0, 0, 1, 0, 0]\n",
      "\n",
      "[1, 1, 0]\n",
      "[1, 1, 0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_classif = []\n",
    "for i in range(len(global_classif)):\n",
    "    cluster_tags = couple(global_truth[i], global_classif[i])\n",
    "    final_classif.append([cluster_tags[i] for i in global_classif[i]])\n",
    "\n",
    "    print(final_classif[i])\n",
    "    print(global_truth[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "              precision    recall  f1-score   support\n\n           0       0.71      0.67      0.69       162\n           1       0.63      0.69      0.66       141\n           2       0.62      0.57      0.60        35\n\n    accuracy                           0.67       338\n   macro avg       0.66      0.64      0.65       338\nweighted avg       0.67      0.67      0.67       338\n\n"
     ]
    }
   ],
   "source": [
    "print(sklearn.metrics.classification_report([y for x in global_truth for y in x], [y for x in final_classif for y in x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}