{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Dev le(s) modèle(s) de désambiguïsation lexicale"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "import nltk\n",
    "import random\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import decomposition\n",
    "from xml.dom.minidom import parse\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.cluster.kmeans import KMeansClusterer\n",
    "\n",
    "trial_corpus_path = \"trial_corpus.xml\"\n",
    "test_corpus_path = \"test_corpus.xml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Documents (1):\n\tDoc  0: 36 sentences\n\nLemmas (124):\n\tLemma 0: guerre_contre_la_drogue -> 1 values\n\tLemma 1: Amérique_Latine -> 2 values\n\tLemma 2: presse -> 9 values\n\tLemma 3: mois -> 2 values\n\tLemma 4: journaliste -> 5 values\n\tLemma 5: trafiquant_de_drogue -> 2 values\n\tLemma 6: guérillero -> 1 values\n\tLemma 7: gauche -> 2 values\n\tLemma 8: personne -> 1 values\n\tLemma 9: Colombie -> 4 values\n\t...\n\n\nEx. lemma 0: \n\t guerre_contre_la_drogue -> [(0, 0, 7, 'bn:00028885n')]\n"
     ]
    }
   ],
   "source": [
    "def loadCorpus(path):\n",
    "    \"\"\"Load a formatted corpus data file\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path: str\n",
    "        Path to the corpus xml file to load\n",
    "\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        A 3 dimensional list containing for each document, the sentences that it is composed of.\n",
    "        Where each sentence is a list of single tokens.\n",
    "    dict\n",
    "        A dictionnary mapping a lemma with it's document/sentence/index position and the BabelNet Sense attributed\n",
    "    \"\"\"\n",
    "\n",
    "    DOMTree = parse(path)\n",
    "\n",
    "    documents = []\n",
    "    sens_dict = {}\n",
    "    for doc in DOMTree.getElementsByTagName(\"document\"):\n",
    "        # For each document\n",
    "        sentences = []\n",
    "        for sent in doc.getElementsByTagName(\"sentence\"):\n",
    "            # And for each sentence\n",
    "            # Append the new sentence\n",
    "            s = sent.getAttribute(\"s\")\n",
    "            sentences.append(s.split())\n",
    "\n",
    "            # Map the lemmas in the sentence with it's doc/sentence/index position and BabelNet sense\n",
    "            for lem in sent.getElementsByTagName(\"lemma\"):\n",
    "                idx = lem.getAttribute(\"idx\")\n",
    "                lemma = lem.getAttribute(\"lemma\")\n",
    "                # Few lemma may have more than 1 BabelNet sense (due to redundancy in BN)\n",
    "                # Only keep the 1st one\n",
    "                sense = lem.getAttribute(\"senses\").split()[0] \n",
    "                \n",
    "                ctx = (int(doc.getAttribute(\"id\")),\n",
    "                        int(sent.getAttribute(\"id\")),\n",
    "                        int(idx),\n",
    "                        sense)\n",
    "                if not lemma in sens_dict:\n",
    "                    sens_dict[lemma] = [ctx]\n",
    "                else:\n",
    "                    sens_dict[lemma].append(ctx)\n",
    "\n",
    "        documents.append(sentences)\n",
    "\n",
    "    return (documents, sens_dict)\n",
    "\n",
    "\n",
    "documents, sens_dict = loadCorpus(trial_corpus_path)\n",
    "\n",
    "\n",
    "print(\"Documents (%d):\"%(len(documents)))\n",
    "for i, d in enumerate(documents):\n",
    "    print(\"\\tDoc %2d: %02d sentences\"%(i, len(d)))\n",
    "\n",
    "print(\"\\nLemmas (%d):\"%(len(sens_dict)))\n",
    "for i, (k, v) in zip(range(10), sens_dict.items()):\n",
    "    print(\"\\tLemma %d: %s -> %d values\"%(i, k, len(v)))\n",
    "print(\"\\t...\\n\")\n",
    "print(\"\\nEx. lemma 0: \")\n",
    "print(\"\\t\", list(sens_dict.keys())[0], \"->\", list(sens_dict.values())[0])"
   ]
  },
  {
   "source": [
    "# Word2vec"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "w2v Vocab size: 429\n"
     ]
    }
   ],
   "source": [
    "def documentsSentences(doc):\n",
    "    docSentences = []\n",
    "    for d in doc:\n",
    "        for s in d:\n",
    "            docSentences.append(s)\n",
    "    return docSentences\n",
    "\n",
    "def createW2vModel():\n",
    "    return Word2Vec(documentsSentences(documents), min_count=1)\n",
    "\n",
    "try:\n",
    "    w2v = Word2Vec.load(\"w2v.model\")\n",
    "except FileNotFoundError:\n",
    "    w2v = createW2vModel()\n",
    "    w2v.save(\"w2v.model\")\n",
    "\n",
    "print(\"w2v Vocab size:\", len(w2v.wv.vocab))"
   ]
  },
  {
   "source": [
    "# Huang"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nEx. guerre_contre_la_drogue\n\t-> [(0, 0, 7, 'bn:00028885n')]\n\t-> ['bn:00028885n']\n"
     ]
    }
   ],
   "source": [
    "def lemma2Senses(lemma):\n",
    "    senses = [bn for _,_,_,bn in sens_dict[lemma]]\n",
    "    return list(set(senses))\n",
    "\n",
    "print(\"\\nEx.\", list(sens_dict.keys())[0])\n",
    "print(\"\\t->\", list(sens_dict.values())[0])\n",
    "print(\"\\t->\", lemma2Senses(list(sens_dict.keys())[0]))"
   ]
  },
  {
   "source": [
    "## Visualisation rapide des lemmes à désambiguïser\n",
    "On remarque que peu de lemmes sont associés à plusieurs sens. Certains apparaissent plusieurs fois avec toujours le meme sens. Pire ! D'autres n'apparaissent qu'une seule fois.<br/>\n",
    "Il est aussi intéressant de remarquer que certains lemmes sont associés 9 fois avec le sens_1 et 1 fois avec le sens_2. Ceci peut trouver son origine dans les annotations via BabelNet qui propose différents sens redondants d'un mot.<br/>\n",
    "Pour exemple, le lemme <i>journaliste</i> est associé aux sens BabelNet suivants :\n",
    "<ol>\n",
    "    <li>bn:00048461n : celui qui recueille, écrit ou distribue des informations</li>\n",
    "    <li>bn:00057562n : celui qui enquête, rapporte ou rédige les actualités</li>\n",
    "</ol>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Nb lemmas: 124\n\npolysems: 3\n\t journaliste  ->  [(0, 1, 8, 'bn:00048461n'), (0, 2, 10, 'bn:00048461n'), (0, 16, 43, 'bn:00048461n'), (0, 18, 38, 'bn:00057562n'), (0, 19, 27, 'bn:00048461n')]\n\t contrôle  ->  [(0, 3, 29, 'bn:00022287n'), (0, 26, 19, 'bn:00022283n')]\n\t journal  ->  [(0, 5, 30, 'bn:00057563n'), (0, 6, 5, 'bn:00057563n'), (0, 7, 11, 'bn:00057563n'), (0, 8, 19, 'bn:00057564n'), (0, 18, 5, 'bn:00057563n')]\n\nsolo: 86\n\t guerre_contre_la_drogue  ->  [(0, 0, 7, 'bn:00028885n')]\n\t guérillero  ->  [(0, 1, 22, 'bn:02557244n')]\n\t personne  ->  [(0, 1, 33, 'bn:00046516n')]\n\t année  ->  [(0, 2, 5, 'bn:00078738n')]\n\t août  ->  [(0, 3, 18, 'bn:00007140n')]\n\nnon polysem: 35\n\t Amérique_Latine  ->  [(0, 0, 9, 'bn:00050165n'), (0, 4, 14, 'bn:00050165n')]\n\t presse  ->  [(0, 0, 23, 'bn:00064245n'), (0, 4, 38, 'bn:00064245n'), (0, 12, 4, 'bn:00064245n'), (0, 16, 29, 'bn:00064245n'), (0, 17, 19, 'bn:00064245n'), (0, 19, 19, 'bn:00064245n'), (0, 20, 9, 'bn:00064245n'), (0, 21, 28, 'bn:00064245n'), (0, 35, 1, 'bn:00064245n')]\n\t mois  ->  [(0, 1, 5, 'bn:00014710n'), (0, 9, 8, 'bn:00014710n')]\n\t trafiquant_de_drogue  ->  [(0, 1, 19, 'bn:00028881n'), (0, 17, 11, 'bn:00028881n')]\n\t gauche  ->  [(0, 1, 24, 'bn:00149192n'), (0, 21, 49, 'bn:00149192n')]\n"
     ]
    }
   ],
   "source": [
    "polysem = {}\n",
    "solo = {}\n",
    "npolysem = {}\n",
    "\n",
    "for k,v in sens_dict.items():\n",
    "    if len(v) == 1:\n",
    "        solo[k] = v\n",
    "        continue\n",
    "    \n",
    "    _,_,_,sense_bn = v[0]\n",
    "    poly = False\n",
    "    for _,_,_,bn in v:\n",
    "        if bn != sense_bn:\n",
    "            polysem[k] = v\n",
    "            poly = True\n",
    "            break\n",
    "    if not poly:\n",
    "        npolysem[k] = v\n",
    "\n",
    "print(\"Nb lemmas:\", len(sens_dict))\n",
    "print()\n",
    "\n",
    "print(\"polysems:\", len(polysem))\n",
    "for _, (k, v) in zip(range(5), polysem.items()):\n",
    "    print(\"\\t\", k, \" -> \", v)\n",
    "\n",
    "print(\"\\nsolo:\", len(solo))\n",
    "for _, (k, v) in zip(range(5), solo.items()):\n",
    "    print(\"\\t\", k, \" -> \", v)\n",
    "\n",
    "print(\"\\nnon polysem:\", len(npolysem))\n",
    "for _, (k, v) in zip(range(5), npolysem.items()):\n",
    "    print(\"\\t\", k, \" -> \", v)"
   ]
  },
  {
   "source": [
    "## Implémentation de la méthode proposée par Huang\n",
    "<ol>\n",
    "    <li>Collecte les fenetres d'occurrence d'un mot </li>\n",
    "    <li>Calcule le vecteur de contexte, moyenne des vecteurs-mots de chaque mots dans un contexte</li>\n",
    "    <li>Cluster les vecteurs de contextes (spherical K-means)</li>\n",
    "    <li>Associe à chaque cluster un sens</li>\n",
    "</ol>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meanSentenceVector(w2vModel, sentence):\n",
    "    return np.array([w2vModel.wv[word] for word in sentence]).mean(axis=0)\n",
    "\n",
    "def sumSentenceVector(w2vModel, sentence):\n",
    "    return np.array([w2vModel.wv[word] for word in sentence]).sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['suis', 'très', 'content', 'de', 'manger']\n['Je', 'suis', 'très', 'content', 'de', 'manger']\n['suis', 'très', 'content', 'de', 'manger', 'une', 'pomme', '!']\n"
     ]
    }
   ],
   "source": [
    "def extractWindow(sentence, start, end):\n",
    "    return sentence[max(0, start) : min(len(sentence), end)+1]\n",
    "\n",
    "print(extractWindow(\"Je suis très content de manger une pomme !\".split(), 1, 5))\n",
    "print(extractWindow(\"Je suis très content de manger une pomme !\".split(), -1, 5))\n",
    "print(extractWindow(\"Je suis très content de manger une pomme !\".split(), 1, 15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[1, 1, 1, 0, 1]\n",
      "[0, 1, 0, 1, 1]\n",
      "\n",
      "[0, 1]\n",
      "[0, 1]\n",
      "\n",
      "[0, 0, 0, 1, 0]\n",
      "[0, 0, 0, 1, 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ctx_w = 11 # Contexte window size\n",
    "\n",
    "global_truth = []\n",
    "global_classif = []\n",
    "\n",
    "for lemma, senses in polysem.items():\n",
    "    labels = lemma2Senses(lemma)\n",
    "    num_senses = len(labels)\n",
    "\n",
    "    # Map clusters with a value from, 0 to num_senses-1\n",
    "    truth = [labels.index(bn) for _,_,_,bn in senses]\n",
    "    global_truth.append(truth)\n",
    "\n",
    "    mean_vectors = []\n",
    "    for d,s,i,_ in senses:\n",
    "        l = len(documents[d][s])\n",
    "        # Extract the words in the contexte window\n",
    "        window = extractWindow(documents[d][s], i-math.floor((ctx_w-1)/2), i+math.ceil((ctx_w-1)/2))\n",
    "\n",
    "        # Compute the context vector (mean of the words vectors in the window)\n",
    "        mean_vectors.append(meanSentenceVector(w2v, window))\n",
    "\n",
    "    # Spherical K-means clustering\n",
    "    skm = KMeansClusterer(num_senses, nltk.cluster.util.cosine_distance, rng=random.Random(0), repeats=10)\n",
    "    assigned_clusters = skm.cluster(mean_vectors, assign_clusters=True)\n",
    "    global_classif.append(assigned_clusters)\n",
    "\n",
    "    print(truth)\n",
    "    print(assigned_clusters)\n",
    "    print()\n",
    "\n",
    "#print(sklearn.metrics.classification_report([y for x in global_truth for y in x], [y for x in global_classif for y in x]))\n"
   ]
  },
  {
   "source": [
    "### Associe à chaque cluster un sens"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{0: 1, 1: 0}"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "def argsmax(lst):\n",
    "    max = lst[0]\n",
    "    argsmax = []\n",
    "    for i in range(len(lst)):\n",
    "        if lst[i] == max:\n",
    "            argsmax.append(i)\n",
    "        elif lst[i] > max:\n",
    "            max = lst[i]\n",
    "            argsmax = [i]\n",
    "    return argsmax\n",
    "\n",
    "def couple(arr1, arr2):\n",
    "    # truth, classif\n",
    "    l = len(arr1)\n",
    "    nb_c = len(set(arr1))\n",
    "    arr = np.zeros((nb_c,nb_c), dtype=int)\n",
    "\n",
    "    for i in range(l):\n",
    "        arr[arr2[i],arr1[i]] += 1\n",
    "\n",
    "    cs = [i for i in range(len(arr))]\n",
    "    ts = [i for i in range(len(arr))]\n",
    "    map = {}\n",
    "\n",
    "    for iter in range(len(arr)):\n",
    "        temp = len(arr)\n",
    "        for i in range(len(arr)):\n",
    "            c = arr[i]\n",
    "            ams = argsmax(c)\n",
    "            if len(ams) == 1:\n",
    "                ams = ams[0]\n",
    "                map[cs[i]] = ts[ams]\n",
    "                arr = np.delete(np.delete(arr, ams, 1), i, 0)\n",
    "                cs = np.delete(cs, i)\n",
    "                ts = np.delete(ts, ams)\n",
    "                break\n",
    "        \n",
    "        if len(arr) == temp:\n",
    "            j = np.argmax(arr[0])\n",
    "            map[cs[0]] = ts[j]\n",
    "            arr = np.delete(np.delete(arr, j, 1), 0, 0)\n",
    "            cs = np.delete(cs, 0)\n",
    "            ts = np.delete(ts, j)\n",
    "\n",
    "    return map\n",
    "\n",
    "couple(global_truth[0], global_classif[0])"
   ]
  },
  {
   "source": [
    "## Premiers Résultats"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "journaliste\n[(0, 1, 8, 'bn:00048461n'), (0, 2, 10, 'bn:00048461n'), (0, 16, 43, 'bn:00048461n'), (0, 18, 38, 'bn:00057562n'), (0, 19, 27, 'bn:00048461n')]\n[1, 0, 1, 0, 0]\n[1, 1, 1, 0, 1]\n\ncontrôle\n[(0, 3, 29, 'bn:00022287n'), (0, 26, 19, 'bn:00022283n')]\n[0, 1]\n[0, 1]\n\njournal\n[(0, 5, 30, 'bn:00057563n'), (0, 6, 5, 'bn:00057563n'), (0, 7, 11, 'bn:00057563n'), (0, 8, 19, 'bn:00057564n'), (0, 18, 5, 'bn:00057563n')]\n[0, 0, 0, 1, 1]\n[0, 0, 0, 1, 0]\n\n"
     ]
    }
   ],
   "source": [
    "final_classif = []\n",
    "for i in range(len(global_classif)):\n",
    "    cluster_tags = couple(global_truth[i], global_classif[i])\n",
    "    final_classif.append([cluster_tags[i] for i in global_classif[i]])\n",
    "\n",
    "    print(list(polysem.keys())[i])\n",
    "    print(list(polysem.values())[i])\n",
    "    print(final_classif[i])\n",
    "    print(global_truth[i])\n",
    "    print()"
   ]
  },
  {
   "source": [
    "Score associations clustering/gold truth"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "              precision    recall  f1-score   support\n\n           0       0.71      0.83      0.77         6\n           1       0.80      0.67      0.73         6\n\n    accuracy                           0.75        12\n   macro avg       0.76      0.75      0.75        12\nweighted avg       0.76      0.75      0.75        12\n\n"
     ]
    }
   ],
   "source": [
    "print(sklearn.metrics.classification_report([y for x in global_truth for y in x], [y for x in final_classif for y in x]))"
   ]
  },
  {
   "source": [
    "# Méthode 2, calule des similarités entre une occurence d'un mot et ses définitions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import xml.etree.ElementTree as et\n",
    "\n",
    "output_classif_path = \"test_corpus.classif\""
   ]
  },
  {
   "source": [
    "Charge le dictionnaire"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Dictionary length: 119\n"
     ]
    }
   ],
   "source": [
    "def load_dictionary(path):\n",
    "    dictionary_dict = {}\n",
    "\n",
    "    with open(path, \"r\") as file:\n",
    "        file.readline()\n",
    "        for row in file:\n",
    "            row = row.split(\";\")\n",
    "            lemma = row.pop(0)\n",
    "            nb = int(row.pop(0))\n",
    "            \n",
    "            # Make sure the definition is not empty...\n",
    "            if nb > 0:\n",
    "                ids = (row.pop(0)).split(\",\")\n",
    "                defs = (\";\".join(row)).split(\"\\\",\\\"\")\n",
    "                temp = defs[0].split(\",\\\"\")\n",
    "                defs = temp + defs[1:]\n",
    "\n",
    "                dictionary_dict[lemma] = (ids, defs)\n",
    "    \n",
    "    return dictionary_dict\n",
    "\n",
    "dictionary = load_dictionary(\"dict.dictionary\")\n",
    "print(\"Dictionary length:\", len(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(['bn:00078546n', 'bn:00118342n'],\n",
       " ['\"Le mardi est le jour de la semaine qui succède au lundi et qui précède le mercredi. Jour de la semaine Le deuxième jour de la semaine en Europe et dans les pays utilisant la norme ISO 8601; le troisième jour de la semaine aux États-Unis d\\'Amérique.',\n",
       "  'Mardi est le troisième livre publié par l\\'écrivain américain Herman Melville.\"\\n'])"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "dictionary[\"mardi\"]"
   ]
  },
  {
   "source": [
    "## Word2Vec\n",
    "Apprend les embeddings sur le corpus et les definitions du dictionnaire"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Extended w2v Vocab size: 6628\n"
     ]
    }
   ],
   "source": [
    "def definitionsSentences():\n",
    "    defSentences = []\n",
    "    for _,defs in dictionary.values():\n",
    "        for s in defs:\n",
    "            defSentences.append(s.split())\n",
    "    return defSentences\n",
    "\n",
    "def createExtendedW2vModel():\n",
    "    return Word2Vec(documentsSentences(documents)+definitionsSentences(), min_count=1)\n",
    "\n",
    "try:\n",
    "    w2v = Word2Vec.load(\"w2v.model.extended\")\n",
    "except FileNotFoundError:\n",
    "    w2v = createExtendedW2vModel()\n",
    "    w2v.save(\"w2v.model.extended\")\n",
    "\n",
    "print(\"Extended w2v Vocab size:\", len(w2v.wv.vocab))"
   ]
  },
  {
   "source": [
    "## Calcule des vecteurs de définitions\n",
    "Un vecteur de définition est la moyenne des embeddings des mots d'une définition"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "defsVectors = {} #map id lemma -> vecteur\n",
    "\n",
    "for k,(ids,defs) in dictionary.items():\n",
    "    defsVectors[k] = np.array([meanSentenceVector(w2v,d.split()) for d in defs]).reshape((len(defs), 100))"
   ]
  },
  {
   "source": [
    "Test association mot->sens avec les vecteurs de definitions et vecteurs de contextes en utilisant la similarité cosine."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sentence:\t Le mardi , les participants à la conférence ont été informés d' une autre atrocité , l' assassinat à Medellin de deux employés d' El_Espectador , le deuxième plus grand journal de Colombie .\n-> In doc 0 s 5 and pos 30 (BabelNet sense: bn:00057563n)\n Output:\n\t bn:17765228n\n\t Un journal est une publication périodique recensant un certain nombre d'événements présentés sous la forme d'articles relatifs à une période donnée, généralement une journée, d'où son nom. Type de journal publié tous les jours, éventuellement six ou cinq fois par semaine\n0.16082408\n\n\nSentence:\t L ’ administrateur local du journal , Luz Maria Lopez , a été abattue et sa mère blessée , tandis que sa voiture était arrêtée à un feu_rouge .\n-> In doc 0 s 6 and pos 5 (BabelNet sense: bn:00057563n)\n Output:\n\t bn:00048455n\n\t Un palier lisse assure le guidage en rotation par glissement. Type de roulement\n0.12836337\n\n\nSentence:\t Une heure plus tard , le directeur de la diffusion du journal , Miguel Soler , a été abattu près de son domicile .\n-> In doc 0 s 7 and pos 11 (BabelNet sense: bn:00057563n)\n Output:\n\t bn:00057564n\n\t Un magnat des médias est une personne qui a un grand contrôle sur une ou plusieurs entreprises importantes du secteur des médias.\n0.06327158\n\n\nSentence:\t Les seigneurs_de_la_drogue qui ont revendiqué la responsabilité des assassinats ont affirmé qu' ils allait faire exploser les bureaux du journal de Bogota si on continuerait à le distribuer à Medellin .\n-> In doc 0 s 8 and pos 19 (BabelNet sense: bn:00057564n)\n Output:\n\t bn:01658173n\n\t Un journal est la partie d'un système de fichiers journalisé qui trace les opérations d'écriture tant qu'elles ne sont pas terminées et cela en vue de garantir l'intégrité des données en cas d'arrêt brutal.\n0.011886548\n\n\nSentence:\t Noriega a fermé tous les journaux et toutes les stations de radio et de télévision indépendantes , et il a fait arrêter ou torturer , ou bien il a contraint à l' exil , une longue liste de journalistes '' affirme la déclaration .\n-> In doc 0 s 18 and pos 5 (BabelNet sense: bn:00057563n)\n Output:\n\t bn:00037558n\n\t Périodique imprimé consacré aux faits de société et aux actualités\n-0.025397262\n\n\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(mean_vectors)):\n",
    "    csim = w2v.wv.cosine_similarities(mean_vectors[i], defsVectors[\"journal\"])\n",
    "    amax = np.argmax(csim)\n",
    "    d,s,p,bn = polysem[\"journal\"][i]\n",
    "    print(\"Sentence:\\t\", \" \".join(documents[d][s]))\n",
    "    print(\"-> In doc %d s %d and pos %d (BabelNet sense: %s)\"%(d, s, p, bn))\n",
    "    print(\" Output:\")\n",
    "    print(\"\\t\", dictionary[\"journal\"][0][amax])\n",
    "    print(\"\\t\", dictionary[\"journal\"][1][amax])\n",
    "    print(csim[amax])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Missing definition for \"seigneur_de_la_drogue\"\nMissing definition for \"seigneur_de_la_drogue\"\nMissing definition for \"el_spectador\"\nMissing definition for \"david_asman\"\nMissing definition for \"jose_abello_silva\"\nMissing definition for \"leonidas_vargas\"\n"
     ]
    }
   ],
   "source": [
    "with open(output_classif_path, \"w\") as file:\n",
    "    file.write(\"lemma;doc_id;sent_id;sent_pos;output_bn_id;output_def\\n\")\n",
    "\n",
    "    for lemma,v in sens_dict.items():\n",
    "        for d,s,p,bn in v:\n",
    "            window = extractWindow(documents[d][s], p-math.floor((ctx_w-1)/2), p+math.ceil((ctx_w-1)/2))\n",
    "            mean_vector = meanSentenceVector(w2v, window)\n",
    "            \n",
    "            lemma = lemma.lower()\n",
    "            if lemma in defsVectors:\n",
    "                csim = w2v.wv.cosine_similarities(mean_vector, defsVectors[lemma])\n",
    "\n",
    "                amax = np.argmax(csim)\n",
    "\n",
    "                bn_id = dictionary[lemma][0][amax]\n",
    "                bn_def = dictionary[lemma][1][amax]\n",
    "\n",
    "                file.write(\"{};{};{};{};{};\\\"{}\\\"\\n\".format(lemma, d, s, p, bn_id, bn_def))\n",
    "            else:\n",
    "                print(\"Missing definition for \\\"%s\\\"\"%(lemma))"
   ]
  }
 ]
}